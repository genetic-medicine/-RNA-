{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "# !mkdir /home/aistudio/external-libraries\n",
    "# !pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 程序在/work/code目录下， 需先加入路径\n",
    "import sys \n",
    "sys.path.append('/home/aistudio/work/code')\n",
    "# fly_paddle是唯一需要直接调用的模块\n",
    "# fly_paddle is the only module users need to interact with\n",
    "import fly_paddle as fp\n",
    "\n",
    "# args包括所有需要的参数， 贯穿于几乎所有的程序调用中\n",
    "# fp.parse_args2() 根据任务初始化args, 要用到的任务包括： ‘train', 'predict'\n",
    "# args is a structure containing most (if not all) parameters\n",
    "# fp.parse_args2() initializes args based on the task to run, such as \"train\", \"predict\"\n",
    "args, _ = fp.parse_args2('train')\n",
    "print(fp.gwio.json_str(args.__dict__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 注： 根据不同的网络等等需要， args可能包含一些用不到的参数\n",
    "# Attention: some parameters in args may not be used depending on the network etc.\n",
    "# 两种更新args的方法： 1） args.update(**dict), 2) args.[key] = value\n",
    "# Two main ways to update values in args: 1) args.update(**dict), 2) args.[key] = value\n",
    "args.update(data_dir='work/data', data_name='train', residue_dbn=True, residue_extra=True)\n",
    "\n",
    "# 网络参数 （net parameters): \n",
    "# 网络的设计主要考虑了三个RNA碱基配对的支配作用： \n",
    "#    1) 来自于全部序列的排列组合（配分）竞争，用Attention机制来模拟\n",
    "#    2）来自于线性大分子的一维序列限制， 用LSTM结构来模拟\n",
    "#    3）来自于局部紧邻碱基的合作（比如，一个孤立的碱基对极不稳定）， 用1D Convolution来模拟\n",
    "# 所以框架由以上三个模块组成， 并在输入和输出层加了1-3个线性层。除非特意说明， 所有的隐藏层的维度为32. \n",
    "# 训练中发现高维度和深度的网络并不能给出更好的结果！\n",
    "# Three main governing mechanisms for RNA base pairing are taken into consideration for the \n",
    "# design of the network architecture. \n",
    "#   1) The combinatorial configurational space of base pairing among all RNA bases, accounted for with Attention Mechanism\n",
    "#   2) The quasi-1D nature of unbranched RNA polymers, accounted for with LSTM\n",
    "#   3) The cooperativity of neighboring bases for stable base pairing, accounted for with 1D Convolution\n",
    "# Hence the neural net comprises of three main building blocks, with addiitional linear layers for input and output. \n",
    "# The dimensions of all hidden layers are 32 unless noted otherwise\n",
    "# Larger and deeper nets gave similar, but no better, performances!\n",
    "args.net='seq2seq_attnlstmconv1d'  # the net name defined in paddle_nets.py\n",
    "args.linear_num = 1 # the number of linear feedforward layers\n",
    "args.attn_num = 1 # the number of transformer encoder layers\n",
    "args.lstm_num = 1 # the number of bidirectional lstm layers\n",
    "args.conv1d_num = 1 # the number of 1D convolutional layers\n",
    "# 输出模块由三个线性层组成， 维度分别为32, 32, 2\n",
    "# three linear layers for the final output, with dimensions of 32, 32, and 2, respectively\n",
    "args.output_dim = [32, 32, 2] \n",
    "args.norm_fn = 'layer' # layer normalization\n",
    "args.batch_size = 1 # 1 is used in consideration of the layer norm above\n",
    "# 最后递交用的损失函数选为 softmax+bce, 也可以用 softmax+mse, 结果几乎一样\n",
    "args.loss_fn = ['softmax+bce'] # softmax is needed here as the final output has a dimension of 2\n",
    "args.label_tone = 'soft'\n",
    "args.loss_sqrt = True # sqrt(loss) is only necessary for softmax+mse\n",
    "args.loss_padding = False # exclude padded residues from loss\n",
    "# 需要运行autoconfig_args()来消除参数的不一致性\n",
    "# autoconfig_args() is needed to resolve inconsistencies between parameters\n",
    "args = fp.autoconfig_args(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:35:15,630 - INFO - Used net definition: \u001b[0;39;46m/home/aistudio/work/code/paddle_nets.py\u001b[0m\n",
      "2021-05-12 23:35:15,704 - INFO - {'total_params': 36418, 'trainable_params': 36418}\n",
      "2021-05-12 23:35:15,705 - INFO - Optimizer method: adam\n",
      "2021-05-12 23:35:15,706 - INFO -    learning rate: 0.003\n",
      "2021-05-12 23:35:15,706 - INFO -     lr_scheduler: reduced\n",
      "2021-05-12 23:35:15,706 - INFO -     weight decay: none\n",
      "2021-05-12 23:35:15,707 - INFO -          l1decay: 0.0001\n",
      "2021-05-12 23:35:15,707 - INFO -          l2decay: 0.0001\n",
      "2021-05-12 23:35:15,708 - INFO - Getting loss function: ['softmax+bce']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "      Layer (type)                          Input Shape                                  Output Shape                   Param #    \n",
      "=====================================================================================================================================\n",
      "   MyEmbeddingLayer-1                      [[2, 512, 10]]                                [2, 512, 10]                      0       \n",
      "        Linear-1                           [[2, 512, 10]]                                [2, 512, 32]                     352      \n",
      "         ReLU-1                            [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-1                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Dropout-1                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "     MyLinearTower-1                       [[2, 512, 10]]                                [2, 512, 32]                      0       \n",
      "    PositionEncoder-1                      [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-2                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Linear-2                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Linear-3                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Linear-4                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Linear-5                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "  MultiHeadAttention-1    [[2, 512, 32], [2, 512, 32], [2, 512, 32], None]               [2, 512, 32]                      0       \n",
      "        Dropout-3                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-3                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Linear-6                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Dropout-2                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "        Linear-7                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Dropout-4                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "TransformerEncoderLayer-1                  [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "  TransformerEncoder-1                  [[2, 512, 32], None]                             [2, 512, 32]                      0       \n",
      "      MyAttnTower-1                        [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "         LSTM-1                            [[2, 512, 32]]                  [[2, 512, 64], [[2, 2, 32], [2, 2, 32]]]     16,896     \n",
      "      MyLSTMTower-1                        [[2, 512, 32]]                                [2, 512, 64]                      0       \n",
      "        Conv1D-1                           [[2, 512, 64]]                                [2, 512, 32]                   10,272     \n",
      "         ReLU-2                            [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-4                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Dropout-5                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "     MyConv1DTower-1                       [[2, 512, 64]]                                [2, 512, 32]                      0       \n",
      "        Linear-8                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "         ReLU-3                            [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-5                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Dropout-6                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "        Linear-9                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "         ReLU-4                            [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-6                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Dropout-7                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "        Linear-10                          [[2, 512, 32]]                                [2, 512, 2]                      66       \n",
      "     MyLinearTower-2                       [[2, 512, 32]]                                [2, 512, 2]                       0       \n",
      "=====================================================================================================================================\n",
      "Total params: 36,418\n",
      "Trainable params: 36,418\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 9.73\n",
      "Params size (MB): 0.14\n",
      "Estimated Total Size (MB): 9.91\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 建立和检测模型 （Get and inspect the model）\n",
    "model = fp.get_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:35:15,714 - INFO - Loading data: work/data/train.pkl\n",
      "2021-05-12 23:35:15,760 - INFO -    # of data: 5000,  max seqlen: 500, user seq_length: [0, 512, -1]\n",
      "2021-05-12 23:35:15,761 - INFO -  residue fmt: vector, nn: 0, dbn: True, attr: False, genre: upp\n",
      "2021-05-12 23:35:15,779 - INFO - Selected 5000 data sets with length range: [0, 512, -1]\n",
      "2021-05-12 23:35:20,618 - INFO - Processing upp data...\n"
     ]
    }
   ],
   "source": [
    "# 读取数据 （read in data)\n",
    "midata = fp.get_midata(args)\n",
    "train_data, valid_data = fp.train_test_split(midata, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:35:21,410 - INFO - Training, data size: 4500\n",
      "2021-05-12 23:35:21,508 - INFO -          batch size: 1\n",
      "2021-05-12 23:35:21,509 - INFO -             shuffle: True\n",
      "2021-05-12 23:35:21,510 - INFO -        # of batches: 4500\n",
      "2021-05-12 23:35:21,511 - INFO -      recap interval: 151\n",
      "2021-05-12 23:35:21,511 - INFO -   validate interval: 450\n",
      "2021-05-12 23:35:21,512 - INFO -         # of epochs: 21\n",
      "2021-05-12 23:35:21,513 - INFO -        loss padding: False\n",
      "2021-05-12 23:35:22,468 - INFO - Epoch/batch: 0/   0, ibatch:    0, loss: \u001b[0;36m0.8524\u001b[0m, std: 0.5392\n",
      "2021-05-12 23:35:30,114 - INFO - loss: \u001b[0;32m0.8924\u001b[0m, std: 0.5263\n",
      "2021-05-12 23:35:36,074 - INFO - Epoch/batch: 0/ 151, ibatch:  151, loss: \u001b[0;36m0.6988\u001b[0m, std: 0.6297\n",
      "2021-05-12 23:35:41,863 - INFO - Epoch/batch: 0/ 302, ibatch:  302, loss: \u001b[0;36m0.6065\u001b[0m, std: 0.6719\n",
      "2021-05-12 23:35:57,380 - INFO - loss: \u001b[0;32m0.5724\u001b[0m, std: 0.6974\n",
      "2021-05-12 23:35:57,396 - INFO - Saved model states in: earlystop_0.5724\n",
      "2021-05-12 23:35:57,397 - INFO - Saved net python code: earlystop_0.5724/paddle_nets.py\n",
      "2021-05-12 23:35:57,407 - INFO - Saved best model: earlystop_0.5724\n",
      "2021-05-12 23:35:57,518 - INFO - Epoch/batch: 0/ 453, ibatch:  453, loss: \u001b[0;36m0.5954\u001b[0m, std: 0.6647\n",
      "2021-05-12 23:36:03,262 - INFO - Epoch/batch: 0/ 604, ibatch:  604, loss: \u001b[0;36m0.5718\u001b[0m, std: 0.6531\n",
      "2021-05-12 23:36:08,893 - INFO - Epoch/batch: 0/ 755, ibatch:  755, loss: \u001b[0;36m0.5827\u001b[0m, std: 0.6680\n",
      "2021-05-12 23:36:24,335 - INFO - loss: \u001b[0;32m0.5670\u001b[0m, std: 0.7107\n",
      "2021-05-12 23:36:24,350 - INFO - Saved model states in: earlystop_0.5670\n",
      "2021-05-12 23:36:24,351 - INFO - Saved net python code: earlystop_0.5670/paddle_nets.py\n",
      "2021-05-12 23:36:24,358 - INFO - Saved best model: earlystop_0.5670\n",
      "2021-05-12 23:36:24,358 - INFO - Removing earlystop model: earlystop_0.5724\n",
      "2021-05-12 23:36:24,609 - INFO - Epoch/batch: 0/ 906, ibatch:  906, loss: \u001b[0;36m0.5474\u001b[0m, std: 0.6387\n",
      "2021-05-12 23:36:30,898 - INFO - Epoch/batch: 0/1057, ibatch: 1057, loss: \u001b[0;36m0.5475\u001b[0m, std: 0.6331\n",
      "2021-05-12 23:36:36,618 - INFO - Epoch/batch: 0/1208, ibatch: 1208, loss: \u001b[0;36m0.5666\u001b[0m, std: 0.6455\n",
      "2021-05-12 23:36:51,707 - INFO - loss: \u001b[0;32m0.5509\u001b[0m, std: 0.6609\n",
      "2021-05-12 23:36:51,724 - INFO - Saved model states in: earlystop_0.5509\n",
      "2021-05-12 23:36:51,726 - INFO - Saved net python code: earlystop_0.5509/paddle_nets.py\n",
      "2021-05-12 23:36:51,733 - INFO - Saved best model: earlystop_0.5509\n",
      "2021-05-12 23:36:51,734 - INFO - Removing earlystop model: earlystop_0.5670\n",
      "2021-05-12 23:36:52,000 - INFO - Epoch/batch: 0/1359, ibatch: 1359, loss: \u001b[0;36m0.5668\u001b[0m, std: 0.6423\n",
      "2021-05-12 23:36:57,291 - INFO - Epoch/batch: 0/1510, ibatch: 1510, loss: \u001b[0;36m0.5609\u001b[0m, std: 0.6387\n",
      "2021-05-12 23:37:02,968 - INFO - Epoch/batch: 0/1661, ibatch: 1661, loss: \u001b[0;36m0.5538\u001b[0m, std: 0.6327\n",
      "2021-05-12 23:37:18,098 - INFO - loss: \u001b[0;32m0.5442\u001b[0m, std: 0.6219\n",
      "2021-05-12 23:37:18,115 - INFO - Saved model states in: earlystop_0.5442\n",
      "2021-05-12 23:37:18,117 - INFO - Saved net python code: earlystop_0.5442/paddle_nets.py\n",
      "2021-05-12 23:37:18,127 - INFO - Saved best model: earlystop_0.5442\n",
      "2021-05-12 23:37:18,128 - INFO - Removing earlystop model: earlystop_0.5509\n",
      "2021-05-12 23:37:18,600 - INFO - Epoch/batch: 0/1812, ibatch: 1812, loss: \u001b[0;36m0.5678\u001b[0m, std: 0.6375\n",
      "2021-05-12 23:37:24,366 - INFO - Epoch/batch: 0/1963, ibatch: 1963, loss: \u001b[0;36m0.5530\u001b[0m, std: 0.6327\n",
      "2021-05-12 23:37:30,014 - INFO - Epoch/batch: 0/2114, ibatch: 2114, loss: \u001b[0;36m0.5748\u001b[0m, std: 0.6540\n",
      "2021-05-12 23:37:45,294 - INFO - loss: \u001b[0;32m0.5427\u001b[0m, std: 0.6480\n",
      "2021-05-12 23:37:45,309 - INFO - Saved model states in: earlystop_0.5427\n",
      "2021-05-12 23:37:45,311 - INFO - Saved net python code: earlystop_0.5427/paddle_nets.py\n",
      "2021-05-12 23:37:45,318 - INFO - Saved best model: earlystop_0.5427\n",
      "2021-05-12 23:37:45,320 - INFO - Removing earlystop model: earlystop_0.5442\n",
      "2021-05-12 23:37:45,963 - INFO - Epoch/batch: 0/2265, ibatch: 2265, loss: \u001b[0;36m0.5509\u001b[0m, std: 0.6343\n",
      "2021-05-12 23:37:51,633 - INFO - Epoch/batch: 0/2416, ibatch: 2416, loss: \u001b[0;36m0.5397\u001b[0m, std: 0.6255\n",
      "2021-05-12 23:37:57,161 - INFO - Epoch/batch: 0/2567, ibatch: 2567, loss: \u001b[0;36m0.5531\u001b[0m, std: 0.6214\n",
      "2021-05-12 23:38:11,943 - INFO - loss: \u001b[0;32m0.5423\u001b[0m, std: 0.6389\n",
      "2021-05-12 23:38:11,958 - INFO - Saved model states in: earlystop_0.5423\n",
      "2021-05-12 23:38:11,959 - INFO - Saved net python code: earlystop_0.5423/paddle_nets.py\n",
      "2021-05-12 23:38:11,965 - INFO - Saved best model: earlystop_0.5423\n",
      "2021-05-12 23:38:11,966 - INFO - Removing earlystop model: earlystop_0.5427\n",
      "2021-05-12 23:38:12,648 - INFO - Epoch/batch: 0/2718, ibatch: 2718, loss: \u001b[0;36m0.5571\u001b[0m, std: 0.6408\n",
      "2021-05-12 23:38:18,371 - INFO - Epoch/batch: 0/2869, ibatch: 2869, loss: \u001b[0;36m0.5479\u001b[0m, std: 0.6258\n",
      "2021-05-12 23:38:23,996 - INFO - Epoch/batch: 0/3020, ibatch: 3020, loss: \u001b[0;36m0.5330\u001b[0m, std: 0.6240\n",
      "2021-05-12 23:38:38,697 - INFO - loss: \u001b[0;32m0.5529\u001b[0m, std: 0.5635\n",
      "2021-05-12 23:38:39,589 - INFO - Epoch/batch: 0/3171, ibatch: 3171, loss: \u001b[0;36m0.5602\u001b[0m, std: 0.6362\n",
      "2021-05-12 23:38:45,403 - INFO - Epoch/batch: 0/3322, ibatch: 3322, loss: \u001b[0;36m0.5391\u001b[0m, std: 0.6231\n",
      "2021-05-12 23:38:51,252 - INFO - Epoch/batch: 0/3473, ibatch: 3473, loss: \u001b[0;36m0.5554\u001b[0m, std: 0.6297\n",
      "2021-05-12 23:39:06,017 - INFO - loss: \u001b[0;32m0.5453\u001b[0m, std: 0.5803\n",
      "2021-05-12 23:39:06,909 - INFO - Epoch/batch: 0/3624, ibatch: 3624, loss: \u001b[0;36m0.5669\u001b[0m, std: 0.6353\n",
      "2021-05-12 23:39:12,582 - INFO - Epoch/batch: 0/3775, ibatch: 3775, loss: \u001b[0;36m0.5632\u001b[0m, std: 0.6389\n",
      "2021-05-12 23:39:18,191 - INFO - Epoch/batch: 0/3926, ibatch: 3926, loss: \u001b[0;36m0.5435\u001b[0m, std: 0.6220\n",
      "2021-05-12 23:39:32,701 - INFO - loss: \u001b[0;32m0.5406\u001b[0m, std: 0.6270\n",
      "2021-05-12 23:39:32,717 - INFO - Saved model states in: earlystop_0.5406\n",
      "2021-05-12 23:39:32,718 - INFO - Saved net python code: earlystop_0.5406/paddle_nets.py\n",
      "2021-05-12 23:39:32,725 - INFO - Saved best model: earlystop_0.5406\n",
      "2021-05-12 23:39:32,726 - INFO - Removing earlystop model: earlystop_0.5423\n",
      "2021-05-12 23:39:33,735 - INFO - Epoch/batch: 0/4077, ibatch: 4077, loss: \u001b[0;36m0.5639\u001b[0m, std: 0.6358\n",
      "2021-05-12 23:39:39,305 - INFO - Epoch/batch: 0/4228, ibatch: 4228, loss: \u001b[0;36m0.5502\u001b[0m, std: 0.6213\n",
      "2021-05-12 23:39:45,142 - INFO - Epoch/batch: 0/4379, ibatch: 4379, loss: \u001b[0;36m0.5714\u001b[0m, std: 0.6373\n",
      "2021-05-12 23:39:59,614 - INFO - loss: \u001b[0;32m0.5379\u001b[0m, std: 0.6245\n",
      "2021-05-12 23:39:59,630 - INFO - Saved model states in: earlystop_0.5379\n",
      "2021-05-12 23:39:59,631 - INFO - Saved net python code: earlystop_0.5379/paddle_nets.py\n",
      "2021-05-12 23:39:59,637 - INFO - Saved best model: earlystop_0.5379\n",
      "2021-05-12 23:39:59,638 - INFO - Removing earlystop model: earlystop_0.5406\n",
      "2021-05-12 23:40:00,112 - INFO - Epoch 0 average training loss: \u001b[0;46m0.5653\u001b[0m std: 0.6377\n",
      "2021-05-12 23:40:00,116 - INFO - Epoch 0 average validate loss: \u001b[0;46m0.5808\u001b[0m std: 0.6272\n",
      "2021-05-12 23:40:02,250 - INFO - Epoch/batch: 1/   0, ibatch: 4500, loss: \u001b[0;36m0.5675\u001b[0m, std: 0.6375\n",
      "2021-05-12 23:40:12,530 - INFO - loss: \u001b[0;32m0.5384\u001b[0m, std: 0.6295\n",
      "2021-05-12 23:40:18,187 - INFO - Epoch/batch: 1/ 151, ibatch: 4651, loss: \u001b[0;36m0.5414\u001b[0m, std: 0.6249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: ReduceOnPlateau set learning rate to 0.0027.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:40:23,826 - INFO - Epoch/batch: 1/ 302, ibatch: 4802, loss: \u001b[0;36m0.5439\u001b[0m, std: 0.6257\n",
      "2021-05-12 23:40:39,182 - INFO - loss: \u001b[0;32m0.5391\u001b[0m, std: 0.6482\n",
      "2021-05-12 23:40:39,290 - INFO - Epoch/batch: 1/ 453, ibatch: 4953, loss: \u001b[0;36m0.5396\u001b[0m, std: 0.6252\n",
      "2021-05-12 23:40:44,939 - INFO - Epoch/batch: 1/ 604, ibatch: 5104, loss: \u001b[0;36m0.5315\u001b[0m, std: 0.6126\n",
      "2021-05-12 23:40:50,707 - INFO - Epoch/batch: 1/ 755, ibatch: 5255, loss: \u001b[0;36m0.5455\u001b[0m, std: 0.6300\n",
      "2021-05-12 23:41:06,312 - INFO - loss: \u001b[0;32m0.5395\u001b[0m, std: 0.6522\n",
      "2021-05-12 23:41:06,537 - INFO - Epoch/batch: 1/ 906, ibatch: 5406, loss: \u001b[0;36m0.5352\u001b[0m, std: 0.6250\n",
      "2021-05-12 23:41:12,258 - INFO - Epoch/batch: 1/1057, ibatch: 5557, loss: \u001b[0;36m0.5537\u001b[0m, std: 0.6323\n",
      "2021-05-12 23:41:18,032 - INFO - Epoch/batch: 1/1208, ibatch: 5708, loss: \u001b[0;36m0.5416\u001b[0m, std: 0.6192\n",
      "2021-05-12 23:41:34,114 - INFO - loss: \u001b[0;32m0.5382\u001b[0m, std: 0.5862\n",
      "2021-05-12 23:41:34,414 - INFO - Epoch/batch: 1/1359, ibatch: 5859, loss: \u001b[0;36m0.5469\u001b[0m, std: 0.6269\n",
      "2021-05-12 23:41:40,255 - INFO - Epoch/batch: 1/1510, ibatch: 6010, loss: \u001b[0;36m0.5546\u001b[0m, std: 0.6302\n",
      "2021-05-12 23:41:45,776 - INFO - Epoch/batch: 1/1661, ibatch: 6161, loss: \u001b[0;36m0.5208\u001b[0m, std: 0.6197\n",
      "2021-05-12 23:42:00,491 - INFO - loss: \u001b[0;32m0.5408\u001b[0m, std: 0.6570\n",
      "2021-05-12 23:42:00,998 - INFO - Epoch/batch: 1/1812, ibatch: 6312, loss: \u001b[0;36m0.5372\u001b[0m, std: 0.6190\n",
      "2021-05-12 23:42:06,417 - INFO - Epoch/batch: 1/1963, ibatch: 6463, loss: \u001b[0;36m0.5565\u001b[0m, std: 0.6294\n",
      "2021-05-12 23:42:12,258 - INFO - Epoch/batch: 1/2114, ibatch: 6614, loss: \u001b[0;36m0.5641\u001b[0m, std: 0.6355\n",
      "2021-05-12 23:42:27,390 - INFO - loss: \u001b[0;32m0.5348\u001b[0m, std: 0.5923\n",
      "2021-05-12 23:42:27,406 - INFO - Saved model states in: earlystop_0.5348\n",
      "2021-05-12 23:42:27,408 - INFO - Saved net python code: earlystop_0.5348/paddle_nets.py\n",
      "2021-05-12 23:42:27,420 - INFO - Saved best model: earlystop_0.5348\n",
      "2021-05-12 23:42:27,420 - INFO - Removing earlystop model: earlystop_0.5379\n",
      "2021-05-12 23:42:27,999 - INFO - Epoch/batch: 1/2265, ibatch: 6765, loss: \u001b[0;36m0.5550\u001b[0m, std: 0.6290\n",
      "2021-05-12 23:42:33,768 - INFO - Epoch/batch: 1/2416, ibatch: 6916, loss: \u001b[0;36m0.5409\u001b[0m, std: 0.6243\n",
      "2021-05-12 23:42:39,639 - INFO - Epoch/batch: 1/2567, ibatch: 7067, loss: \u001b[0;36m0.5451\u001b[0m, std: 0.6083\n",
      "2021-05-12 23:42:54,087 - INFO - loss: \u001b[0;32m0.5339\u001b[0m, std: 0.6237\n",
      "2021-05-12 23:42:54,118 - INFO - Saved model states in: earlystop_0.5339\n",
      "2021-05-12 23:42:54,120 - INFO - Saved net python code: earlystop_0.5339/paddle_nets.py\n",
      "2021-05-12 23:42:54,129 - INFO - Saved best model: earlystop_0.5339\n",
      "2021-05-12 23:42:54,130 - INFO - Removing earlystop model: earlystop_0.5348\n",
      "2021-05-12 23:42:54,888 - INFO - Epoch/batch: 1/2718, ibatch: 7218, loss: \u001b[0;36m0.5361\u001b[0m, std: 0.6250\n",
      "2021-05-12 23:43:00,492 - INFO - Epoch/batch: 1/2869, ibatch: 7369, loss: \u001b[0;36m0.5530\u001b[0m, std: 0.6210\n",
      "2021-05-12 23:43:06,137 - INFO - Epoch/batch: 1/3020, ibatch: 7520, loss: \u001b[0;36m0.5630\u001b[0m, std: 0.6380\n",
      "2021-05-12 23:43:21,169 - INFO - loss: \u001b[0;32m0.5456\u001b[0m, std: 0.5678\n",
      "2021-05-12 23:43:21,948 - INFO - Epoch/batch: 1/3171, ibatch: 7671, loss: \u001b[0;36m0.5800\u001b[0m, std: 0.6366\n",
      "2021-05-12 23:43:27,496 - INFO - Epoch/batch: 1/3322, ibatch: 7822, loss: \u001b[0;36m0.5514\u001b[0m, std: 0.6204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: ReduceOnPlateau set learning rate to 0.0024300000000000003.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:43:33,070 - INFO - Epoch/batch: 1/3473, ibatch: 7973, loss: \u001b[0;36m0.5384\u001b[0m, std: 0.6229\n",
      "2021-05-12 23:43:47,382 - INFO - loss: \u001b[0;32m0.5342\u001b[0m, std: 0.6276\n",
      "2021-05-12 23:43:48,310 - INFO - Epoch/batch: 1/3624, ibatch: 8124, loss: \u001b[0;36m0.5465\u001b[0m, std: 0.6263\n",
      "2021-05-12 23:43:54,213 - INFO - Epoch/batch: 1/3775, ibatch: 8275, loss: \u001b[0;36m0.5469\u001b[0m, std: 0.6219\n",
      "2021-05-12 23:44:00,371 - INFO - Epoch/batch: 1/3926, ibatch: 8426, loss: \u001b[0;36m0.5618\u001b[0m, std: 0.6340\n",
      "2021-05-12 23:44:14,705 - INFO - loss: \u001b[0;32m0.5343\u001b[0m, std: 0.6310\n",
      "2021-05-12 23:44:15,692 - INFO - Epoch/batch: 1/4077, ibatch: 8577, loss: \u001b[0;36m0.5432\u001b[0m, std: 0.6203\n",
      "2021-05-12 23:44:21,373 - INFO - Epoch/batch: 1/4228, ibatch: 8728, loss: \u001b[0;36m0.5372\u001b[0m, std: 0.6168\n",
      "2021-05-12 23:44:26,814 - INFO - Epoch/batch: 1/4379, ibatch: 8879, loss: \u001b[0;36m0.5421\u001b[0m, std: 0.6231\n",
      "2021-05-12 23:44:40,678 - INFO - loss: \u001b[0;32m0.5385\u001b[0m, std: 0.5695\n",
      "2021-05-12 23:44:41,911 - INFO - Epoch 1 average training loss: \u001b[0;46m0.5462\u001b[0m std: 0.6246\n",
      "2021-05-12 23:44:41,920 - INFO - Epoch 1 average validate loss: \u001b[0;46m0.5379\u001b[0m std: 0.6168\n",
      "2021-05-12 23:44:43,785 - INFO - Epoch/batch: 2/   0, ibatch: 9000, loss: \u001b[0;36m0.5305\u001b[0m, std: 0.6114\n",
      "2021-05-12 23:44:53,430 - INFO - loss: \u001b[0;32m0.5378\u001b[0m, std: 0.5717\n",
      "2021-05-12 23:44:59,245 - INFO - Epoch/batch: 2/ 151, ibatch: 9151, loss: \u001b[0;36m0.5525\u001b[0m, std: 0.6287\n",
      "2021-05-12 23:45:05,087 - INFO - Epoch/batch: 2/ 302, ibatch: 9302, loss: \u001b[0;36m0.5293\u001b[0m, std: 0.6078\n",
      "2021-05-12 23:45:20,070 - INFO - loss: \u001b[0;32m0.5381\u001b[0m, std: 0.5957\n",
      "2021-05-12 23:45:20,174 - INFO - Epoch/batch: 2/ 453, ibatch: 9453, loss: \u001b[0;36m0.5303\u001b[0m, std: 0.6091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: ReduceOnPlateau set learning rate to 0.002187.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:45:25,712 - INFO - Epoch/batch: 2/ 604, ibatch: 9604, loss: \u001b[0;36m0.5291\u001b[0m, std: 0.6114\n",
      "2021-05-12 23:45:31,264 - INFO - Epoch/batch: 2/ 755, ibatch: 9755, loss: \u001b[0;36m0.5527\u001b[0m, std: 0.6204\n",
      "2021-05-12 23:45:45,999 - INFO - loss: \u001b[0;32m0.5343\u001b[0m, std: 0.5891\n",
      "2021-05-12 23:45:46,246 - INFO - Epoch/batch: 2/ 906, ibatch: 9906, loss: \u001b[0;36m0.5330\u001b[0m, std: 0.6142\n",
      "2021-05-12 23:45:51,964 - INFO - Epoch/batch: 2/1057, ibatch: 10057, loss: \u001b[0;36m0.5457\u001b[0m, std: 0.6274\n",
      "2021-05-12 23:45:57,653 - INFO - Epoch/batch: 2/1208, ibatch: 10208, loss: \u001b[0;36m0.5433\u001b[0m, std: 0.6217\n",
      "2021-05-12 23:46:12,735 - INFO - loss: \u001b[0;32m0.5305\u001b[0m, std: 0.6121\n",
      "2021-05-12 23:46:12,751 - INFO - Saved model states in: earlystop_0.5305\n",
      "2021-05-12 23:46:12,753 - INFO - Saved net python code: earlystop_0.5305/paddle_nets.py\n",
      "2021-05-12 23:46:12,760 - INFO - Saved best model: earlystop_0.5305\n",
      "2021-05-12 23:46:12,761 - INFO - Removing earlystop model: earlystop_0.5339\n",
      "2021-05-12 23:46:13,103 - INFO - Epoch/batch: 2/1359, ibatch: 10359, loss: \u001b[0;36m0.5519\u001b[0m, std: 0.6303\n",
      "2021-05-12 23:46:19,097 - INFO - Epoch/batch: 2/1510, ibatch: 10510, loss: \u001b[0;36m0.5386\u001b[0m, std: 0.6140\n",
      "2021-05-12 23:46:24,812 - INFO - Epoch/batch: 2/1661, ibatch: 10661, loss: \u001b[0;36m0.5376\u001b[0m, std: 0.6158\n",
      "2021-05-12 23:46:40,326 - INFO - loss: \u001b[0;32m0.5311\u001b[0m, std: 0.6108\n",
      "2021-05-12 23:46:40,800 - INFO - Epoch/batch: 2/1812, ibatch: 10812, loss: \u001b[0;36m0.5497\u001b[0m, std: 0.6264\n",
      "2021-05-12 23:46:46,561 - INFO - Epoch/batch: 2/1963, ibatch: 10963, loss: \u001b[0;36m0.5498\u001b[0m, std: 0.6279\n",
      "2021-05-12 23:46:52,103 - INFO - Epoch/batch: 2/2114, ibatch: 11114, loss: \u001b[0;36m0.5458\u001b[0m, std: 0.6199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75: ReduceOnPlateau set learning rate to 0.0019683.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:47:07,046 - INFO - loss: \u001b[0;32m0.5364\u001b[0m, std: 0.5905\n",
      "2021-05-12 23:47:07,597 - INFO - Epoch/batch: 2/2265, ibatch: 11265, loss: \u001b[0;36m0.5546\u001b[0m, std: 0.6260\n",
      "2021-05-12 23:47:13,089 - INFO - Epoch/batch: 2/2416, ibatch: 11416, loss: \u001b[0;36m0.5535\u001b[0m, std: 0.6319\n",
      "2021-05-12 23:47:18,609 - INFO - Epoch/batch: 2/2567, ibatch: 11567, loss: \u001b[0;36m0.5466\u001b[0m, std: 0.6297\n",
      "2021-05-12 23:47:33,241 - INFO - loss: \u001b[0;32m0.5426\u001b[0m, std: 0.5611\n",
      "2021-05-12 23:47:33,877 - INFO - Epoch/batch: 2/2718, ibatch: 11718, loss: \u001b[0;36m0.5277\u001b[0m, std: 0.6112\n",
      "2021-05-12 23:47:39,525 - INFO - Epoch/batch: 2/2869, ibatch: 11869, loss: \u001b[0;36m0.5164\u001b[0m, std: 0.5988\n",
      "2021-05-12 23:47:45,399 - INFO - Epoch/batch: 2/3020, ibatch: 12020, loss: \u001b[0;36m0.5466\u001b[0m, std: 0.6318\n",
      "2021-05-12 23:48:00,534 - INFO - loss: \u001b[0;32m0.5323\u001b[0m, std: 0.6269\n",
      "2021-05-12 23:48:01,355 - INFO - Epoch/batch: 2/3171, ibatch: 12171, loss: \u001b[0;36m0.5377\u001b[0m, std: 0.6173\n",
      "2021-05-12 23:48:07,217 - INFO - Epoch/batch: 2/3322, ibatch: 12322, loss: \u001b[0;36m0.5424\u001b[0m, std: 0.6107\n",
      "2021-05-12 23:48:12,749 - INFO - Epoch/batch: 2/3473, ibatch: 12473, loss: \u001b[0;36m0.5163\u001b[0m, std: 0.6067\n",
      "2021-05-12 23:48:27,334 - INFO - loss: \u001b[0;32m0.5317\u001b[0m, std: 0.5897\n",
      "2021-05-12 23:48:28,264 - INFO - Epoch/batch: 2/3624, ibatch: 12624, loss: \u001b[0;36m0.5447\u001b[0m, std: 0.6249\n",
      "2021-05-12 23:48:34,053 - INFO - Epoch/batch: 2/3775, ibatch: 12775, loss: \u001b[0;36m0.5435\u001b[0m, std: 0.6268\n",
      "2021-05-12 23:48:39,859 - INFO - Epoch/batch: 2/3926, ibatch: 12926, loss: \u001b[0;36m0.5416\u001b[0m, std: 0.6177\n",
      "2021-05-12 23:48:54,052 - INFO - loss: \u001b[0;32m0.5295\u001b[0m, std: 0.6066\n",
      "2021-05-12 23:48:54,068 - INFO - Saved model states in: earlystop_0.5295\n",
      "2021-05-12 23:48:54,069 - INFO - Saved net python code: earlystop_0.5295/paddle_nets.py\n",
      "2021-05-12 23:48:54,076 - INFO - Saved best model: earlystop_0.5295\n",
      "2021-05-12 23:48:54,076 - INFO - Removing earlystop model: earlystop_0.5305\n",
      "2021-05-12 23:48:55,238 - INFO - Epoch/batch: 2/4077, ibatch: 13077, loss: \u001b[0;36m0.5551\u001b[0m, std: 0.6325\n",
      "2021-05-12 23:49:00,839 - INFO - Epoch/batch: 2/4228, ibatch: 13228, loss: \u001b[0;36m0.5391\u001b[0m, std: 0.6174\n",
      "2021-05-12 23:49:06,648 - INFO - Epoch/batch: 2/4379, ibatch: 13379, loss: \u001b[0;36m0.5565\u001b[0m, std: 0.6229\n",
      "2021-05-12 23:49:21,070 - INFO - loss: \u001b[0;32m0.5323\u001b[0m, std: 0.5930\n",
      "2021-05-12 23:49:22,308 - INFO - Epoch 2 average training loss: \u001b[0;46m0.5417\u001b[0m std: 0.6202\n",
      "2021-05-12 23:49:22,316 - INFO - Epoch 2 average validate loss: \u001b[0;46m0.5342\u001b[0m std: 0.5952\n",
      "2021-05-12 23:49:24,170 - INFO - Epoch/batch: 3/   0, ibatch: 13500, loss: \u001b[0;36m0.5392\u001b[0m, std: 0.6257\n",
      "2021-05-12 23:49:34,324 - INFO - loss: \u001b[0;32m0.5325\u001b[0m, std: 0.5918\n",
      "2021-05-12 23:49:40,476 - INFO - Epoch/batch: 3/ 151, ibatch: 13651, loss: \u001b[0;36m0.5537\u001b[0m, std: 0.6282\n",
      "2021-05-12 23:49:46,343 - INFO - Epoch/batch: 3/ 302, ibatch: 13802, loss: \u001b[0;36m0.5379\u001b[0m, std: 0.6146\n",
      "2021-05-12 23:50:01,955 - INFO - loss: \u001b[0;32m0.5328\u001b[0m, std: 0.6361\n",
      "2021-05-12 23:50:02,088 - INFO - Epoch/batch: 3/ 453, ibatch: 13953, loss: \u001b[0;36m0.5392\u001b[0m, std: 0.6164\n",
      "2021-05-12 23:50:07,517 - INFO - Epoch/batch: 3/ 604, ibatch: 14104, loss: \u001b[0;36m0.5252\u001b[0m, std: 0.6125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: ReduceOnPlateau set learning rate to 0.00177147.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:50:12,843 - INFO - Epoch/batch: 3/ 755, ibatch: 14255, loss: \u001b[0;36m0.5260\u001b[0m, std: 0.6131\n",
      "2021-05-12 23:50:27,841 - INFO - loss: \u001b[0;32m0.5334\u001b[0m, std: 0.6372\n",
      "2021-05-12 23:50:28,093 - INFO - Epoch/batch: 3/ 906, ibatch: 14406, loss: \u001b[0;36m0.5455\u001b[0m, std: 0.6239\n",
      "2021-05-12 23:50:33,742 - INFO - Epoch/batch: 3/1057, ibatch: 14557, loss: \u001b[0;36m0.5522\u001b[0m, std: 0.6231\n",
      "2021-05-12 23:50:39,259 - INFO - Epoch/batch: 3/1208, ibatch: 14708, loss: \u001b[0;36m0.5372\u001b[0m, std: 0.6090\n",
      "2021-05-12 23:50:54,072 - INFO - loss: \u001b[0;32m0.5333\u001b[0m, std: 0.6407\n",
      "2021-05-12 23:50:54,363 - INFO - Epoch/batch: 3/1359, ibatch: 14859, loss: \u001b[0;36m0.5296\u001b[0m, std: 0.6085\n",
      "2021-05-12 23:51:00,146 - INFO - Epoch/batch: 3/1510, ibatch: 15010, loss: \u001b[0;36m0.5460\u001b[0m, std: 0.6181\n",
      "2021-05-12 23:51:06,151 - INFO - Epoch/batch: 3/1661, ibatch: 15161, loss: \u001b[0;36m0.5439\u001b[0m, std: 0.6218\n",
      "2021-05-12 23:51:21,267 - INFO - loss: \u001b[0;32m0.5324\u001b[0m, std: 0.5850\n",
      "2021-05-12 23:51:21,765 - INFO - Epoch/batch: 3/1812, ibatch: 15312, loss: \u001b[0;36m0.5398\u001b[0m, std: 0.6185\n",
      "2021-05-12 23:51:27,756 - INFO - Epoch/batch: 3/1963, ibatch: 15463, loss: \u001b[0;36m0.5326\u001b[0m, std: 0.6121\n",
      "2021-05-12 23:51:33,409 - INFO - Epoch/batch: 3/2114, ibatch: 15614, loss: \u001b[0;36m0.5374\u001b[0m, std: 0.6162\n",
      "2021-05-12 23:51:47,734 - INFO - loss: \u001b[0;32m0.5292\u001b[0m, std: 0.6028\n",
      "2021-05-12 23:51:47,750 - INFO - Saved model states in: earlystop_0.5292\n",
      "2021-05-12 23:51:47,752 - INFO - Saved net python code: earlystop_0.5292/paddle_nets.py\n",
      "2021-05-12 23:51:47,759 - INFO - Saved best model: earlystop_0.5292\n",
      "2021-05-12 23:51:47,760 - INFO - Removing earlystop model: earlystop_0.5295\n",
      "2021-05-12 23:51:48,338 - INFO - Epoch/batch: 3/2265, ibatch: 15765, loss: \u001b[0;36m0.5460\u001b[0m, std: 0.6262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106: ReduceOnPlateau set learning rate to 0.0015943230000000001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:51:53,697 - INFO - Epoch/batch: 3/2416, ibatch: 15916, loss: \u001b[0;36m0.5244\u001b[0m, std: 0.6038\n",
      "2021-05-12 23:51:59,199 - INFO - Epoch/batch: 3/2567, ibatch: 16067, loss: \u001b[0;36m0.5416\u001b[0m, std: 0.6203\n",
      "2021-05-12 23:52:13,608 - INFO - loss: \u001b[0;32m0.5309\u001b[0m, std: 0.6057\n",
      "2021-05-12 23:52:14,380 - INFO - Epoch/batch: 3/2718, ibatch: 16218, loss: \u001b[0;36m0.5563\u001b[0m, std: 0.6316\n",
      "2021-05-12 23:52:19,880 - INFO - Epoch/batch: 3/2869, ibatch: 16369, loss: \u001b[0;36m0.5472\u001b[0m, std: 0.6187\n",
      "2021-05-12 23:52:25,362 - INFO - Epoch/batch: 3/3020, ibatch: 16520, loss: \u001b[0;36m0.5252\u001b[0m, std: 0.6070\n",
      "2021-05-12 23:52:39,682 - INFO - loss: \u001b[0;32m0.5295\u001b[0m, std: 0.6070\n",
      "2021-05-12 23:52:40,399 - INFO - Epoch/batch: 3/3171, ibatch: 16671, loss: \u001b[0;36m0.5312\u001b[0m, std: 0.6129\n",
      "2021-05-12 23:52:46,244 - INFO - Epoch/batch: 3/3322, ibatch: 16822, loss: \u001b[0;36m0.5491\u001b[0m, std: 0.6285\n",
      "2021-05-12 23:52:51,999 - INFO - Epoch/batch: 3/3473, ibatch: 16973, loss: \u001b[0;36m0.5360\u001b[0m, std: 0.6189\n",
      "2021-05-12 23:53:06,120 - INFO - loss: \u001b[0;32m0.5304\u001b[0m, std: 0.5953\n",
      "2021-05-12 23:53:07,101 - INFO - Epoch/batch: 3/3624, ibatch: 17124, loss: \u001b[0;36m0.5422\u001b[0m, std: 0.6191\n",
      "2021-05-12 23:53:12,904 - INFO - Epoch/batch: 3/3775, ibatch: 17275, loss: \u001b[0;36m0.5455\u001b[0m, std: 0.6210\n",
      "2021-05-12 23:53:18,364 - INFO - Epoch/batch: 3/3926, ibatch: 17426, loss: \u001b[0;36m0.5317\u001b[0m, std: 0.6154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117: ReduceOnPlateau set learning rate to 0.0014348907.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:53:33,689 - INFO - loss: \u001b[0;32m0.5287\u001b[0m, std: 0.5965\n",
      "2021-05-12 23:53:33,718 - INFO - Saved model states in: earlystop_0.5287\n",
      "2021-05-12 23:53:33,720 - INFO - Saved net python code: earlystop_0.5287/paddle_nets.py\n",
      "2021-05-12 23:53:33,727 - INFO - Saved best model: earlystop_0.5287\n",
      "2021-05-12 23:53:33,728 - INFO - Removing earlystop model: earlystop_0.5292\n",
      "2021-05-12 23:53:34,776 - INFO - Epoch/batch: 3/4077, ibatch: 17577, loss: \u001b[0;36m0.5310\u001b[0m, std: 0.6136\n",
      "2021-05-12 23:53:40,267 - INFO - Epoch/batch: 3/4228, ibatch: 17728, loss: \u001b[0;36m0.5238\u001b[0m, std: 0.6057\n",
      "2021-05-12 23:53:45,859 - INFO - Epoch/batch: 3/4379, ibatch: 17879, loss: \u001b[0;36m0.5502\u001b[0m, std: 0.6343\n",
      "2021-05-12 23:53:59,882 - INFO - loss: \u001b[0;32m0.5285\u001b[0m, std: 0.6159\n",
      "2021-05-12 23:53:59,898 - INFO - Saved model states in: earlystop_0.5285\n",
      "2021-05-12 23:53:59,899 - INFO - Saved net python code: earlystop_0.5285/paddle_nets.py\n",
      "2021-05-12 23:53:59,905 - INFO - Saved best model: earlystop_0.5285\n",
      "2021-05-12 23:53:59,906 - INFO - Removing earlystop model: earlystop_0.5287\n",
      "2021-05-12 23:54:01,186 - INFO - Epoch 3 average training loss: \u001b[0;46m0.5388\u001b[0m std: 0.6176\n",
      "2021-05-12 23:54:01,213 - INFO - Epoch 3 average validate loss: \u001b[0;46m0.5311\u001b[0m std: 0.6104\n",
      "2021-05-12 23:54:03,310 - INFO - Epoch/batch: 4/   0, ibatch: 18000, loss: \u001b[0;36m0.5349\u001b[0m, std: 0.6135\n",
      "2021-05-12 23:54:13,108 - INFO - loss: \u001b[0;32m0.5285\u001b[0m, std: 0.6151\n",
      "2021-05-12 23:54:19,043 - INFO - Epoch/batch: 4/ 151, ibatch: 18151, loss: \u001b[0;36m0.5327\u001b[0m, std: 0.6138\n",
      "2021-05-12 23:54:24,404 - INFO - Epoch/batch: 4/ 302, ibatch: 18302, loss: \u001b[0;36m0.5455\u001b[0m, std: 0.6199\n",
      "2021-05-12 23:54:39,489 - INFO - loss: \u001b[0;32m0.5288\u001b[0m, std: 0.6037\n",
      "2021-05-12 23:54:39,647 - INFO - Epoch/batch: 4/ 453, ibatch: 18453, loss: \u001b[0;36m0.5425\u001b[0m, std: 0.6166\n",
      "2021-05-12 23:54:45,530 - INFO - Epoch/batch: 4/ 604, ibatch: 18604, loss: \u001b[0;36m0.5506\u001b[0m, std: 0.6368\n",
      "2021-05-12 23:54:51,168 - INFO - Epoch/batch: 4/ 755, ibatch: 18755, loss: \u001b[0;36m0.5428\u001b[0m, std: 0.6224\n",
      "2021-05-12 23:55:06,011 - INFO - loss: \u001b[0;32m0.5279\u001b[0m, std: 0.6012\n",
      "2021-05-12 23:55:06,029 - INFO - Saved model states in: earlystop_0.5279\n",
      "2021-05-12 23:55:06,031 - INFO - Saved net python code: earlystop_0.5279/paddle_nets.py\n",
      "2021-05-12 23:55:06,039 - INFO - Saved best model: earlystop_0.5279\n",
      "2021-05-12 23:55:06,039 - INFO - Removing earlystop model: earlystop_0.5285\n",
      "2021-05-12 23:55:06,275 - INFO - Epoch/batch: 4/ 906, ibatch: 18906, loss: \u001b[0;36m0.5278\u001b[0m, std: 0.6129\n",
      "2021-05-12 23:55:12,306 - INFO - Epoch/batch: 4/1057, ibatch: 19057, loss: \u001b[0;36m0.5375\u001b[0m, std: 0.6146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128: ReduceOnPlateau set learning rate to 0.00129140163.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:55:18,216 - INFO - Epoch/batch: 4/1208, ibatch: 19208, loss: \u001b[0;36m0.5346\u001b[0m, std: 0.6132\n",
      "2021-05-12 23:55:33,520 - INFO - loss: \u001b[0;32m0.5287\u001b[0m, std: 0.6228\n",
      "2021-05-12 23:55:33,855 - INFO - Epoch/batch: 4/1359, ibatch: 19359, loss: \u001b[0;36m0.5302\u001b[0m, std: 0.6208\n",
      "2021-05-12 23:55:39,411 - INFO - Epoch/batch: 4/1510, ibatch: 19510, loss: \u001b[0;36m0.5391\u001b[0m, std: 0.6123\n",
      "2021-05-12 23:55:44,988 - INFO - Epoch/batch: 4/1661, ibatch: 19661, loss: \u001b[0;36m0.5457\u001b[0m, std: 0.6171\n",
      "2021-05-12 23:56:00,222 - INFO - loss: \u001b[0;32m0.5299\u001b[0m, std: 0.6276\n",
      "2021-05-12 23:56:00,781 - INFO - Epoch/batch: 4/1812, ibatch: 19812, loss: \u001b[0;36m0.5245\u001b[0m, std: 0.6055\n",
      "2021-05-12 23:56:06,384 - INFO - Epoch/batch: 4/1963, ibatch: 19963, loss: \u001b[0;36m0.5322\u001b[0m, std: 0.6120\n",
      "2021-05-12 23:56:11,669 - INFO - Epoch/batch: 4/2114, ibatch: 20114, loss: \u001b[0;36m0.5117\u001b[0m, std: 0.5965\n",
      "2021-05-12 23:56:26,357 - INFO - loss: \u001b[0;32m0.5297\u001b[0m, std: 0.6286\n",
      "2021-05-12 23:56:26,973 - INFO - Epoch/batch: 4/2265, ibatch: 20265, loss: \u001b[0;36m0.5374\u001b[0m, std: 0.6138\n",
      "2021-05-12 23:56:32,648 - INFO - Epoch/batch: 4/2416, ibatch: 20416, loss: \u001b[0;36m0.5417\u001b[0m, std: 0.6125\n",
      "2021-05-12 23:56:38,121 - INFO - Epoch/batch: 4/2567, ibatch: 20567, loss: \u001b[0;36m0.5312\u001b[0m, std: 0.6130\n",
      "2021-05-12 23:56:52,820 - INFO - loss: \u001b[0;32m0.5280\u001b[0m, std: 0.5980\n",
      "2021-05-12 23:56:53,514 - INFO - Epoch/batch: 4/2718, ibatch: 20718, loss: \u001b[0;36m0.5342\u001b[0m, std: 0.6162\n",
      "2021-05-12 23:56:58,927 - INFO - Epoch/batch: 4/2869, ibatch: 20869, loss: \u001b[0;36m0.5177\u001b[0m, std: 0.6099\n",
      "2021-05-12 23:57:04,641 - INFO - Epoch/batch: 4/3020, ibatch: 21020, loss: \u001b[0;36m0.5582\u001b[0m, std: 0.6259\n",
      "2021-05-12 23:57:18,910 - INFO - loss: \u001b[0;32m0.5300\u001b[0m, std: 0.5902\n",
      "2021-05-12 23:57:19,648 - INFO - Epoch/batch: 4/3171, ibatch: 21171, loss: \u001b[0;36m0.5271\u001b[0m, std: 0.5985\n",
      "2021-05-12 23:57:25,349 - INFO - Epoch/batch: 4/3322, ibatch: 21322, loss: \u001b[0;36m0.5592\u001b[0m, std: 0.6357\n",
      "2021-05-12 23:57:31,221 - INFO - Epoch/batch: 4/3473, ibatch: 21473, loss: \u001b[0;36m0.5502\u001b[0m, std: 0.6284\n",
      "2021-05-12 23:57:45,690 - INFO - loss: \u001b[0;32m0.5281\u001b[0m, std: 0.6198\n",
      "2021-05-12 23:57:46,561 - INFO - Epoch/batch: 4/3624, ibatch: 21624, loss: \u001b[0;36m0.5255\u001b[0m, std: 0.6067\n",
      "2021-05-12 23:57:52,060 - INFO - Epoch/batch: 4/3775, ibatch: 21775, loss: \u001b[0;36m0.5507\u001b[0m, std: 0.6207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146: ReduceOnPlateau set learning rate to 0.001162261467.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:57:57,576 - INFO - Epoch/batch: 4/3926, ibatch: 21926, loss: \u001b[0;36m0.5337\u001b[0m, std: 0.6135\n",
      "2021-05-12 23:58:11,522 - INFO - loss: \u001b[0;32m0.5285\u001b[0m, std: 0.6255\n",
      "2021-05-12 23:58:12,462 - INFO - Epoch/batch: 4/4077, ibatch: 22077, loss: \u001b[0;36m0.5400\u001b[0m, std: 0.6168\n",
      "2021-05-12 23:58:18,278 - INFO - Epoch/batch: 4/4228, ibatch: 22228, loss: \u001b[0;36m0.5220\u001b[0m, std: 0.6005\n",
      "2021-05-12 23:58:24,385 - INFO - Epoch/batch: 4/4379, ibatch: 22379, loss: \u001b[0;36m0.5389\u001b[0m, std: 0.6214\n",
      "2021-05-12 23:58:38,410 - INFO - loss: \u001b[0;32m0.5274\u001b[0m, std: 0.5995\n",
      "2021-05-12 23:58:38,426 - INFO - Saved model states in: earlystop_0.5274\n",
      "2021-05-12 23:58:38,428 - INFO - Saved net python code: earlystop_0.5274/paddle_nets.py\n",
      "2021-05-12 23:58:38,435 - INFO - Saved best model: earlystop_0.5274\n",
      "2021-05-12 23:58:38,435 - INFO - Removing earlystop model: earlystop_0.5279\n",
      "2021-05-12 23:58:39,708 - INFO - Epoch 4 average training loss: \u001b[0;46m0.5364\u001b[0m std: 0.6153\n",
      "2021-05-12 23:58:39,712 - INFO - Epoch 4 average validate loss: \u001b[0;46m0.5287\u001b[0m std: 0.6120\n",
      "2021-05-12 23:58:41,779 - INFO - Epoch/batch: 5/   0, ibatch: 22500, loss: \u001b[0;36m0.5235\u001b[0m, std: 0.6112\n",
      "2021-05-12 23:58:51,523 - INFO - loss: \u001b[0;32m0.5277\u001b[0m, std: 0.5971\n",
      "2021-05-12 23:58:57,314 - INFO - Epoch/batch: 5/ 151, ibatch: 22651, loss: \u001b[0;36m0.5560\u001b[0m, std: 0.6263\n",
      "2021-05-12 23:59:03,065 - INFO - Epoch/batch: 5/ 302, ibatch: 22802, loss: \u001b[0;36m0.5547\u001b[0m, std: 0.6280\n",
      "2021-05-12 23:59:18,101 - INFO - loss: \u001b[0;32m0.5276\u001b[0m, std: 0.6074\n",
      "2021-05-12 23:59:18,209 - INFO - Epoch/batch: 5/ 453, ibatch: 22953, loss: \u001b[0;36m0.5357\u001b[0m, std: 0.6194\n",
      "2021-05-12 23:59:23,663 - INFO - Epoch/batch: 5/ 604, ibatch: 23104, loss: \u001b[0;36m0.5183\u001b[0m, std: 0.5974\n",
      "2021-05-12 23:59:29,297 - INFO - Epoch/batch: 5/ 755, ibatch: 23255, loss: \u001b[0;36m0.5244\u001b[0m, std: 0.6016\n",
      "2021-05-12 23:59:44,920 - INFO - loss: \u001b[0;32m0.5271\u001b[0m, std: 0.6116\n",
      "2021-05-12 23:59:44,937 - INFO - Saved model states in: earlystop_0.5271\n",
      "2021-05-12 23:59:44,939 - INFO - Saved net python code: earlystop_0.5271/paddle_nets.py\n",
      "2021-05-12 23:59:44,947 - INFO - Saved best model: earlystop_0.5271\n",
      "2021-05-12 23:59:44,947 - INFO - Removing earlystop model: earlystop_0.5274\n",
      "2021-05-12 23:59:45,227 - INFO - Epoch/batch: 5/ 906, ibatch: 23406, loss: \u001b[0;36m0.5304\u001b[0m, std: 0.6144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157: ReduceOnPlateau set learning rate to 0.0010460353203000001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:59:51,096 - INFO - Epoch/batch: 5/1057, ibatch: 23557, loss: \u001b[0;36m0.5337\u001b[0m, std: 0.6090\n",
      "2021-05-12 23:59:56,996 - INFO - Epoch/batch: 5/1208, ibatch: 23708, loss: \u001b[0;36m0.5254\u001b[0m, std: 0.6104\n",
      "2021-05-13 00:00:12,188 - INFO - loss: \u001b[0;32m0.5276\u001b[0m, std: 0.6115\n",
      "2021-05-13 00:00:12,607 - INFO - Epoch/batch: 5/1359, ibatch: 23859, loss: \u001b[0;36m0.5378\u001b[0m, std: 0.6192\n",
      "2021-05-13 00:00:18,059 - INFO - Epoch/batch: 5/1510, ibatch: 24010, loss: \u001b[0;36m0.5162\u001b[0m, std: 0.6083\n",
      "2021-05-13 00:00:24,042 - INFO - Epoch/batch: 5/1661, ibatch: 24161, loss: \u001b[0;36m0.5394\u001b[0m, std: 0.6181\n",
      "2021-05-13 00:00:38,868 - INFO - loss: \u001b[0;32m0.5299\u001b[0m, std: 0.5788\n",
      "2021-05-13 00:00:39,365 - INFO - Epoch/batch: 5/1812, ibatch: 24312, loss: \u001b[0;36m0.5562\u001b[0m, std: 0.6248\n",
      "2021-05-13 00:00:45,181 - INFO - Epoch/batch: 5/1963, ibatch: 24463, loss: \u001b[0;36m0.5412\u001b[0m, std: 0.6161\n",
      "2021-05-13 00:00:50,943 - INFO - Epoch/batch: 5/2114, ibatch: 24614, loss: \u001b[0;36m0.5493\u001b[0m, std: 0.6223\n",
      "2021-05-13 00:01:06,114 - INFO - loss: \u001b[0;32m0.5263\u001b[0m, std: 0.6098\n",
      "2021-05-13 00:01:06,138 - INFO - Saved model states in: earlystop_0.5263\n",
      "2021-05-13 00:01:06,140 - INFO - Saved net python code: earlystop_0.5263/paddle_nets.py\n",
      "2021-05-13 00:01:06,151 - INFO - Saved best model: earlystop_0.5263\n",
      "2021-05-13 00:01:06,152 - INFO - Removing earlystop model: earlystop_0.5271\n",
      "2021-05-13 00:01:06,699 - INFO - Epoch/batch: 5/2265, ibatch: 24765, loss: \u001b[0;36m0.5352\u001b[0m, std: 0.6135\n",
      "2021-05-13 00:01:12,095 - INFO - Epoch/batch: 5/2416, ibatch: 24916, loss: \u001b[0;36m0.5323\u001b[0m, std: 0.6088\n",
      "2021-05-13 00:01:17,743 - INFO - Epoch/batch: 5/2567, ibatch: 25067, loss: \u001b[0;36m0.5273\u001b[0m, std: 0.6114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168: ReduceOnPlateau set learning rate to 0.0009414317882700001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:01:32,665 - INFO - loss: \u001b[0;32m0.5275\u001b[0m, std: 0.6024\n",
      "2021-05-13 00:01:33,395 - INFO - Epoch/batch: 5/2718, ibatch: 25218, loss: \u001b[0;36m0.5457\u001b[0m, std: 0.6195\n",
      "2021-05-13 00:01:39,030 - INFO - Epoch/batch: 5/2869, ibatch: 25369, loss: \u001b[0;36m0.5239\u001b[0m, std: 0.5998\n",
      "2021-05-13 00:01:44,630 - INFO - Epoch/batch: 5/3020, ibatch: 25520, loss: \u001b[0;36m0.5239\u001b[0m, std: 0.6033\n",
      "2021-05-13 00:01:59,226 - INFO - loss: \u001b[0;32m0.5265\u001b[0m, std: 0.6178\n",
      "2021-05-13 00:02:00,081 - INFO - Epoch/batch: 5/3171, ibatch: 25671, loss: \u001b[0;36m0.5372\u001b[0m, std: 0.6221\n",
      "2021-05-13 00:02:05,627 - INFO - Epoch/batch: 5/3322, ibatch: 25822, loss: \u001b[0;36m0.5249\u001b[0m, std: 0.6114\n",
      "2021-05-13 00:02:11,183 - INFO - Epoch/batch: 5/3473, ibatch: 25973, loss: \u001b[0;36m0.5221\u001b[0m, std: 0.6072\n",
      "2021-05-13 00:02:26,001 - INFO - loss: \u001b[0;32m0.5259\u001b[0m, std: 0.6022\n",
      "2021-05-13 00:02:26,017 - INFO - Saved model states in: earlystop_0.5259\n",
      "2021-05-13 00:02:26,019 - INFO - Saved net python code: earlystop_0.5259/paddle_nets.py\n",
      "2021-05-13 00:02:26,025 - INFO - Saved best model: earlystop_0.5259\n",
      "2021-05-13 00:02:26,026 - INFO - Removing earlystop model: earlystop_0.5263\n",
      "2021-05-13 00:02:26,974 - INFO - Epoch/batch: 5/3624, ibatch: 26124, loss: \u001b[0;36m0.5491\u001b[0m, std: 0.6224\n",
      "2021-05-13 00:02:32,961 - INFO - Epoch/batch: 5/3775, ibatch: 26275, loss: \u001b[0;36m0.5314\u001b[0m, std: 0.6127\n",
      "2021-05-13 00:02:38,815 - INFO - Epoch/batch: 5/3926, ibatch: 26426, loss: \u001b[0;36m0.5206\u001b[0m, std: 0.5964\n",
      "2021-05-13 00:02:53,332 - INFO - loss: \u001b[0;32m0.5262\u001b[0m, std: 0.6089\n",
      "2021-05-13 00:02:54,349 - INFO - Epoch/batch: 5/4077, ibatch: 26577, loss: \u001b[0;36m0.5243\u001b[0m, std: 0.6118\n",
      "2021-05-13 00:03:00,073 - INFO - Epoch/batch: 5/4228, ibatch: 26728, loss: \u001b[0;36m0.5363\u001b[0m, std: 0.6203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179: ReduceOnPlateau set learning rate to 0.0008472886094430002.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:03:06,050 - INFO - Epoch/batch: 5/4379, ibatch: 26879, loss: \u001b[0;36m0.5440\u001b[0m, std: 0.6207\n",
      "2021-05-13 00:03:20,487 - INFO - loss: \u001b[0;32m0.5262\u001b[0m, std: 0.6102\n",
      "2021-05-13 00:03:21,669 - INFO - Epoch 5 average training loss: \u001b[0;46m0.5345\u001b[0m std: 0.6137\n",
      "2021-05-13 00:03:21,711 - INFO - Epoch 5 average validate loss: \u001b[0;46m0.5271\u001b[0m std: 0.6052\n",
      "2021-05-13 00:03:23,576 - INFO - Epoch/batch: 6/   0, ibatch: 27000, loss: \u001b[0;36m0.5378\u001b[0m, std: 0.6141\n",
      "2021-05-13 00:03:33,212 - INFO - loss: \u001b[0;32m0.5262\u001b[0m, std: 0.6107\n",
      "2021-05-13 00:03:38,842 - INFO - Epoch/batch: 6/ 151, ibatch: 27151, loss: \u001b[0;36m0.5322\u001b[0m, std: 0.6126\n",
      "2021-05-13 00:03:44,509 - INFO - Epoch/batch: 6/ 302, ibatch: 27302, loss: \u001b[0;36m0.5281\u001b[0m, std: 0.6105\n",
      "2021-05-13 00:03:59,992 - INFO - loss: \u001b[0;32m0.5267\u001b[0m, std: 0.5979\n",
      "2021-05-13 00:04:00,121 - INFO - Epoch/batch: 6/ 453, ibatch: 27453, loss: \u001b[0;36m0.5474\u001b[0m, std: 0.6248\n",
      "2021-05-13 00:04:05,624 - INFO - Epoch/batch: 6/ 604, ibatch: 27604, loss: \u001b[0;36m0.5323\u001b[0m, std: 0.6093\n",
      "2021-05-13 00:04:11,246 - INFO - Epoch/batch: 6/ 755, ibatch: 27755, loss: \u001b[0;36m0.5321\u001b[0m, std: 0.6038\n",
      "2021-05-13 00:04:26,478 - INFO - loss: \u001b[0;32m0.5264\u001b[0m, std: 0.6007\n",
      "2021-05-13 00:04:26,717 - INFO - Epoch/batch: 6/ 906, ibatch: 27906, loss: \u001b[0;36m0.5291\u001b[0m, std: 0.6063\n",
      "2021-05-13 00:04:32,313 - INFO - Epoch/batch: 6/1057, ibatch: 28057, loss: \u001b[0;36m0.5211\u001b[0m, std: 0.5974\n",
      "2021-05-13 00:04:38,030 - INFO - Epoch/batch: 6/1208, ibatch: 28208, loss: \u001b[0;36m0.5333\u001b[0m, std: 0.6172\n",
      "2021-05-13 00:04:53,827 - INFO - loss: \u001b[0;32m0.5262\u001b[0m, std: 0.5947\n",
      "2021-05-13 00:04:54,228 - INFO - Epoch/batch: 6/1359, ibatch: 28359, loss: \u001b[0;36m0.5457\u001b[0m, std: 0.6266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190: ReduceOnPlateau set learning rate to 0.0007625597484987002.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:05:00,835 - INFO - Epoch/batch: 6/1510, ibatch: 28510, loss: \u001b[0;36m0.5438\u001b[0m, std: 0.6166\n",
      "2021-05-13 00:05:07,511 - INFO - Epoch/batch: 6/1661, ibatch: 28661, loss: \u001b[0;36m0.5366\u001b[0m, std: 0.6135\n",
      "2021-05-13 00:05:22,590 - INFO - loss: \u001b[0;32m0.5284\u001b[0m, std: 0.6358\n",
      "2021-05-13 00:05:23,051 - INFO - Epoch/batch: 6/1812, ibatch: 28812, loss: \u001b[0;36m0.5207\u001b[0m, std: 0.6035\n",
      "2021-05-13 00:05:28,726 - INFO - Epoch/batch: 6/1963, ibatch: 28963, loss: \u001b[0;36m0.5305\u001b[0m, std: 0.6102\n",
      "2021-05-13 00:05:34,317 - INFO - Epoch/batch: 6/2114, ibatch: 29114, loss: \u001b[0;36m0.5261\u001b[0m, std: 0.6116\n",
      "2021-05-13 00:05:49,382 - INFO - loss: \u001b[0;32m0.5262\u001b[0m, std: 0.6010\n",
      "2021-05-13 00:05:49,960 - INFO - Epoch/batch: 6/2265, ibatch: 29265, loss: \u001b[0;36m0.5277\u001b[0m, std: 0.6077\n",
      "2021-05-13 00:05:56,388 - INFO - Epoch/batch: 6/2416, ibatch: 29416, loss: \u001b[0;36m0.5196\u001b[0m, std: 0.6059\n",
      "2021-05-13 00:06:02,762 - INFO - Epoch/batch: 6/2567, ibatch: 29567, loss: \u001b[0;36m0.5224\u001b[0m, std: 0.6047\n",
      "2021-05-13 00:06:17,789 - INFO - loss: \u001b[0;32m0.5254\u001b[0m, std: 0.6049\n",
      "2021-05-13 00:06:17,805 - INFO - Saved model states in: earlystop_0.5254\n",
      "2021-05-13 00:06:17,807 - INFO - Saved net python code: earlystop_0.5254/paddle_nets.py\n",
      "2021-05-13 00:06:17,815 - INFO - Saved best model: earlystop_0.5254\n",
      "2021-05-13 00:06:17,816 - INFO - Removing earlystop model: earlystop_0.5259\n",
      "2021-05-13 00:06:18,493 - INFO - Epoch/batch: 6/2718, ibatch: 29718, loss: \u001b[0;36m0.5471\u001b[0m, std: 0.6258\n",
      "2021-05-13 00:06:24,053 - INFO - Epoch/batch: 6/2869, ibatch: 29869, loss: \u001b[0;36m0.5354\u001b[0m, std: 0.6061\n",
      "2021-05-13 00:06:29,971 - INFO - Epoch/batch: 6/3020, ibatch: 30020, loss: \u001b[0;36m0.5655\u001b[0m, std: 0.6338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201: ReduceOnPlateau set learning rate to 0.0006863037736488302.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:06:44,479 - INFO - loss: \u001b[0;32m0.5266\u001b[0m, std: 0.6221\n",
      "2021-05-13 00:06:45,340 - INFO - Epoch/batch: 6/3171, ibatch: 30171, loss: \u001b[0;36m0.5230\u001b[0m, std: 0.5964\n",
      "2021-05-13 00:06:51,243 - INFO - Epoch/batch: 6/3322, ibatch: 30322, loss: \u001b[0;36m0.5300\u001b[0m, std: 0.6162\n",
      "2021-05-13 00:06:57,054 - INFO - Epoch/batch: 6/3473, ibatch: 30473, loss: \u001b[0;36m0.5368\u001b[0m, std: 0.6102\n",
      "2021-05-13 00:07:11,288 - INFO - loss: \u001b[0;32m0.5261\u001b[0m, std: 0.6108\n",
      "2021-05-13 00:07:12,219 - INFO - Epoch/batch: 6/3624, ibatch: 30624, loss: \u001b[0;36m0.5231\u001b[0m, std: 0.6029\n",
      "2021-05-13 00:07:18,075 - INFO - Epoch/batch: 6/3775, ibatch: 30775, loss: \u001b[0;36m0.5430\u001b[0m, std: 0.6242\n",
      "2021-05-13 00:07:23,689 - INFO - Epoch/batch: 6/3926, ibatch: 30926, loss: \u001b[0;36m0.5319\u001b[0m, std: 0.6139\n",
      "2021-05-13 00:07:38,163 - INFO - loss: \u001b[0;32m0.5276\u001b[0m, std: 0.6307\n",
      "2021-05-13 00:07:39,121 - INFO - Epoch/batch: 6/4077, ibatch: 31077, loss: \u001b[0;36m0.5268\u001b[0m, std: 0.6022\n",
      "2021-05-13 00:07:44,930 - INFO - Epoch/batch: 6/4228, ibatch: 31228, loss: \u001b[0;36m0.5331\u001b[0m, std: 0.6135\n",
      "2021-05-13 00:07:50,496 - INFO - Epoch/batch: 6/4379, ibatch: 31379, loss: \u001b[0;36m0.5232\u001b[0m, std: 0.6028\n",
      "2021-05-13 00:08:04,672 - INFO - loss: \u001b[0;32m0.5257\u001b[0m, std: 0.6063\n",
      "2021-05-13 00:08:05,926 - INFO - Epoch 6 average training loss: \u001b[0;46m0.5328\u001b[0m std: 0.6117\n",
      "2021-05-13 00:08:05,930 - INFO - Epoch 6 average validate loss: \u001b[0;46m0.5265\u001b[0m std: 0.6105\n",
      "2021-05-13 00:08:07,857 - INFO - Epoch/batch: 7/   0, ibatch: 31500, loss: \u001b[0;36m0.5384\u001b[0m, std: 0.6250\n",
      "2021-05-13 00:08:18,035 - INFO - loss: \u001b[0;32m0.5257\u001b[0m, std: 0.6049\n",
      "2021-05-13 00:08:24,294 - INFO - Epoch/batch: 7/ 151, ibatch: 31651, loss: \u001b[0;36m0.5372\u001b[0m, std: 0.6188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 212: ReduceOnPlateau set learning rate to 0.0006176733962839472.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:08:30,221 - INFO - Epoch/batch: 7/ 302, ibatch: 31802, loss: \u001b[0;36m0.5374\u001b[0m, std: 0.6177\n",
      "2021-05-13 00:08:46,073 - INFO - loss: \u001b[0;32m0.5259\u001b[0m, std: 0.6058\n",
      "2021-05-13 00:08:46,171 - INFO - Epoch/batch: 7/ 453, ibatch: 31953, loss: \u001b[0;36m0.5431\u001b[0m, std: 0.6134\n",
      "2021-05-13 00:08:51,946 - INFO - Epoch/batch: 7/ 604, ibatch: 32104, loss: \u001b[0;36m0.5214\u001b[0m, std: 0.5979\n",
      "2021-05-13 00:08:57,810 - INFO - Epoch/batch: 7/ 755, ibatch: 32255, loss: \u001b[0;36m0.5394\u001b[0m, std: 0.6224\n",
      "2021-05-13 00:09:13,069 - INFO - loss: \u001b[0;32m0.5258\u001b[0m, std: 0.6143\n",
      "2021-05-13 00:09:13,459 - INFO - Epoch/batch: 7/ 906, ibatch: 32406, loss: \u001b[0;36m0.5345\u001b[0m, std: 0.6121\n",
      "2021-05-13 00:09:19,353 - INFO - Epoch/batch: 7/1057, ibatch: 32557, loss: \u001b[0;36m0.5363\u001b[0m, std: 0.6100\n",
      "2021-05-13 00:09:24,983 - INFO - Epoch/batch: 7/1208, ibatch: 32708, loss: \u001b[0;36m0.5129\u001b[0m, std: 0.6035\n",
      "2021-05-13 00:09:40,362 - INFO - loss: \u001b[0;32m0.5254\u001b[0m, std: 0.5930\n",
      "2021-05-13 00:09:40,698 - INFO - Epoch/batch: 7/1359, ibatch: 32859, loss: \u001b[0;36m0.5293\u001b[0m, std: 0.6205\n",
      "2021-05-13 00:09:46,445 - INFO - Epoch/batch: 7/1510, ibatch: 33010, loss: \u001b[0;36m0.5248\u001b[0m, std: 0.6059\n",
      "2021-05-13 00:09:51,997 - INFO - Epoch/batch: 7/1661, ibatch: 33161, loss: \u001b[0;36m0.5122\u001b[0m, std: 0.5972\n",
      "2021-05-13 00:10:06,954 - INFO - loss: \u001b[0;32m0.5257\u001b[0m, std: 0.6097\n",
      "2021-05-13 00:10:07,434 - INFO - Epoch/batch: 7/1812, ibatch: 33312, loss: \u001b[0;36m0.5166\u001b[0m, std: 0.5976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223: ReduceOnPlateau set learning rate to 0.0005559060566555524.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:10:13,018 - INFO - Epoch/batch: 7/1963, ibatch: 33463, loss: \u001b[0;36m0.5368\u001b[0m, std: 0.6184\n",
      "2021-05-13 00:10:18,504 - INFO - Epoch/batch: 7/2114, ibatch: 33614, loss: \u001b[0;36m0.5105\u001b[0m, std: 0.5957\n",
      "2021-05-13 00:10:33,902 - INFO - loss: \u001b[0;32m0.5250\u001b[0m, std: 0.6094\n",
      "2021-05-13 00:10:33,925 - INFO - Saved model states in: earlystop_0.5250\n",
      "2021-05-13 00:10:33,927 - INFO - Saved net python code: earlystop_0.5250/paddle_nets.py\n",
      "2021-05-13 00:10:33,935 - INFO - Saved best model: earlystop_0.5250\n",
      "2021-05-13 00:10:33,936 - INFO - Removing earlystop model: earlystop_0.5254\n",
      "2021-05-13 00:10:34,558 - INFO - Epoch/batch: 7/2265, ibatch: 33765, loss: \u001b[0;36m0.5386\u001b[0m, std: 0.6039\n",
      "2021-05-13 00:10:40,233 - INFO - Epoch/batch: 7/2416, ibatch: 33916, loss: \u001b[0;36m0.5600\u001b[0m, std: 0.6214\n",
      "2021-05-13 00:10:46,090 - INFO - Epoch/batch: 7/2567, ibatch: 34067, loss: \u001b[0;36m0.5371\u001b[0m, std: 0.6211\n",
      "2021-05-13 00:11:01,117 - INFO - loss: \u001b[0;32m0.5252\u001b[0m, std: 0.6075\n",
      "2021-05-13 00:11:01,888 - INFO - Epoch/batch: 7/2718, ibatch: 34218, loss: \u001b[0;36m0.5406\u001b[0m, std: 0.6163\n",
      "2021-05-13 00:11:07,592 - INFO - Epoch/batch: 7/2869, ibatch: 34369, loss: \u001b[0;36m0.5315\u001b[0m, std: 0.6015\n",
      "2021-05-13 00:11:13,402 - INFO - Epoch/batch: 7/3020, ibatch: 34520, loss: \u001b[0;36m0.5569\u001b[0m, std: 0.6365\n",
      "2021-05-13 00:11:28,816 - INFO - loss: \u001b[0;32m0.5261\u001b[0m, std: 0.6174\n",
      "2021-05-13 00:11:29,580 - INFO - Epoch/batch: 7/3171, ibatch: 34671, loss: \u001b[0;36m0.5403\u001b[0m, std: 0.6197\n",
      "2021-05-13 00:11:35,540 - INFO - Epoch/batch: 7/3322, ibatch: 34822, loss: \u001b[0;36m0.5396\u001b[0m, std: 0.6074\n",
      "2021-05-13 00:11:41,518 - INFO - Epoch/batch: 7/3473, ibatch: 34973, loss: \u001b[0;36m0.5284\u001b[0m, std: 0.6049\n",
      "2021-05-13 00:11:56,360 - INFO - loss: \u001b[0;32m0.5263\u001b[0m, std: 0.6281\n",
      "2021-05-13 00:11:57,274 - INFO - Epoch/batch: 7/3624, ibatch: 35124, loss: \u001b[0;36m0.5270\u001b[0m, std: 0.6126\n",
      "2021-05-13 00:12:02,836 - INFO - Epoch/batch: 7/3775, ibatch: 35275, loss: \u001b[0;36m0.5344\u001b[0m, std: 0.6122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236: ReduceOnPlateau set learning rate to 0.0005003154509899972.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:12:08,483 - INFO - Epoch/batch: 7/3926, ibatch: 35426, loss: \u001b[0;36m0.5347\u001b[0m, std: 0.6143\n",
      "2021-05-13 00:12:22,933 - INFO - loss: \u001b[0;32m0.5273\u001b[0m, std: 0.5855\n",
      "2021-05-13 00:12:23,997 - INFO - Epoch/batch: 7/4077, ibatch: 35577, loss: \u001b[0;36m0.5327\u001b[0m, std: 0.6161\n",
      "2021-05-13 00:12:29,886 - INFO - Epoch/batch: 7/4228, ibatch: 35728, loss: \u001b[0;36m0.5250\u001b[0m, std: 0.6037\n",
      "2021-05-13 00:12:36,369 - INFO - Epoch/batch: 7/4379, ibatch: 35879, loss: \u001b[0;36m0.5090\u001b[0m, std: 0.5892\n",
      "2021-05-13 00:12:51,119 - INFO - loss: \u001b[0;32m0.5254\u001b[0m, std: 0.6082\n",
      "2021-05-13 00:12:52,511 - INFO - Epoch 7 average training loss: \u001b[0;46m0.5316\u001b[0m std: 0.6104\n",
      "2021-05-13 00:12:52,516 - INFO - Epoch 7 average validate loss: \u001b[0;46m0.5258\u001b[0m std: 0.6076\n",
      "2021-05-13 00:12:54,534 - INFO - Epoch/batch: 8/   0, ibatch: 36000, loss: \u001b[0;36m0.5165\u001b[0m, std: 0.6003\n",
      "2021-05-13 00:13:04,445 - INFO - loss: \u001b[0;32m0.5254\u001b[0m, std: 0.6088\n",
      "2021-05-13 00:13:10,225 - INFO - Epoch/batch: 8/ 151, ibatch: 36151, loss: \u001b[0;36m0.5391\u001b[0m, std: 0.6181\n",
      "2021-05-13 00:13:16,953 - INFO - Epoch/batch: 8/ 302, ibatch: 36302, loss: \u001b[0;36m0.5276\u001b[0m, std: 0.6058\n",
      "2021-05-13 00:13:32,796 - INFO - loss: \u001b[0;32m0.5247\u001b[0m, std: 0.6066\n",
      "2021-05-13 00:13:32,814 - INFO - Saved model states in: earlystop_0.5247\n",
      "2021-05-13 00:13:32,817 - INFO - Saved net python code: earlystop_0.5247/paddle_nets.py\n",
      "2021-05-13 00:13:32,826 - INFO - Saved best model: earlystop_0.5247\n",
      "2021-05-13 00:13:32,827 - INFO - Removing earlystop model: earlystop_0.5250\n",
      "2021-05-13 00:13:32,932 - INFO - Epoch/batch: 8/ 453, ibatch: 36453, loss: \u001b[0;36m0.5220\u001b[0m, std: 0.6034\n",
      "2021-05-13 00:13:39,068 - INFO - Epoch/batch: 8/ 604, ibatch: 36604, loss: \u001b[0;36m0.5380\u001b[0m, std: 0.6141\n",
      "2021-05-13 00:13:44,924 - INFO - Epoch/batch: 8/ 755, ibatch: 36755, loss: \u001b[0;36m0.5341\u001b[0m, std: 0.6070\n",
      "2021-05-13 00:14:00,768 - INFO - loss: \u001b[0;32m0.5257\u001b[0m, std: 0.5922\n",
      "2021-05-13 00:14:00,974 - INFO - Epoch/batch: 8/ 906, ibatch: 36906, loss: \u001b[0;36m0.5530\u001b[0m, std: 0.6288\n",
      "2021-05-13 00:14:06,914 - INFO - Epoch/batch: 8/1057, ibatch: 37057, loss: \u001b[0;36m0.5366\u001b[0m, std: 0.6154\n",
      "2021-05-13 00:14:12,373 - INFO - Epoch/batch: 8/1208, ibatch: 37208, loss: \u001b[0;36m0.5117\u001b[0m, std: 0.5982\n",
      "2021-05-13 00:14:27,322 - INFO - loss: \u001b[0;32m0.5259\u001b[0m, std: 0.6030\n",
      "2021-05-13 00:14:27,592 - INFO - Epoch/batch: 8/1359, ibatch: 37359, loss: \u001b[0;36m0.5360\u001b[0m, std: 0.6134\n",
      "2021-05-13 00:14:33,100 - INFO - Epoch/batch: 8/1510, ibatch: 37510, loss: \u001b[0;36m0.5145\u001b[0m, std: 0.5972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 251: ReduceOnPlateau set learning rate to 0.00045028390589099747.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:14:38,566 - INFO - Epoch/batch: 8/1661, ibatch: 37661, loss: \u001b[0;36m0.5284\u001b[0m, std: 0.6158\n",
      "2021-05-13 00:14:53,236 - INFO - loss: \u001b[0;32m0.5256\u001b[0m, std: 0.5945\n",
      "2021-05-13 00:14:53,708 - INFO - Epoch/batch: 8/1812, ibatch: 37812, loss: \u001b[0;36m0.5406\u001b[0m, std: 0.6183\n",
      "2021-05-13 00:14:59,297 - INFO - Epoch/batch: 8/1963, ibatch: 37963, loss: \u001b[0;36m0.5218\u001b[0m, std: 0.5954\n",
      "2021-05-13 00:15:05,021 - INFO - Epoch/batch: 8/2114, ibatch: 38114, loss: \u001b[0;36m0.5294\u001b[0m, std: 0.6097\n",
      "2021-05-13 00:15:20,672 - INFO - loss: \u001b[0;32m0.5254\u001b[0m, std: 0.5976\n",
      "2021-05-13 00:15:21,335 - INFO - Epoch/batch: 8/2265, ibatch: 38265, loss: \u001b[0;36m0.5431\u001b[0m, std: 0.6169\n",
      "2021-05-13 00:15:27,062 - INFO - Epoch/batch: 8/2416, ibatch: 38416, loss: \u001b[0;36m0.5279\u001b[0m, std: 0.5995\n",
      "2021-05-13 00:15:32,660 - INFO - Epoch/batch: 8/2567, ibatch: 38567, loss: \u001b[0;36m0.5176\u001b[0m, std: 0.6018\n",
      "2021-05-13 00:15:47,425 - INFO - loss: \u001b[0;32m0.5248\u001b[0m, std: 0.6066\n",
      "2021-05-13 00:15:48,133 - INFO - Epoch/batch: 8/2718, ibatch: 38718, loss: \u001b[0;36m0.5374\u001b[0m, std: 0.6183\n",
      "2021-05-13 00:15:53,903 - INFO - Epoch/batch: 8/2869, ibatch: 38869, loss: \u001b[0;36m0.5347\u001b[0m, std: 0.6141\n",
      "2021-05-13 00:15:59,350 - INFO - Epoch/batch: 8/3020, ibatch: 39020, loss: \u001b[0;36m0.5187\u001b[0m, std: 0.6040\n",
      "2021-05-13 00:16:13,917 - INFO - loss: \u001b[0;32m0.5249\u001b[0m, std: 0.6102\n",
      "2021-05-13 00:16:14,762 - INFO - Epoch/batch: 8/3171, ibatch: 39171, loss: \u001b[0;36m0.5329\u001b[0m, std: 0.6102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 262: ReduceOnPlateau set learning rate to 0.0004052555153018977.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:16:20,430 - INFO - Epoch/batch: 8/3322, ibatch: 39322, loss: \u001b[0;36m0.5374\u001b[0m, std: 0.6225\n",
      "2021-05-13 00:16:26,404 - INFO - Epoch/batch: 8/3473, ibatch: 39473, loss: \u001b[0;36m0.5368\u001b[0m, std: 0.6218\n",
      "2021-05-13 00:16:41,621 - INFO - loss: \u001b[0;32m0.5244\u001b[0m, std: 0.5992\n",
      "2021-05-13 00:16:41,637 - INFO - Saved model states in: earlystop_0.5244\n",
      "2021-05-13 00:16:41,638 - INFO - Saved net python code: earlystop_0.5244/paddle_nets.py\n",
      "2021-05-13 00:16:41,645 - INFO - Saved best model: earlystop_0.5244\n",
      "2021-05-13 00:16:41,646 - INFO - Removing earlystop model: earlystop_0.5247\n",
      "2021-05-13 00:16:42,584 - INFO - Epoch/batch: 8/3624, ibatch: 39624, loss: \u001b[0;36m0.5304\u001b[0m, std: 0.6076\n",
      "2021-05-13 00:16:48,274 - INFO - Epoch/batch: 8/3775, ibatch: 39775, loss: \u001b[0;36m0.5320\u001b[0m, std: 0.6046\n",
      "2021-05-13 00:16:53,942 - INFO - Epoch/batch: 8/3926, ibatch: 39926, loss: \u001b[0;36m0.5384\u001b[0m, std: 0.6122\n",
      "2021-05-13 00:17:08,780 - INFO - loss: \u001b[0;32m0.5256\u001b[0m, std: 0.6207\n",
      "2021-05-13 00:17:09,781 - INFO - Epoch/batch: 8/4077, ibatch: 40077, loss: \u001b[0;36m0.5059\u001b[0m, std: 0.5903\n",
      "2021-05-13 00:17:15,608 - INFO - Epoch/batch: 8/4228, ibatch: 40228, loss: \u001b[0;36m0.5414\u001b[0m, std: 0.6229\n",
      "2021-05-13 00:17:21,105 - INFO - Epoch/batch: 8/4379, ibatch: 40379, loss: \u001b[0;36m0.5214\u001b[0m, std: 0.5969\n",
      "2021-05-13 00:17:35,273 - INFO - loss: \u001b[0;32m0.5250\u001b[0m, std: 0.6098\n",
      "2021-05-13 00:17:36,608 - INFO - Epoch 8 average training loss: \u001b[0;46m0.5308\u001b[0m std: 0.6098\n",
      "2021-05-13 00:17:36,614 - INFO - Epoch 8 average validate loss: \u001b[0;46m0.5252\u001b[0m std: 0.6045\n",
      "2021-05-13 00:17:38,551 - INFO - Epoch/batch: 9/   0, ibatch: 40500, loss: \u001b[0;36m0.5361\u001b[0m, std: 0.6081\n",
      "2021-05-13 00:17:48,537 - INFO - loss: \u001b[0;32m0.5249\u001b[0m, std: 0.6088\n",
      "2021-05-13 00:17:54,562 - INFO - Epoch/batch: 9/ 151, ibatch: 40651, loss: \u001b[0;36m0.5379\u001b[0m, std: 0.6122\n",
      "2021-05-13 00:18:00,284 - INFO - Epoch/batch: 9/ 302, ibatch: 40802, loss: \u001b[0;36m0.5392\u001b[0m, std: 0.6249\n",
      "2021-05-13 00:18:15,665 - INFO - loss: \u001b[0;32m0.5251\u001b[0m, std: 0.6114\n",
      "2021-05-13 00:18:15,788 - INFO - Epoch/batch: 9/ 453, ibatch: 40953, loss: \u001b[0;36m0.5340\u001b[0m, std: 0.6064\n",
      "2021-05-13 00:18:21,217 - INFO - Epoch/batch: 9/ 604, ibatch: 41104, loss: \u001b[0;36m0.5132\u001b[0m, std: 0.5987\n",
      "2021-05-13 00:18:26,950 - INFO - Epoch/batch: 9/ 755, ibatch: 41255, loss: \u001b[0;36m0.5199\u001b[0m, std: 0.6003\n",
      "2021-05-13 00:18:42,066 - INFO - loss: \u001b[0;32m0.5260\u001b[0m, std: 0.6199\n",
      "2021-05-13 00:18:42,312 - INFO - Epoch/batch: 9/ 906, ibatch: 41406, loss: \u001b[0;36m0.5219\u001b[0m, std: 0.6034\n",
      "2021-05-13 00:18:48,048 - INFO - Epoch/batch: 9/1057, ibatch: 41557, loss: \u001b[0;36m0.5278\u001b[0m, std: 0.6090\n",
      "2021-05-13 00:18:53,490 - INFO - Epoch/batch: 9/1208, ibatch: 41708, loss: \u001b[0;36m0.5414\u001b[0m, std: 0.6233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 279: ReduceOnPlateau set learning rate to 0.00036472996377170795.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:19:08,877 - INFO - loss: \u001b[0;32m0.5244\u001b[0m, std: 0.6018\n",
      "2021-05-13 00:19:08,895 - INFO - Saved model states in: earlystop_0.5244.1\n",
      "2021-05-13 00:19:08,896 - INFO - Saved net python code: earlystop_0.5244.1/paddle_nets.py\n",
      "2021-05-13 00:19:08,904 - INFO - Saved best model: earlystop_0.5244.1\n",
      "2021-05-13 00:19:08,905 - INFO - Removing earlystop model: earlystop_0.5244\n",
      "2021-05-13 00:19:09,271 - INFO - Epoch/batch: 9/1359, ibatch: 41859, loss: \u001b[0;36m0.5181\u001b[0m, std: 0.6017\n",
      "2021-05-13 00:19:15,276 - INFO - Epoch/batch: 9/1510, ibatch: 42010, loss: \u001b[0;36m0.5521\u001b[0m, std: 0.6284\n",
      "2021-05-13 00:19:21,106 - INFO - Epoch/batch: 9/1661, ibatch: 42161, loss: \u001b[0;36m0.5266\u001b[0m, std: 0.6060\n",
      "2021-05-13 00:19:36,188 - INFO - loss: \u001b[0;32m0.5259\u001b[0m, std: 0.5931\n",
      "2021-05-13 00:19:36,675 - INFO - Epoch/batch: 9/1812, ibatch: 42312, loss: \u001b[0;36m0.5452\u001b[0m, std: 0.6202\n",
      "2021-05-13 00:19:42,269 - INFO - Epoch/batch: 9/1963, ibatch: 42463, loss: \u001b[0;36m0.5166\u001b[0m, std: 0.6012\n",
      "2021-05-13 00:19:47,841 - INFO - Epoch/batch: 9/2114, ibatch: 42614, loss: \u001b[0;36m0.5292\u001b[0m, std: 0.6102\n",
      "2021-05-13 00:20:02,465 - INFO - loss: \u001b[0;32m0.5245\u001b[0m, std: 0.5946\n",
      "2021-05-13 00:20:03,035 - INFO - Epoch/batch: 9/2265, ibatch: 42765, loss: \u001b[0;36m0.5184\u001b[0m, std: 0.5980\n",
      "2021-05-13 00:20:08,636 - INFO - Epoch/batch: 9/2416, ibatch: 42916, loss: \u001b[0;36m0.5495\u001b[0m, std: 0.6191\n",
      "2021-05-13 00:20:14,251 - INFO - Epoch/batch: 9/2567, ibatch: 43067, loss: \u001b[0;36m0.5235\u001b[0m, std: 0.6033\n",
      "2021-05-13 00:20:29,278 - INFO - loss: \u001b[0;32m0.5242\u001b[0m, std: 0.6101\n",
      "2021-05-13 00:20:29,293 - INFO - Saved model states in: earlystop_0.5242\n",
      "2021-05-13 00:20:29,294 - INFO - Saved net python code: earlystop_0.5242/paddle_nets.py\n",
      "2021-05-13 00:20:29,300 - INFO - Saved best model: earlystop_0.5242\n",
      "2021-05-13 00:20:29,301 - INFO - Removing earlystop model: earlystop_0.5244.1\n",
      "2021-05-13 00:20:29,950 - INFO - Epoch/batch: 9/2718, ibatch: 43218, loss: \u001b[0;36m0.5110\u001b[0m, std: 0.5955\n",
      "2021-05-13 00:20:35,775 - INFO - Epoch/batch: 9/2869, ibatch: 43369, loss: \u001b[0;36m0.5511\u001b[0m, std: 0.6182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 290: ReduceOnPlateau set learning rate to 0.00032825696739453717.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:20:41,509 - INFO - Epoch/batch: 9/3020, ibatch: 43520, loss: \u001b[0;36m0.5289\u001b[0m, std: 0.6071\n",
      "2021-05-13 00:20:56,197 - INFO - loss: \u001b[0;32m0.5244\u001b[0m, std: 0.6011\n",
      "2021-05-13 00:20:57,057 - INFO - Epoch/batch: 9/3171, ibatch: 43671, loss: \u001b[0;36m0.5282\u001b[0m, std: 0.5991\n",
      "2021-05-13 00:21:03,127 - INFO - Epoch/batch: 9/3322, ibatch: 43822, loss: \u001b[0;36m0.5237\u001b[0m, std: 0.6113\n",
      "2021-05-13 00:21:08,845 - INFO - Epoch/batch: 9/3473, ibatch: 43973, loss: \u001b[0;36m0.5250\u001b[0m, std: 0.5992\n",
      "2021-05-13 00:21:22,983 - INFO - loss: \u001b[0;32m0.5241\u001b[0m, std: 0.6107\n",
      "2021-05-13 00:21:22,998 - INFO - Saved model states in: earlystop_0.5241\n",
      "2021-05-13 00:21:23,008 - INFO - Saved net python code: earlystop_0.5241/paddle_nets.py\n",
      "2021-05-13 00:21:23,014 - INFO - Saved best model: earlystop_0.5241\n",
      "2021-05-13 00:21:23,015 - INFO - Removing earlystop model: earlystop_0.5242\n",
      "2021-05-13 00:21:24,114 - INFO - Epoch/batch: 9/3624, ibatch: 44124, loss: \u001b[0;36m0.5110\u001b[0m, std: 0.5998\n",
      "2021-05-13 00:21:29,882 - INFO - Epoch/batch: 9/3775, ibatch: 44275, loss: \u001b[0;36m0.5308\u001b[0m, std: 0.6039\n",
      "2021-05-13 00:21:35,771 - INFO - Epoch/batch: 9/3926, ibatch: 44426, loss: \u001b[0;36m0.5431\u001b[0m, std: 0.6129\n",
      "2021-05-13 00:21:50,802 - INFO - loss: \u001b[0;32m0.5242\u001b[0m, std: 0.5970\n",
      "2021-05-13 00:21:51,875 - INFO - Epoch/batch: 9/4077, ibatch: 44577, loss: \u001b[0;36m0.5430\u001b[0m, std: 0.6202\n",
      "2021-05-13 00:21:57,334 - INFO - Epoch/batch: 9/4228, ibatch: 44728, loss: \u001b[0;36m0.5114\u001b[0m, std: 0.5962\n",
      "2021-05-13 00:22:03,013 - INFO - Epoch/batch: 9/4379, ibatch: 44879, loss: \u001b[0;36m0.5303\u001b[0m, std: 0.6084\n",
      "2021-05-13 00:22:17,278 - INFO - loss: \u001b[0;32m0.5259\u001b[0m, std: 0.5880\n",
      "2021-05-13 00:22:18,608 - INFO - Epoch 9 average training loss: \u001b[0;46m0.5299\u001b[0m std: 0.6088\n",
      "2021-05-13 00:22:18,613 - INFO - Epoch 9 average validate loss: \u001b[0;46m0.5249\u001b[0m std: 0.6033\n",
      "2021-05-13 00:22:20,567 - INFO - Epoch/batch: 10/   0, ibatch: 45000, loss: \u001b[0;36m0.5521\u001b[0m, std: 0.6318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301: ReduceOnPlateau set learning rate to 0.00029543127065508344.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:22:30,336 - INFO - loss: \u001b[0;32m0.5258\u001b[0m, std: 0.5888\n",
      "2021-05-13 00:22:36,073 - INFO - Epoch/batch: 10/ 151, ibatch: 45151, loss: \u001b[0;36m0.5214\u001b[0m, std: 0.5921\n",
      "2021-05-13 00:22:41,859 - INFO - Epoch/batch: 10/ 302, ibatch: 45302, loss: \u001b[0;36m0.5416\u001b[0m, std: 0.6268\n",
      "2021-05-13 00:22:57,347 - INFO - loss: \u001b[0;32m0.5255\u001b[0m, std: 0.5912\n",
      "2021-05-13 00:22:57,461 - INFO - Epoch/batch: 10/ 453, ibatch: 45453, loss: \u001b[0;36m0.5440\u001b[0m, std: 0.6126\n",
      "2021-05-13 00:23:03,452 - INFO - Epoch/batch: 10/ 604, ibatch: 45604, loss: \u001b[0;36m0.5296\u001b[0m, std: 0.5996\n",
      "2021-05-13 00:23:09,307 - INFO - Epoch/batch: 10/ 755, ibatch: 45755, loss: \u001b[0;36m0.5357\u001b[0m, std: 0.6130\n",
      "2021-05-13 00:23:24,851 - INFO - loss: \u001b[0;32m0.5245\u001b[0m, std: 0.6032\n",
      "2021-05-13 00:23:25,077 - INFO - Epoch/batch: 10/ 906, ibatch: 45906, loss: \u001b[0;36m0.5381\u001b[0m, std: 0.6115\n",
      "2021-05-13 00:23:31,042 - INFO - Epoch/batch: 10/1057, ibatch: 46057, loss: \u001b[0;36m0.5430\u001b[0m, std: 0.6208\n",
      "2021-05-13 00:23:36,681 - INFO - Epoch/batch: 10/1208, ibatch: 46208, loss: \u001b[0;36m0.5402\u001b[0m, std: 0.6196\n",
      "2021-05-13 00:23:52,900 - INFO - loss: \u001b[0;32m0.5250\u001b[0m, std: 0.5981\n",
      "2021-05-13 00:23:53,248 - INFO - Epoch/batch: 10/1359, ibatch: 46359, loss: \u001b[0;36m0.5393\u001b[0m, std: 0.6088\n",
      "2021-05-13 00:23:59,079 - INFO - Epoch/batch: 10/1510, ibatch: 46510, loss: \u001b[0;36m0.5272\u001b[0m, std: 0.6182\n",
      "2021-05-13 00:24:04,394 - INFO - Epoch/batch: 10/1661, ibatch: 46661, loss: \u001b[0;36m0.5202\u001b[0m, std: 0.5978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 312: ReduceOnPlateau set learning rate to 0.0002658881435895751.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:24:19,096 - INFO - loss: \u001b[0;32m0.5248\u001b[0m, std: 0.5930\n",
      "2021-05-13 00:24:19,570 - INFO - Epoch/batch: 10/1812, ibatch: 46812, loss: \u001b[0;36m0.5209\u001b[0m, std: 0.6100\n",
      "2021-05-13 00:24:25,311 - INFO - Epoch/batch: 10/1963, ibatch: 46963, loss: \u001b[0;36m0.5237\u001b[0m, std: 0.5978\n",
      "2021-05-13 00:24:30,959 - INFO - Epoch/batch: 10/2114, ibatch: 47114, loss: \u001b[0;36m0.5165\u001b[0m, std: 0.5939\n",
      "2021-05-13 00:24:45,797 - INFO - loss: \u001b[0;32m0.5242\u001b[0m, std: 0.6050\n",
      "2021-05-13 00:24:46,440 - INFO - Epoch/batch: 10/2265, ibatch: 47265, loss: \u001b[0;36m0.5470\u001b[0m, std: 0.6203\n",
      "2021-05-13 00:24:52,437 - INFO - Epoch/batch: 10/2416, ibatch: 47416, loss: \u001b[0;36m0.5223\u001b[0m, std: 0.6039\n",
      "2021-05-13 00:24:58,042 - INFO - Epoch/batch: 10/2567, ibatch: 47567, loss: \u001b[0;36m0.5269\u001b[0m, std: 0.6129\n",
      "2021-05-13 00:25:12,858 - INFO - loss: \u001b[0;32m0.5242\u001b[0m, std: 0.6091\n",
      "2021-05-13 00:25:13,552 - INFO - Epoch/batch: 10/2718, ibatch: 47718, loss: \u001b[0;36m0.5202\u001b[0m, std: 0.5987\n",
      "2021-05-13 00:25:19,230 - INFO - Epoch/batch: 10/2869, ibatch: 47869, loss: \u001b[0;36m0.5289\u001b[0m, std: 0.6093\n",
      "2021-05-13 00:25:24,997 - INFO - Epoch/batch: 10/3020, ibatch: 48020, loss: \u001b[0;36m0.5126\u001b[0m, std: 0.5931\n",
      "2021-05-13 00:25:40,007 - INFO - loss: \u001b[0;32m0.5239\u001b[0m, std: 0.6077\n",
      "2021-05-13 00:25:40,024 - INFO - Saved model states in: earlystop_0.5239\n",
      "2021-05-13 00:25:40,026 - INFO - Saved net python code: earlystop_0.5239/paddle_nets.py\n",
      "2021-05-13 00:25:40,034 - INFO - Saved best model: earlystop_0.5239\n",
      "2021-05-13 00:25:40,034 - INFO - Removing earlystop model: earlystop_0.5241\n",
      "2021-05-13 00:25:40,893 - INFO - Epoch/batch: 10/3171, ibatch: 48171, loss: \u001b[0;36m0.5254\u001b[0m, std: 0.6060\n",
      "2021-05-13 00:25:46,545 - INFO - Epoch/batch: 10/3322, ibatch: 48322, loss: \u001b[0;36m0.5374\u001b[0m, std: 0.6070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 323: ReduceOnPlateau set learning rate to 0.0002392993292306176.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:25:51,853 - INFO - Epoch/batch: 10/3473, ibatch: 48473, loss: \u001b[0;36m0.5214\u001b[0m, std: 0.6141\n",
      "2021-05-13 00:26:06,512 - INFO - loss: \u001b[0;32m0.5241\u001b[0m, std: 0.6139\n",
      "2021-05-13 00:26:07,537 - INFO - Epoch/batch: 10/3624, ibatch: 48624, loss: \u001b[0;36m0.5336\u001b[0m, std: 0.6218\n",
      "2021-05-13 00:26:13,193 - INFO - Epoch/batch: 10/3775, ibatch: 48775, loss: \u001b[0;36m0.5230\u001b[0m, std: 0.6067\n",
      "2021-05-13 00:26:19,008 - INFO - Epoch/batch: 10/3926, ibatch: 48926, loss: \u001b[0;36m0.5238\u001b[0m, std: 0.6059\n",
      "2021-05-13 00:26:33,978 - INFO - loss: \u001b[0;32m0.5245\u001b[0m, std: 0.6151\n",
      "2021-05-13 00:26:35,055 - INFO - Epoch/batch: 10/4077, ibatch: 49077, loss: \u001b[0;36m0.5221\u001b[0m, std: 0.6037\n",
      "2021-05-13 00:26:40,403 - INFO - Epoch/batch: 10/4228, ibatch: 49228, loss: \u001b[0;36m0.5231\u001b[0m, std: 0.6044\n",
      "2021-05-13 00:26:45,925 - INFO - Epoch/batch: 10/4379, ibatch: 49379, loss: \u001b[0;36m0.5375\u001b[0m, std: 0.6167\n",
      "2021-05-13 00:26:59,978 - INFO - loss: \u001b[0;32m0.5244\u001b[0m, std: 0.5947\n",
      "2021-05-13 00:27:01,287 - INFO - Epoch 10 average training loss: \u001b[0;46m0.5296\u001b[0m std: 0.6088\n",
      "2021-05-13 00:27:01,290 - INFO - Epoch 10 average validate loss: \u001b[0;46m0.5246\u001b[0m std: 0.6018\n",
      "2021-05-13 00:27:03,262 - INFO - Epoch/batch: 11/   0, ibatch: 49500, loss: \u001b[0;36m0.5404\u001b[0m, std: 0.6175\n",
      "2021-05-13 00:27:13,262 - INFO - loss: \u001b[0;32m0.5245\u001b[0m, std: 0.5946\n",
      "2021-05-13 00:27:19,121 - INFO - Epoch/batch: 11/ 151, ibatch: 49651, loss: \u001b[0;36m0.5329\u001b[0m, std: 0.6070\n",
      "2021-05-13 00:27:25,745 - INFO - Epoch/batch: 11/ 302, ibatch: 49802, loss: \u001b[0;36m0.5285\u001b[0m, std: 0.6051\n",
      "2021-05-13 00:27:41,782 - INFO - loss: \u001b[0;32m0.5241\u001b[0m, std: 0.6028\n",
      "2021-05-13 00:27:41,938 - INFO - Epoch/batch: 11/ 453, ibatch: 49953, loss: \u001b[0;36m0.5051\u001b[0m, std: 0.5964\n",
      "2021-05-13 00:27:47,655 - INFO - Epoch/batch: 11/ 604, ibatch: 50104, loss: \u001b[0;36m0.5227\u001b[0m, std: 0.5955\n",
      "2021-05-13 00:27:53,414 - INFO - Epoch/batch: 11/ 755, ibatch: 50255, loss: \u001b[0;36m0.5364\u001b[0m, std: 0.6189\n",
      "2021-05-13 00:28:08,480 - INFO - loss: \u001b[0;32m0.5245\u001b[0m, std: 0.6139\n",
      "2021-05-13 00:28:08,730 - INFO - Epoch/batch: 11/ 906, ibatch: 50406, loss: \u001b[0;36m0.5093\u001b[0m, std: 0.6001\n",
      "2021-05-13 00:28:14,287 - INFO - Epoch/batch: 11/1057, ibatch: 50557, loss: \u001b[0;36m0.5198\u001b[0m, std: 0.6003\n",
      "2021-05-13 00:28:19,799 - INFO - Epoch/batch: 11/1208, ibatch: 50708, loss: \u001b[0;36m0.5128\u001b[0m, std: 0.5927\n",
      "2021-05-13 00:28:34,715 - INFO - loss: \u001b[0;32m0.5244\u001b[0m, std: 0.6135\n",
      "2021-05-13 00:28:35,048 - INFO - Epoch/batch: 11/1359, ibatch: 50859, loss: \u001b[0;36m0.5130\u001b[0m, std: 0.6001\n",
      "2021-05-13 00:28:40,834 - INFO - Epoch/batch: 11/1510, ibatch: 51010, loss: \u001b[0;36m0.5535\u001b[0m, std: 0.6360\n",
      "2021-05-13 00:28:46,369 - INFO - Epoch/batch: 11/1661, ibatch: 51161, loss: \u001b[0;36m0.5449\u001b[0m, std: 0.6143\n",
      "2021-05-13 00:29:01,727 - INFO - loss: \u001b[0;32m0.5249\u001b[0m, std: 0.5983\n",
      "2021-05-13 00:29:02,232 - INFO - Epoch/batch: 11/1812, ibatch: 51312, loss: \u001b[0;36m0.5440\u001b[0m, std: 0.6188\n",
      "2021-05-13 00:29:07,793 - INFO - Epoch/batch: 11/1963, ibatch: 51463, loss: \u001b[0;36m0.5382\u001b[0m, std: 0.6073\n",
      "2021-05-13 00:29:13,591 - INFO - Epoch/batch: 11/2114, ibatch: 51614, loss: \u001b[0;36m0.5230\u001b[0m, std: 0.6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 345: ReduceOnPlateau set learning rate to 0.00021536939630755584.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:29:29,180 - INFO - loss: \u001b[0;32m0.5241\u001b[0m, std: 0.6069\n",
      "2021-05-13 00:29:29,897 - INFO - Epoch/batch: 11/2265, ibatch: 51765, loss: \u001b[0;36m0.5189\u001b[0m, std: 0.5966\n",
      "2021-05-13 00:29:35,285 - INFO - Epoch/batch: 11/2416, ibatch: 51916, loss: \u001b[0;36m0.5113\u001b[0m, std: 0.6023\n",
      "2021-05-13 00:29:40,968 - INFO - Epoch/batch: 11/2567, ibatch: 52067, loss: \u001b[0;36m0.5115\u001b[0m, std: 0.5897\n",
      "2021-05-13 00:29:55,597 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.6039\n",
      "2021-05-13 00:29:55,614 - INFO - Saved model states in: earlystop_0.5238\n",
      "2021-05-13 00:29:55,616 - INFO - Saved net python code: earlystop_0.5238/paddle_nets.py\n",
      "2021-05-13 00:29:55,624 - INFO - Saved best model: earlystop_0.5238\n",
      "2021-05-13 00:29:55,625 - INFO - Removing earlystop model: earlystop_0.5239\n",
      "2021-05-13 00:29:56,343 - INFO - Epoch/batch: 11/2718, ibatch: 52218, loss: \u001b[0;36m0.5411\u001b[0m, std: 0.6234\n",
      "2021-05-13 00:30:02,368 - INFO - Epoch/batch: 11/2869, ibatch: 52369, loss: \u001b[0;36m0.5531\u001b[0m, std: 0.6221\n",
      "2021-05-13 00:30:08,429 - INFO - Epoch/batch: 11/3020, ibatch: 52520, loss: \u001b[0;36m0.5472\u001b[0m, std: 0.6200\n",
      "2021-05-13 00:30:23,043 - INFO - loss: \u001b[0;32m0.5244\u001b[0m, std: 0.5986\n",
      "2021-05-13 00:30:23,836 - INFO - Epoch/batch: 11/3171, ibatch: 52671, loss: \u001b[0;36m0.5463\u001b[0m, std: 0.6148\n",
      "2021-05-13 00:30:29,483 - INFO - Epoch/batch: 11/3322, ibatch: 52822, loss: \u001b[0;36m0.5393\u001b[0m, std: 0.6114\n",
      "2021-05-13 00:30:35,534 - INFO - Epoch/batch: 11/3473, ibatch: 52973, loss: \u001b[0;36m0.5293\u001b[0m, std: 0.6020\n",
      "2021-05-13 00:30:50,000 - INFO - loss: \u001b[0;32m0.5240\u001b[0m, std: 0.5992\n",
      "2021-05-13 00:30:51,030 - INFO - Epoch/batch: 11/3624, ibatch: 53124, loss: \u001b[0;36m0.5345\u001b[0m, std: 0.6201\n",
      "2021-05-13 00:30:56,732 - INFO - Epoch/batch: 11/3775, ibatch: 53275, loss: \u001b[0;36m0.5318\u001b[0m, std: 0.6145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 356: ReduceOnPlateau set learning rate to 0.00019383245667680025.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:31:02,276 - INFO - Epoch/batch: 11/3926, ibatch: 53426, loss: \u001b[0;36m0.5321\u001b[0m, std: 0.6132\n",
      "2021-05-13 00:31:16,561 - INFO - loss: \u001b[0;32m0.5241\u001b[0m, std: 0.6028\n",
      "2021-05-13 00:31:18,178 - INFO - Epoch/batch: 11/4077, ibatch: 53577, loss: \u001b[0;36m0.5145\u001b[0m, std: 0.5926\n",
      "2021-05-13 00:31:23,951 - INFO - Epoch/batch: 11/4228, ibatch: 53728, loss: \u001b[0;36m0.5370\u001b[0m, std: 0.6125\n",
      "2021-05-13 00:31:29,583 - INFO - Epoch/batch: 11/4379, ibatch: 53879, loss: \u001b[0;36m0.5064\u001b[0m, std: 0.5958\n",
      "2021-05-13 00:31:43,612 - INFO - loss: \u001b[0;32m0.5240\u001b[0m, std: 0.6109\n",
      "2021-05-13 00:31:44,917 - INFO - Epoch 11 average training loss: \u001b[0;46m0.5288\u001b[0m std: 0.6077\n",
      "2021-05-13 00:31:45,011 - INFO - Epoch 11 average validate loss: \u001b[0;46m0.5242\u001b[0m std: 0.6041\n",
      "2021-05-13 00:31:46,900 - INFO - Epoch/batch: 12/   0, ibatch: 54000, loss: \u001b[0;36m0.5274\u001b[0m, std: 0.6088\n",
      "2021-05-13 00:31:56,646 - INFO - loss: \u001b[0;32m0.5239\u001b[0m, std: 0.6109\n",
      "2021-05-13 00:32:02,265 - INFO - Epoch/batch: 12/ 151, ibatch: 54151, loss: \u001b[0;36m0.5211\u001b[0m, std: 0.6015\n",
      "2021-05-13 00:32:07,821 - INFO - Epoch/batch: 12/ 302, ibatch: 54302, loss: \u001b[0;36m0.5198\u001b[0m, std: 0.5982\n",
      "2021-05-13 00:32:23,111 - INFO - loss: \u001b[0;32m0.5241\u001b[0m, std: 0.6085\n",
      "2021-05-13 00:32:23,208 - INFO - Epoch/batch: 12/ 453, ibatch: 54453, loss: \u001b[0;36m0.5111\u001b[0m, std: 0.5937\n",
      "2021-05-13 00:32:29,034 - INFO - Epoch/batch: 12/ 604, ibatch: 54604, loss: \u001b[0;36m0.5148\u001b[0m, std: 0.5942\n",
      "2021-05-13 00:32:35,268 - INFO - Epoch/batch: 12/ 755, ibatch: 54755, loss: \u001b[0;36m0.5613\u001b[0m, std: 0.6316\n",
      "2021-05-13 00:32:50,574 - INFO - loss: \u001b[0;32m0.5242\u001b[0m, std: 0.6158\n",
      "2021-05-13 00:32:50,787 - INFO - Epoch/batch: 12/ 906, ibatch: 54906, loss: \u001b[0;36m0.5240\u001b[0m, std: 0.5978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 367: ReduceOnPlateau set learning rate to 0.00017444921100912022.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:32:56,522 - INFO - Epoch/batch: 12/1057, ibatch: 55057, loss: \u001b[0;36m0.5395\u001b[0m, std: 0.6237\n",
      "2021-05-13 00:33:02,239 - INFO - Epoch/batch: 12/1208, ibatch: 55208, loss: \u001b[0;36m0.5308\u001b[0m, std: 0.6037\n",
      "2021-05-13 00:33:17,460 - INFO - loss: \u001b[0;32m0.5239\u001b[0m, std: 0.6038\n",
      "2021-05-13 00:33:17,864 - INFO - Epoch/batch: 12/1359, ibatch: 55359, loss: \u001b[0;36m0.5221\u001b[0m, std: 0.6052\n",
      "2021-05-13 00:33:24,319 - INFO - Epoch/batch: 12/1510, ibatch: 55510, loss: \u001b[0;36m0.5355\u001b[0m, std: 0.6104\n",
      "2021-05-13 00:33:30,967 - INFO - Epoch/batch: 12/1661, ibatch: 55661, loss: \u001b[0;36m0.5339\u001b[0m, std: 0.6138\n",
      "2021-05-13 00:33:47,129 - INFO - loss: \u001b[0;32m0.5239\u001b[0m, std: 0.6072\n",
      "2021-05-13 00:33:47,600 - INFO - Epoch/batch: 12/1812, ibatch: 55812, loss: \u001b[0;36m0.5283\u001b[0m, std: 0.6081\n",
      "2021-05-13 00:33:53,617 - INFO - Epoch/batch: 12/1963, ibatch: 55963, loss: \u001b[0;36m0.5442\u001b[0m, std: 0.6284\n",
      "2021-05-13 00:33:59,278 - INFO - Epoch/batch: 12/2114, ibatch: 56114, loss: \u001b[0;36m0.5179\u001b[0m, std: 0.5920\n",
      "2021-05-13 00:34:14,579 - INFO - loss: \u001b[0;32m0.5241\u001b[0m, std: 0.6122\n",
      "2021-05-13 00:34:15,221 - INFO - Epoch/batch: 12/2265, ibatch: 56265, loss: \u001b[0;36m0.5198\u001b[0m, std: 0.6042\n",
      "2021-05-13 00:34:21,546 - INFO - Epoch/batch: 12/2416, ibatch: 56416, loss: \u001b[0;36m0.5433\u001b[0m, std: 0.6232\n",
      "2021-05-13 00:34:28,273 - INFO - Epoch/batch: 12/2567, ibatch: 56567, loss: \u001b[0;36m0.5242\u001b[0m, std: 0.6064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 378: ReduceOnPlateau set learning rate to 0.0001570042899082082.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:34:43,969 - INFO - loss: \u001b[0;32m0.5240\u001b[0m, std: 0.6012\n",
      "2021-05-13 00:34:44,545 - INFO - Epoch/batch: 12/2718, ibatch: 56718, loss: \u001b[0;36m0.5253\u001b[0m, std: 0.6002\n",
      "2021-05-13 00:34:50,175 - INFO - Epoch/batch: 12/2869, ibatch: 56869, loss: \u001b[0;36m0.5237\u001b[0m, std: 0.6041\n",
      "2021-05-13 00:34:55,921 - INFO - Epoch/batch: 12/3020, ibatch: 57020, loss: \u001b[0;36m0.5498\u001b[0m, std: 0.6220\n",
      "2021-05-13 00:35:10,434 - INFO - loss: \u001b[0;32m0.5249\u001b[0m, std: 0.5924\n",
      "2021-05-13 00:35:11,203 - INFO - Epoch/batch: 12/3171, ibatch: 57171, loss: \u001b[0;36m0.5387\u001b[0m, std: 0.6161\n",
      "2021-05-13 00:35:16,833 - INFO - Epoch/batch: 12/3322, ibatch: 57322, loss: \u001b[0;36m0.5415\u001b[0m, std: 0.6159\n",
      "2021-05-13 00:35:22,510 - INFO - Epoch/batch: 12/3473, ibatch: 57473, loss: \u001b[0;36m0.5210\u001b[0m, std: 0.6031\n",
      "2021-05-13 00:35:36,472 - INFO - loss: \u001b[0;32m0.5241\u001b[0m, std: 0.6076\n",
      "2021-05-13 00:35:37,415 - INFO - Epoch/batch: 12/3624, ibatch: 57624, loss: \u001b[0;36m0.5089\u001b[0m, std: 0.5899\n",
      "2021-05-13 00:35:42,959 - INFO - Epoch/batch: 12/3775, ibatch: 57775, loss: \u001b[0;36m0.5244\u001b[0m, std: 0.6111\n",
      "2021-05-13 00:35:48,596 - INFO - Epoch/batch: 12/3926, ibatch: 57926, loss: \u001b[0;36m0.5305\u001b[0m, std: 0.6040\n",
      "2021-05-13 00:36:02,676 - INFO - loss: \u001b[0;32m0.5244\u001b[0m, std: 0.6099\n",
      "2021-05-13 00:36:03,665 - INFO - Epoch/batch: 12/4077, ibatch: 58077, loss: \u001b[0;36m0.5234\u001b[0m, std: 0.5970\n",
      "2021-05-13 00:36:09,337 - INFO - Epoch/batch: 12/4228, ibatch: 58228, loss: \u001b[0;36m0.5265\u001b[0m, std: 0.6082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 389: ReduceOnPlateau set learning rate to 0.0001413038609173874.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:36:15,005 - INFO - Epoch/batch: 12/4379, ibatch: 58379, loss: \u001b[0;36m0.5342\u001b[0m, std: 0.6078\n",
      "2021-05-13 00:36:28,883 - INFO - loss: \u001b[0;32m0.5241\u001b[0m, std: 0.6049\n",
      "2021-05-13 00:36:30,209 - INFO - Epoch 12 average training loss: \u001b[0;46m0.5285\u001b[0m std: 0.6074\n",
      "2021-05-13 00:36:30,213 - INFO - Epoch 12 average validate loss: \u001b[0;46m0.5242\u001b[0m std: 0.6068\n",
      "2021-05-13 00:36:32,074 - INFO - Epoch/batch: 13/   0, ibatch: 58500, loss: \u001b[0;36m0.5113\u001b[0m, std: 0.6066\n",
      "2021-05-13 00:36:41,767 - INFO - loss: \u001b[0;32m0.5241\u001b[0m, std: 0.6049\n",
      "2021-05-13 00:36:47,678 - INFO - Epoch/batch: 13/ 151, ibatch: 58651, loss: \u001b[0;36m0.5175\u001b[0m, std: 0.6047\n",
      "2021-05-13 00:36:53,355 - INFO - Epoch/batch: 13/ 302, ibatch: 58802, loss: \u001b[0;36m0.5232\u001b[0m, std: 0.6072\n",
      "2021-05-13 00:37:08,371 - INFO - loss: \u001b[0;32m0.5244\u001b[0m, std: 0.6169\n",
      "2021-05-13 00:37:08,503 - INFO - Epoch/batch: 13/ 453, ibatch: 58953, loss: \u001b[0;36m0.5049\u001b[0m, std: 0.5862\n",
      "2021-05-13 00:37:14,252 - INFO - Epoch/batch: 13/ 604, ibatch: 59104, loss: \u001b[0;36m0.5319\u001b[0m, std: 0.6145\n",
      "2021-05-13 00:37:20,075 - INFO - Epoch/batch: 13/ 755, ibatch: 59255, loss: \u001b[0;36m0.5299\u001b[0m, std: 0.6062\n",
      "2021-05-13 00:37:35,379 - INFO - loss: \u001b[0;32m0.5239\u001b[0m, std: 0.6020\n",
      "2021-05-13 00:37:35,636 - INFO - Epoch/batch: 13/ 906, ibatch: 59406, loss: \u001b[0;36m0.5341\u001b[0m, std: 0.6109\n",
      "2021-05-13 00:37:41,077 - INFO - Epoch/batch: 13/1057, ibatch: 59557, loss: \u001b[0;36m0.5062\u001b[0m, std: 0.5882\n",
      "2021-05-13 00:37:46,592 - INFO - Epoch/batch: 13/1208, ibatch: 59708, loss: \u001b[0;36m0.5406\u001b[0m, std: 0.6141\n",
      "2021-05-13 00:38:01,317 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.6093\n",
      "2021-05-13 00:38:01,694 - INFO - Epoch/batch: 13/1359, ibatch: 59859, loss: \u001b[0;36m0.5226\u001b[0m, std: 0.6021\n",
      "2021-05-13 00:38:07,274 - INFO - Epoch/batch: 13/1510, ibatch: 60010, loss: \u001b[0;36m0.5216\u001b[0m, std: 0.6038\n",
      "2021-05-13 00:38:12,780 - INFO - Epoch/batch: 13/1661, ibatch: 60161, loss: \u001b[0;36m0.5129\u001b[0m, std: 0.5996\n",
      "2021-05-13 00:38:27,515 - INFO - loss: \u001b[0;32m0.5239\u001b[0m, std: 0.6105\n",
      "2021-05-13 00:38:28,021 - INFO - Epoch/batch: 13/1812, ibatch: 60312, loss: \u001b[0;36m0.5449\u001b[0m, std: 0.6066\n",
      "2021-05-13 00:38:34,830 - INFO - Epoch/batch: 13/1963, ibatch: 60463, loss: \u001b[0;36m0.5508\u001b[0m, std: 0.6313\n",
      "2021-05-13 00:38:40,897 - INFO - Epoch/batch: 13/2114, ibatch: 60614, loss: \u001b[0;36m0.5346\u001b[0m, std: 0.6143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 405: ReduceOnPlateau set learning rate to 0.00012717347482564865.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:38:57,263 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.5987\n",
      "2021-05-13 00:38:57,860 - INFO - Epoch/batch: 13/2265, ibatch: 60765, loss: \u001b[0;36m0.5352\u001b[0m, std: 0.6170\n",
      "2021-05-13 00:39:03,164 - INFO - Epoch/batch: 13/2416, ibatch: 60916, loss: \u001b[0;36m0.5322\u001b[0m, std: 0.6012\n",
      "2021-05-13 00:39:08,927 - INFO - Epoch/batch: 13/2567, ibatch: 61067, loss: \u001b[0;36m0.5303\u001b[0m, std: 0.6145\n",
      "2021-05-13 00:39:23,838 - INFO - loss: \u001b[0;32m0.5244\u001b[0m, std: 0.6009\n",
      "2021-05-13 00:39:24,588 - INFO - Epoch/batch: 13/2718, ibatch: 61218, loss: \u001b[0;36m0.5333\u001b[0m, std: 0.6138\n",
      "2021-05-13 00:39:29,988 - INFO - Epoch/batch: 13/2869, ibatch: 61369, loss: \u001b[0;36m0.5212\u001b[0m, std: 0.5929\n",
      "2021-05-13 00:39:35,503 - INFO - Epoch/batch: 13/3020, ibatch: 61520, loss: \u001b[0;36m0.5210\u001b[0m, std: 0.6083\n",
      "2021-05-13 00:39:50,619 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.6040\n",
      "2021-05-13 00:39:51,329 - INFO - Epoch/batch: 13/3171, ibatch: 61671, loss: \u001b[0;36m0.5376\u001b[0m, std: 0.6170\n",
      "2021-05-13 00:39:56,617 - INFO - Epoch/batch: 13/3322, ibatch: 61822, loss: \u001b[0;36m0.5200\u001b[0m, std: 0.6017\n",
      "2021-05-13 00:40:02,354 - INFO - Epoch/batch: 13/3473, ibatch: 61973, loss: \u001b[0;36m0.5394\u001b[0m, std: 0.6098\n",
      "2021-05-13 00:40:16,563 - INFO - loss: \u001b[0;32m0.5239\u001b[0m, std: 0.6044\n",
      "2021-05-13 00:40:17,491 - INFO - Epoch/batch: 13/3624, ibatch: 62124, loss: \u001b[0;36m0.5139\u001b[0m, std: 0.5976\n",
      "2021-05-13 00:40:22,936 - INFO - Epoch/batch: 13/3775, ibatch: 62275, loss: \u001b[0;36m0.5073\u001b[0m, std: 0.5890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 416: ReduceOnPlateau set learning rate to 0.00011445612734308378.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:40:28,227 - INFO - Epoch/batch: 13/3926, ibatch: 62426, loss: \u001b[0;36m0.5321\u001b[0m, std: 0.6137\n",
      "2021-05-13 00:40:42,289 - INFO - loss: \u001b[0;32m0.5240\u001b[0m, std: 0.6023\n",
      "2021-05-13 00:40:43,406 - INFO - Epoch/batch: 13/4077, ibatch: 62577, loss: \u001b[0;36m0.5304\u001b[0m, std: 0.6056\n",
      "2021-05-13 00:40:49,118 - INFO - Epoch/batch: 13/4228, ibatch: 62728, loss: \u001b[0;36m0.5417\u001b[0m, std: 0.6187\n",
      "2021-05-13 00:40:54,924 - INFO - Epoch/batch: 13/4379, ibatch: 62879, loss: \u001b[0;36m0.5375\u001b[0m, std: 0.6218\n",
      "2021-05-13 00:41:08,985 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.5999\n",
      "2021-05-13 00:41:10,311 - INFO - Epoch 13 average training loss: \u001b[0;46m0.5281\u001b[0m std: 0.6073\n",
      "2021-05-13 00:41:10,315 - INFO - Epoch 13 average validate loss: \u001b[0;46m0.5240\u001b[0m std: 0.6049\n",
      "2021-05-13 00:41:12,143 - INFO - Epoch/batch: 14/   0, ibatch: 63000, loss: \u001b[0;36m0.5377\u001b[0m, std: 0.6074\n",
      "2021-05-13 00:41:21,953 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.6003\n",
      "2021-05-13 00:41:21,973 - INFO - Saved model states in: earlystop_0.5238.1\n",
      "2021-05-13 00:41:21,974 - INFO - Saved net python code: earlystop_0.5238.1/paddle_nets.py\n",
      "2021-05-13 00:41:22,010 - INFO - Saved best model: earlystop_0.5238.1\n",
      "2021-05-13 00:41:22,011 - INFO - Removing earlystop model: earlystop_0.5238\n",
      "2021-05-13 00:41:27,784 - INFO - Epoch/batch: 14/ 151, ibatch: 63151, loss: \u001b[0;36m0.5155\u001b[0m, std: 0.5961\n",
      "2021-05-13 00:41:33,841 - INFO - Epoch/batch: 14/ 302, ibatch: 63302, loss: \u001b[0;36m0.5311\u001b[0m, std: 0.6115\n",
      "2021-05-13 00:41:48,992 - INFO - loss: \u001b[0;32m0.5241\u001b[0m, std: 0.5995\n",
      "2021-05-13 00:41:49,110 - INFO - Epoch/batch: 14/ 453, ibatch: 63453, loss: \u001b[0;36m0.5231\u001b[0m, std: 0.5996\n",
      "2021-05-13 00:41:54,596 - INFO - Epoch/batch: 14/ 604, ibatch: 63604, loss: \u001b[0;36m0.5150\u001b[0m, std: 0.5931\n",
      "2021-05-13 00:42:00,799 - INFO - Epoch/batch: 14/ 755, ibatch: 63755, loss: \u001b[0;36m0.5329\u001b[0m, std: 0.6069\n",
      "2021-05-13 00:42:16,764 - INFO - loss: \u001b[0;32m0.5241\u001b[0m, std: 0.6101\n",
      "2021-05-13 00:42:16,989 - INFO - Epoch/batch: 14/ 906, ibatch: 63906, loss: \u001b[0;36m0.5227\u001b[0m, std: 0.6097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427: ReduceOnPlateau set learning rate to 0.00010301051460877541.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:42:22,649 - INFO - Epoch/batch: 14/1057, ibatch: 64057, loss: \u001b[0;36m0.5205\u001b[0m, std: 0.5993\n",
      "2021-05-13 00:42:28,208 - INFO - Epoch/batch: 14/1208, ibatch: 64208, loss: \u001b[0;36m0.5167\u001b[0m, std: 0.5940\n",
      "2021-05-13 00:42:43,425 - INFO - loss: \u001b[0;32m0.5240\u001b[0m, std: 0.6084\n",
      "2021-05-13 00:42:43,795 - INFO - Epoch/batch: 14/1359, ibatch: 64359, loss: \u001b[0;36m0.5208\u001b[0m, std: 0.5984\n",
      "2021-05-13 00:42:49,792 - INFO - Epoch/batch: 14/1510, ibatch: 64510, loss: \u001b[0;36m0.5364\u001b[0m, std: 0.6190\n",
      "2021-05-13 00:42:55,637 - INFO - Epoch/batch: 14/1661, ibatch: 64661, loss: \u001b[0;36m0.5208\u001b[0m, std: 0.6009\n",
      "2021-05-13 00:43:10,976 - INFO - loss: \u001b[0;32m0.5242\u001b[0m, std: 0.6154\n",
      "2021-05-13 00:43:11,406 - INFO - Epoch/batch: 14/1812, ibatch: 64812, loss: \u001b[0;36m0.5230\u001b[0m, std: 0.6017\n",
      "2021-05-13 00:43:16,789 - INFO - Epoch/batch: 14/1963, ibatch: 64963, loss: \u001b[0;36m0.5170\u001b[0m, std: 0.6049\n",
      "2021-05-13 00:43:22,513 - INFO - Epoch/batch: 14/2114, ibatch: 65114, loss: \u001b[0;36m0.5325\u001b[0m, std: 0.6015\n",
      "2021-05-13 00:43:37,329 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.6019\n",
      "2021-05-13 00:43:37,932 - INFO - Epoch/batch: 14/2265, ibatch: 65265, loss: \u001b[0;36m0.5283\u001b[0m, std: 0.6104\n",
      "2021-05-13 00:43:43,498 - INFO - Epoch/batch: 14/2416, ibatch: 65416, loss: \u001b[0;36m0.5303\u001b[0m, std: 0.6143\n",
      "2021-05-13 00:43:49,302 - INFO - Epoch/batch: 14/2567, ibatch: 65567, loss: \u001b[0;36m0.5323\u001b[0m, std: 0.6144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 438: ReduceOnPlateau set learning rate to 9.270946314789788e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:44:04,035 - INFO - loss: \u001b[0;32m0.5240\u001b[0m, std: 0.5985\n",
      "2021-05-13 00:44:04,732 - INFO - Epoch/batch: 14/2718, ibatch: 65718, loss: \u001b[0;36m0.5244\u001b[0m, std: 0.5976\n",
      "2021-05-13 00:44:10,590 - INFO - Epoch/batch: 14/2869, ibatch: 65869, loss: \u001b[0;36m0.5412\u001b[0m, std: 0.6185\n",
      "2021-05-13 00:44:16,696 - INFO - Epoch/batch: 14/3020, ibatch: 66020, loss: \u001b[0;36m0.5398\u001b[0m, std: 0.6249\n",
      "2021-05-13 00:44:31,781 - INFO - loss: \u001b[0;32m0.5240\u001b[0m, std: 0.5951\n",
      "2021-05-13 00:44:32,592 - INFO - Epoch/batch: 14/3171, ibatch: 66171, loss: \u001b[0;36m0.5306\u001b[0m, std: 0.6061\n",
      "2021-05-13 00:44:38,926 - INFO - Epoch/batch: 14/3322, ibatch: 66322, loss: \u001b[0;36m0.5427\u001b[0m, std: 0.6152\n",
      "2021-05-13 00:44:44,831 - INFO - Epoch/batch: 14/3473, ibatch: 66473, loss: \u001b[0;36m0.5308\u001b[0m, std: 0.6152\n",
      "2021-05-13 00:44:59,088 - INFO - loss: \u001b[0;32m0.5236\u001b[0m, std: 0.6025\n",
      "2021-05-13 00:44:59,103 - INFO - Saved model states in: earlystop_0.5236\n",
      "2021-05-13 00:44:59,105 - INFO - Saved net python code: earlystop_0.5236/paddle_nets.py\n",
      "2021-05-13 00:44:59,114 - INFO - Saved best model: earlystop_0.5236\n",
      "2021-05-13 00:44:59,115 - INFO - Removing earlystop model: earlystop_0.5238.1\n",
      "2021-05-13 00:45:00,048 - INFO - Epoch/batch: 14/3624, ibatch: 66624, loss: \u001b[0;36m0.5193\u001b[0m, std: 0.5944\n",
      "2021-05-13 00:45:05,972 - INFO - Epoch/batch: 14/3775, ibatch: 66775, loss: \u001b[0;36m0.5276\u001b[0m, std: 0.6075\n",
      "2021-05-13 00:45:11,573 - INFO - Epoch/batch: 14/3926, ibatch: 66926, loss: \u001b[0;36m0.5094\u001b[0m, std: 0.5835\n",
      "2021-05-13 00:45:25,684 - INFO - loss: \u001b[0;32m0.5237\u001b[0m, std: 0.6060\n",
      "2021-05-13 00:45:26,670 - INFO - Epoch/batch: 14/4077, ibatch: 67077, loss: \u001b[0;36m0.5371\u001b[0m, std: 0.6215\n",
      "2021-05-13 00:45:32,421 - INFO - Epoch/batch: 14/4228, ibatch: 67228, loss: \u001b[0;36m0.5433\u001b[0m, std: 0.6187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449: ReduceOnPlateau set learning rate to 8.343851683310809e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:45:37,945 - INFO - Epoch/batch: 14/4379, ibatch: 67379, loss: \u001b[0;36m0.5443\u001b[0m, std: 0.6261\n",
      "2021-05-13 00:45:51,754 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.6076\n",
      "2021-05-13 00:45:53,211 - INFO - Epoch 14 average training loss: \u001b[0;46m0.5278\u001b[0m std: 0.6066\n",
      "2021-05-13 00:45:53,308 - INFO - Epoch 14 average validate loss: \u001b[0;46m0.5239\u001b[0m std: 0.6041\n",
      "2021-05-13 00:45:55,185 - INFO - Epoch/batch: 15/   0, ibatch: 67500, loss: \u001b[0;36m0.5196\u001b[0m, std: 0.5877\n",
      "2021-05-13 00:46:04,811 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.6075\n",
      "2021-05-13 00:46:11,691 - INFO - Epoch/batch: 15/ 151, ibatch: 67651, loss: \u001b[0;36m0.5335\u001b[0m, std: 0.6153\n",
      "2021-05-13 00:46:18,205 - INFO - Epoch/batch: 15/ 302, ibatch: 67802, loss: \u001b[0;36m0.5279\u001b[0m, std: 0.6010\n",
      "2021-05-13 00:46:35,076 - INFO - loss: \u001b[0;32m0.5239\u001b[0m, std: 0.6000\n",
      "2021-05-13 00:46:35,254 - INFO - Epoch/batch: 15/ 453, ibatch: 67953, loss: \u001b[0;36m0.5375\u001b[0m, std: 0.6208\n",
      "2021-05-13 00:46:41,432 - INFO - Epoch/batch: 15/ 604, ibatch: 68104, loss: \u001b[0;36m0.5240\u001b[0m, std: 0.5959\n",
      "2021-05-13 00:46:47,442 - INFO - Epoch/batch: 15/ 755, ibatch: 68255, loss: \u001b[0;36m0.5288\u001b[0m, std: 0.6099\n",
      "2021-05-13 00:47:02,279 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.6060\n",
      "2021-05-13 00:47:02,555 - INFO - Epoch/batch: 15/ 906, ibatch: 68406, loss: \u001b[0;36m0.5284\u001b[0m, std: 0.6105\n",
      "2021-05-13 00:47:08,795 - INFO - Epoch/batch: 15/1057, ibatch: 68557, loss: \u001b[0;36m0.5314\u001b[0m, std: 0.6070\n",
      "2021-05-13 00:47:14,585 - INFO - Epoch/batch: 15/1208, ibatch: 68708, loss: \u001b[0;36m0.5387\u001b[0m, std: 0.6149\n",
      "2021-05-13 00:47:29,953 - INFO - loss: \u001b[0;32m0.5240\u001b[0m, std: 0.6088\n",
      "2021-05-13 00:47:30,274 - INFO - Epoch/batch: 15/1359, ibatch: 68859, loss: \u001b[0;36m0.5322\u001b[0m, std: 0.6072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 460: ReduceOnPlateau set learning rate to 7.509466514979728e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:47:35,931 - INFO - Epoch/batch: 15/1510, ibatch: 69010, loss: \u001b[0;36m0.5144\u001b[0m, std: 0.6000\n",
      "2021-05-13 00:47:42,148 - INFO - Epoch/batch: 15/1661, ibatch: 69161, loss: \u001b[0;36m0.5355\u001b[0m, std: 0.6201\n",
      "2021-05-13 00:47:58,186 - INFO - loss: \u001b[0;32m0.5237\u001b[0m, std: 0.6053\n",
      "2021-05-13 00:47:58,664 - INFO - Epoch/batch: 15/1812, ibatch: 69312, loss: \u001b[0;36m0.5209\u001b[0m, std: 0.6047\n",
      "2021-05-13 00:48:04,900 - INFO - Epoch/batch: 15/1963, ibatch: 69463, loss: \u001b[0;36m0.5191\u001b[0m, std: 0.6018\n",
      "2021-05-13 00:48:11,505 - INFO - Epoch/batch: 15/2114, ibatch: 69614, loss: \u001b[0;36m0.5261\u001b[0m, std: 0.5954\n",
      "2021-05-13 00:48:26,854 - INFO - loss: \u001b[0;32m0.5239\u001b[0m, std: 0.6025\n",
      "2021-05-13 00:48:27,357 - INFO - Epoch/batch: 15/2265, ibatch: 69765, loss: \u001b[0;36m0.5392\u001b[0m, std: 0.6243\n",
      "2021-05-13 00:48:32,653 - INFO - Epoch/batch: 15/2416, ibatch: 69916, loss: \u001b[0;36m0.5168\u001b[0m, std: 0.5912\n",
      "2021-05-13 00:48:38,267 - INFO - Epoch/batch: 15/2567, ibatch: 70067, loss: \u001b[0;36m0.5311\u001b[0m, std: 0.6143\n",
      "2021-05-13 00:48:52,377 - INFO - loss: \u001b[0;32m0.5242\u001b[0m, std: 0.6127\n",
      "2021-05-13 00:48:53,009 - INFO - Epoch/batch: 15/2718, ibatch: 70218, loss: \u001b[0;36m0.5080\u001b[0m, std: 0.5814\n",
      "2021-05-13 00:48:58,501 - INFO - Epoch/batch: 15/2869, ibatch: 70369, loss: \u001b[0;36m0.5460\u001b[0m, std: 0.6267\n",
      "2021-05-13 00:49:03,837 - INFO - Epoch/batch: 15/3020, ibatch: 70520, loss: \u001b[0;36m0.5208\u001b[0m, std: 0.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 471: ReduceOnPlateau set learning rate to 6.758519863481756e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:49:17,748 - INFO - loss: \u001b[0;32m0.5239\u001b[0m, std: 0.6074\n",
      "2021-05-13 00:49:18,571 - INFO - Epoch/batch: 15/3171, ibatch: 70671, loss: \u001b[0;36m0.5209\u001b[0m, std: 0.6052\n",
      "2021-05-13 00:49:23,975 - INFO - Epoch/batch: 15/3322, ibatch: 70822, loss: \u001b[0;36m0.5313\u001b[0m, std: 0.6053\n",
      "2021-05-13 00:49:29,401 - INFO - Epoch/batch: 15/3473, ibatch: 70973, loss: \u001b[0;36m0.5283\u001b[0m, std: 0.6064\n",
      "2021-05-13 00:49:43,373 - INFO - loss: \u001b[0;32m0.5243\u001b[0m, std: 0.5967\n",
      "2021-05-13 00:49:44,209 - INFO - Epoch/batch: 15/3624, ibatch: 71124, loss: \u001b[0;36m0.5407\u001b[0m, std: 0.6168\n",
      "2021-05-13 00:49:49,959 - INFO - Epoch/batch: 15/3775, ibatch: 71275, loss: \u001b[0;36m0.5449\u001b[0m, std: 0.6194\n",
      "2021-05-13 00:49:55,205 - INFO - Epoch/batch: 15/3926, ibatch: 71426, loss: \u001b[0;36m0.5091\u001b[0m, std: 0.5830\n",
      "2021-05-13 00:50:09,508 - INFO - loss: \u001b[0;32m0.5240\u001b[0m, std: 0.6123\n",
      "2021-05-13 00:50:10,607 - INFO - Epoch/batch: 15/4077, ibatch: 71577, loss: \u001b[0;36m0.5327\u001b[0m, std: 0.6096\n",
      "2021-05-13 00:50:16,267 - INFO - Epoch/batch: 15/4228, ibatch: 71728, loss: \u001b[0;36m0.5355\u001b[0m, std: 0.6240\n",
      "2021-05-13 00:50:21,960 - INFO - Epoch/batch: 15/4379, ibatch: 71879, loss: \u001b[0;36m0.5210\u001b[0m, std: 0.5969\n",
      "2021-05-13 00:50:35,633 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.6100\n",
      "2021-05-13 00:50:36,808 - INFO - Epoch 15 average training loss: \u001b[0;46m0.5275\u001b[0m std: 0.6064\n",
      "2021-05-13 00:50:36,812 - INFO - Epoch 15 average validate loss: \u001b[0;46m0.5239\u001b[0m std: 0.6063\n",
      "2021-05-13 00:50:38,666 - INFO - Epoch/batch: 16/   0, ibatch: 72000, loss: \u001b[0;36m0.4945\u001b[0m, std: 0.5807\n",
      "2021-05-13 00:50:48,216 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.6100\n",
      "2021-05-13 00:50:53,582 - INFO - Epoch/batch: 16/ 151, ibatch: 72151, loss: \u001b[0;36m0.5199\u001b[0m, std: 0.5998\n",
      "2021-05-13 00:50:59,063 - INFO - Epoch/batch: 16/ 302, ibatch: 72302, loss: \u001b[0;36m0.5400\u001b[0m, std: 0.6212\n",
      "2021-05-13 00:51:14,169 - INFO - loss: \u001b[0;32m0.5237\u001b[0m, std: 0.6103\n",
      "2021-05-13 00:51:14,350 - INFO - Epoch/batch: 16/ 453, ibatch: 72453, loss: \u001b[0;36m0.5332\u001b[0m, std: 0.6043\n",
      "2021-05-13 00:51:19,654 - INFO - Epoch/batch: 16/ 604, ibatch: 72604, loss: \u001b[0;36m0.5245\u001b[0m, std: 0.6097\n",
      "2021-05-13 00:51:24,766 - INFO - Epoch/batch: 16/ 755, ibatch: 72755, loss: \u001b[0;36m0.5296\u001b[0m, std: 0.6000\n",
      "2021-05-13 00:51:39,360 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.6044\n",
      "2021-05-13 00:51:39,583 - INFO - Epoch/batch: 16/ 906, ibatch: 72906, loss: \u001b[0;36m0.5110\u001b[0m, std: 0.5933\n",
      "2021-05-13 00:51:45,252 - INFO - Epoch/batch: 16/1057, ibatch: 73057, loss: \u001b[0;36m0.5154\u001b[0m, std: 0.6038\n",
      "2021-05-13 00:51:50,608 - INFO - Epoch/batch: 16/1208, ibatch: 73208, loss: \u001b[0;36m0.5356\u001b[0m, std: 0.6195\n",
      "2021-05-13 00:52:05,296 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.6056\n",
      "2021-05-13 00:52:05,624 - INFO - Epoch/batch: 16/1359, ibatch: 73359, loss: \u001b[0;36m0.5165\u001b[0m, std: 0.5984\n",
      "2021-05-13 00:52:11,333 - INFO - Epoch/batch: 16/1510, ibatch: 73510, loss: \u001b[0;36m0.5351\u001b[0m, std: 0.6159\n",
      "2021-05-13 00:52:16,898 - INFO - Epoch/batch: 16/1661, ibatch: 73661, loss: \u001b[0;36m0.5233\u001b[0m, std: 0.6064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 492: ReduceOnPlateau set learning rate to 6.0826678771335806e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:52:31,422 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.6065\n",
      "2021-05-13 00:52:31,862 - INFO - Epoch/batch: 16/1812, ibatch: 73812, loss: \u001b[0;36m0.5299\u001b[0m, std: 0.6068\n",
      "2021-05-13 00:52:37,473 - INFO - Epoch/batch: 16/1963, ibatch: 73963, loss: \u001b[0;36m0.5297\u001b[0m, std: 0.5956\n",
      "2021-05-13 00:52:42,899 - INFO - Epoch/batch: 16/2114, ibatch: 74114, loss: \u001b[0;36m0.5257\u001b[0m, std: 0.6076\n",
      "2021-05-13 00:52:57,992 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.6066\n",
      "2021-05-13 00:52:58,590 - INFO - Epoch/batch: 16/2265, ibatch: 74265, loss: \u001b[0;36m0.5259\u001b[0m, std: 0.6036\n",
      "2021-05-13 00:53:04,105 - INFO - Epoch/batch: 16/2416, ibatch: 74416, loss: \u001b[0;36m0.5332\u001b[0m, std: 0.6143\n",
      "2021-05-13 00:53:09,405 - INFO - Epoch/batch: 16/2567, ibatch: 74567, loss: \u001b[0;36m0.5170\u001b[0m, std: 0.6023\n",
      "2021-05-13 00:53:23,735 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.6014\n",
      "2021-05-13 00:53:24,389 - INFO - Epoch/batch: 16/2718, ibatch: 74718, loss: \u001b[0;36m0.5261\u001b[0m, std: 0.6025\n",
      "2021-05-13 00:53:29,716 - INFO - Epoch/batch: 16/2869, ibatch: 74869, loss: \u001b[0;36m0.5086\u001b[0m, std: 0.5867\n",
      "2021-05-13 00:53:35,300 - INFO - Epoch/batch: 16/3020, ibatch: 75020, loss: \u001b[0;36m0.5349\u001b[0m, std: 0.6213\n",
      "2021-05-13 00:53:49,138 - INFO - loss: \u001b[0;32m0.5239\u001b[0m, std: 0.5976\n",
      "2021-05-13 00:53:49,854 - INFO - Epoch/batch: 16/3171, ibatch: 75171, loss: \u001b[0;36m0.5242\u001b[0m, std: 0.6043\n",
      "2021-05-13 00:53:55,527 - INFO - Epoch/batch: 16/3322, ibatch: 75322, loss: \u001b[0;36m0.5300\u001b[0m, std: 0.6016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 503: ReduceOnPlateau set learning rate to 5.4744010894202224e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:54:01,028 - INFO - Epoch/batch: 16/3473, ibatch: 75473, loss: \u001b[0;36m0.5329\u001b[0m, std: 0.6050\n",
      "2021-05-13 00:54:14,947 - INFO - loss: \u001b[0;32m0.5236\u001b[0m, std: 0.6023\n",
      "2021-05-13 00:54:15,974 - INFO - Epoch/batch: 16/3624, ibatch: 75624, loss: \u001b[0;36m0.5393\u001b[0m, std: 0.6176\n",
      "2021-05-13 00:54:21,276 - INFO - Epoch/batch: 16/3775, ibatch: 75775, loss: \u001b[0;36m0.5112\u001b[0m, std: 0.5937\n",
      "2021-05-13 00:54:26,908 - INFO - Epoch/batch: 16/3926, ibatch: 75926, loss: \u001b[0;36m0.5432\u001b[0m, std: 0.6250\n",
      "2021-05-13 00:54:40,886 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.5993\n",
      "2021-05-13 00:54:41,978 - INFO - Epoch/batch: 16/4077, ibatch: 76077, loss: \u001b[0;36m0.5345\u001b[0m, std: 0.6123\n",
      "2021-05-13 00:54:47,734 - INFO - Epoch/batch: 16/4228, ibatch: 76228, loss: \u001b[0;36m0.5242\u001b[0m, std: 0.6000\n",
      "2021-05-13 00:54:54,282 - INFO - Epoch/batch: 16/4379, ibatch: 76379, loss: \u001b[0;36m0.5394\u001b[0m, std: 0.6151\n",
      "2021-05-13 00:55:09,153 - INFO - loss: \u001b[0;32m0.5237\u001b[0m, std: 0.6064\n",
      "2021-05-13 00:55:10,362 - INFO - Epoch 16 average training loss: \u001b[0;46m0.5274\u001b[0m std: 0.6066\n",
      "2021-05-13 00:55:10,411 - INFO - Epoch 16 average validate loss: \u001b[0;46m0.5238\u001b[0m std: 0.6046\n",
      "2021-05-13 00:55:12,123 - INFO - Epoch/batch: 17/   0, ibatch: 76500, loss: \u001b[0;36m0.5278\u001b[0m, std: 0.6116\n",
      "2021-05-13 00:55:21,475 - INFO - loss: \u001b[0;32m0.5237\u001b[0m, std: 0.6063\n",
      "2021-05-13 00:55:27,105 - INFO - Epoch/batch: 17/ 151, ibatch: 76651, loss: \u001b[0;36m0.5326\u001b[0m, std: 0.6116\n",
      "2021-05-13 00:55:32,657 - INFO - Epoch/batch: 17/ 302, ibatch: 76802, loss: \u001b[0;36m0.4995\u001b[0m, std: 0.5828\n",
      "2021-05-13 00:55:47,400 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.6056\n",
      "2021-05-13 00:55:47,534 - INFO - Epoch/batch: 17/ 453, ibatch: 76953, loss: \u001b[0;36m0.5315\u001b[0m, std: 0.6176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 514: ReduceOnPlateau set learning rate to 4.9269609804782e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:55:52,819 - INFO - Epoch/batch: 17/ 604, ibatch: 77104, loss: \u001b[0;36m0.5309\u001b[0m, std: 0.6099\n",
      "2021-05-13 00:55:58,536 - INFO - Epoch/batch: 17/ 755, ibatch: 77255, loss: \u001b[0;36m0.5481\u001b[0m, std: 0.6132\n",
      "2021-05-13 00:56:13,170 - INFO - loss: \u001b[0;32m0.5237\u001b[0m, std: 0.6035\n",
      "2021-05-13 00:56:13,412 - INFO - Epoch/batch: 17/ 906, ibatch: 77406, loss: \u001b[0;36m0.5302\u001b[0m, std: 0.6085\n",
      "2021-05-13 00:56:19,200 - INFO - Epoch/batch: 17/1057, ibatch: 77557, loss: \u001b[0;36m0.5426\u001b[0m, std: 0.6248\n",
      "2021-05-13 00:56:25,271 - INFO - Epoch/batch: 17/1208, ibatch: 77708, loss: \u001b[0;36m0.5310\u001b[0m, std: 0.6114\n",
      "2021-05-13 00:56:40,826 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.6005\n",
      "2021-05-13 00:56:41,196 - INFO - Epoch/batch: 17/1359, ibatch: 77859, loss: \u001b[0;36m0.5258\u001b[0m, std: 0.6080\n",
      "2021-05-13 00:56:46,931 - INFO - Epoch/batch: 17/1510, ibatch: 78010, loss: \u001b[0;36m0.5321\u001b[0m, std: 0.6078\n",
      "2021-05-13 00:56:52,751 - INFO - Epoch/batch: 17/1661, ibatch: 78161, loss: \u001b[0;36m0.5291\u001b[0m, std: 0.6078\n",
      "2021-05-13 00:57:07,200 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.6064\n",
      "2021-05-13 00:57:07,734 - INFO - Epoch/batch: 17/1812, ibatch: 78312, loss: \u001b[0;36m0.5206\u001b[0m, std: 0.5986\n",
      "2021-05-13 00:57:13,525 - INFO - Epoch/batch: 17/1963, ibatch: 78463, loss: \u001b[0;36m0.5313\u001b[0m, std: 0.6072\n",
      "2021-05-13 00:57:19,479 - INFO - Epoch/batch: 17/2114, ibatch: 78614, loss: \u001b[0;36m0.5119\u001b[0m, std: 0.5942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 525: ReduceOnPlateau set learning rate to 4.43426488243038e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:57:34,388 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.6096\n",
      "2021-05-13 00:57:34,860 - INFO - Epoch/batch: 17/2265, ibatch: 78765, loss: \u001b[0;36m0.5135\u001b[0m, std: 0.6005\n",
      "2021-05-13 00:57:40,791 - INFO - Epoch/batch: 17/2416, ibatch: 78916, loss: \u001b[0;36m0.5312\u001b[0m, std: 0.6097\n",
      "2021-05-13 00:57:46,705 - INFO - Epoch/batch: 17/2567, ibatch: 79067, loss: \u001b[0;36m0.5277\u001b[0m, std: 0.6158\n",
      "2021-05-13 00:58:01,368 - INFO - loss: \u001b[0;32m0.5236\u001b[0m, std: 0.6022\n",
      "2021-05-13 00:58:02,074 - INFO - Epoch/batch: 17/2718, ibatch: 79218, loss: \u001b[0;36m0.5309\u001b[0m, std: 0.6083\n",
      "2021-05-13 00:58:07,933 - INFO - Epoch/batch: 17/2869, ibatch: 79369, loss: \u001b[0;36m0.5573\u001b[0m, std: 0.6347\n",
      "2021-05-13 00:58:13,743 - INFO - Epoch/batch: 17/3020, ibatch: 79520, loss: \u001b[0;36m0.5469\u001b[0m, std: 0.6119\n",
      "2021-05-13 00:58:28,534 - INFO - loss: \u001b[0;32m0.5237\u001b[0m, std: 0.6077\n",
      "2021-05-13 00:58:29,315 - INFO - Epoch/batch: 17/3171, ibatch: 79671, loss: \u001b[0;36m0.5105\u001b[0m, std: 0.5800\n",
      "2021-05-13 00:58:34,782 - INFO - Epoch/batch: 17/3322, ibatch: 79822, loss: \u001b[0;36m0.5007\u001b[0m, std: 0.5783\n",
      "2021-05-13 00:58:40,641 - INFO - Epoch/batch: 17/3473, ibatch: 79973, loss: \u001b[0;36m0.5309\u001b[0m, std: 0.6147\n",
      "2021-05-13 00:58:55,282 - INFO - loss: \u001b[0;32m0.5236\u001b[0m, std: 0.6045\n",
      "2021-05-13 00:58:56,196 - INFO - Epoch/batch: 17/3624, ibatch: 80124, loss: \u001b[0;36m0.5315\u001b[0m, std: 0.6113\n",
      "2021-05-13 00:59:02,520 - INFO - Epoch/batch: 17/3775, ibatch: 80275, loss: \u001b[0;36m0.5327\u001b[0m, std: 0.6130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 536: ReduceOnPlateau set learning rate to 3.990838394187342e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:59:09,136 - INFO - Epoch/batch: 17/3926, ibatch: 80426, loss: \u001b[0;36m0.5187\u001b[0m, std: 0.5943\n",
      "2021-05-13 00:59:24,393 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.6106\n",
      "2021-05-13 00:59:25,487 - INFO - Epoch/batch: 17/4077, ibatch: 80577, loss: \u001b[0;36m0.5130\u001b[0m, std: 0.5986\n",
      "2021-05-13 00:59:31,240 - INFO - Epoch/batch: 17/4228, ibatch: 80728, loss: \u001b[0;36m0.5028\u001b[0m, std: 0.5924\n",
      "2021-05-13 00:59:36,604 - INFO - Epoch/batch: 17/4379, ibatch: 80879, loss: \u001b[0;36m0.5400\u001b[0m, std: 0.6140\n",
      "2021-05-13 00:59:50,070 - INFO - loss: \u001b[0;32m0.5236\u001b[0m, std: 0.6038\n",
      "2021-05-13 00:59:50,087 - INFO - Saved model states in: earlystop_0.5236.1\n",
      "2021-05-13 00:59:50,089 - INFO - Saved net python code: earlystop_0.5236.1/paddle_nets.py\n",
      "2021-05-13 00:59:50,108 - INFO - Saved best model: earlystop_0.5236.1\n",
      "2021-05-13 00:59:50,109 - INFO - Removing earlystop model: earlystop_0.5236\n",
      "2021-05-13 00:59:51,355 - INFO - Epoch 17 average training loss: \u001b[0;46m0.5271\u001b[0m std: 0.6062\n",
      "2021-05-13 00:59:51,411 - INFO - Epoch 17 average validate loss: \u001b[0;46m0.5237\u001b[0m std: 0.6055\n",
      "2021-05-13 00:59:53,256 - INFO - Epoch/batch: 18/   0, ibatch: 81000, loss: \u001b[0;36m0.5250\u001b[0m, std: 0.6042\n",
      "2021-05-13 01:00:02,830 - INFO - loss: \u001b[0;32m0.5236\u001b[0m, std: 0.6036\n",
      "2021-05-13 01:00:08,403 - INFO - Epoch/batch: 18/ 151, ibatch: 81151, loss: \u001b[0;36m0.5098\u001b[0m, std: 0.5863\n",
      "2021-05-13 01:00:14,018 - INFO - Epoch/batch: 18/ 302, ibatch: 81302, loss: \u001b[0;36m0.5124\u001b[0m, std: 0.5935\n",
      "2021-05-13 01:00:29,272 - INFO - loss: \u001b[0;32m0.5236\u001b[0m, std: 0.6061\n",
      "2021-05-13 01:00:29,403 - INFO - Epoch/batch: 18/ 453, ibatch: 81453, loss: \u001b[0;36m0.5266\u001b[0m, std: 0.6077\n",
      "2021-05-13 01:00:35,045 - INFO - Epoch/batch: 18/ 604, ibatch: 81604, loss: \u001b[0;36m0.5281\u001b[0m, std: 0.6038\n",
      "2021-05-13 01:00:40,618 - INFO - Epoch/batch: 18/ 755, ibatch: 81755, loss: \u001b[0;36m0.5478\u001b[0m, std: 0.6253\n",
      "2021-05-13 01:00:55,759 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.6008\n",
      "2021-05-13 01:00:55,981 - INFO - Epoch/batch: 18/ 906, ibatch: 81906, loss: \u001b[0;36m0.5358\u001b[0m, std: 0.6082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 547: ReduceOnPlateau set learning rate to 3.591754554768608e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 01:01:01,890 - INFO - Epoch/batch: 18/1057, ibatch: 82057, loss: \u001b[0;36m0.5216\u001b[0m, std: 0.6043\n",
      "2021-05-13 01:01:07,460 - INFO - Epoch/batch: 18/1208, ibatch: 82208, loss: \u001b[0;36m0.5256\u001b[0m, std: 0.6004\n",
      "2021-05-13 01:01:22,332 - INFO - loss: \u001b[0;32m0.5239\u001b[0m, std: 0.6019\n",
      "2021-05-13 01:01:22,669 - INFO - Epoch/batch: 18/1359, ibatch: 82359, loss: \u001b[0;36m0.5378\u001b[0m, std: 0.6181\n",
      "2021-05-13 01:01:28,505 - INFO - Epoch/batch: 18/1510, ibatch: 82510, loss: \u001b[0;36m0.5314\u001b[0m, std: 0.6098\n",
      "2021-05-13 01:01:34,364 - INFO - Epoch/batch: 18/1661, ibatch: 82661, loss: \u001b[0;36m0.5337\u001b[0m, std: 0.6062\n",
      "2021-05-13 01:01:49,250 - INFO - loss: \u001b[0;32m0.5237\u001b[0m, std: 0.6072\n",
      "2021-05-13 01:01:49,737 - INFO - Epoch/batch: 18/1812, ibatch: 82812, loss: \u001b[0;36m0.5098\u001b[0m, std: 0.5920\n",
      "2021-05-13 01:01:55,395 - INFO - Epoch/batch: 18/1963, ibatch: 82963, loss: \u001b[0;36m0.5165\u001b[0m, std: 0.5966\n",
      "2021-05-13 01:02:01,095 - INFO - Epoch/batch: 18/2114, ibatch: 83114, loss: \u001b[0;36m0.5260\u001b[0m, std: 0.6054\n",
      "2021-05-13 01:02:15,709 - INFO - loss: \u001b[0;32m0.5237\u001b[0m, std: 0.6063\n",
      "2021-05-13 01:02:16,277 - INFO - Epoch/batch: 18/2265, ibatch: 83265, loss: \u001b[0;36m0.5247\u001b[0m, std: 0.6025\n",
      "2021-05-13 01:02:22,336 - INFO - Epoch/batch: 18/2416, ibatch: 83416, loss: \u001b[0;36m0.5455\u001b[0m, std: 0.6246\n",
      "2021-05-13 01:02:28,017 - INFO - Epoch/batch: 18/2567, ibatch: 83567, loss: \u001b[0;36m0.5365\u001b[0m, std: 0.6183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 558: ReduceOnPlateau set learning rate to 3.232579099291747e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 01:02:42,489 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.6032\n",
      "2021-05-13 01:02:43,119 - INFO - Epoch/batch: 18/2718, ibatch: 83718, loss: \u001b[0;36m0.5135\u001b[0m, std: 0.5908\n",
      "2021-05-13 01:02:48,764 - INFO - Epoch/batch: 18/2869, ibatch: 83869, loss: \u001b[0;36m0.5264\u001b[0m, std: 0.5980\n",
      "2021-05-13 01:02:54,528 - INFO - Epoch/batch: 18/3020, ibatch: 84020, loss: \u001b[0;36m0.5262\u001b[0m, std: 0.6085\n",
      "2021-05-13 01:03:09,196 - INFO - loss: \u001b[0;32m0.5238\u001b[0m, std: 0.6041\n",
      "2021-05-13 01:03:09,978 - INFO - Epoch/batch: 18/3171, ibatch: 84171, loss: \u001b[0;36m0.5338\u001b[0m, std: 0.6206\n",
      "2021-05-13 01:03:16,054 - INFO - Epoch/batch: 18/3322, ibatch: 84322, loss: \u001b[0;36m0.5402\u001b[0m, std: 0.6230\n",
      "2021-05-13 01:03:21,721 - INFO - Epoch/batch: 18/3473, ibatch: 84473, loss: \u001b[0;36m0.5344\u001b[0m, std: 0.6116\n",
      "2021-05-13 01:03:36,488 - INFO - loss: \u001b[0;32m0.5236\u001b[0m, std: 0.6041\n",
      "2021-05-13 01:03:37,612 - INFO - Epoch/batch: 18/3624, ibatch: 84624, loss: \u001b[0;36m0.5216\u001b[0m, std: 0.5985\n",
      "2021-05-13 01:03:44,047 - INFO - Epoch/batch: 18/3775, ibatch: 84775, loss: \u001b[0;36m0.5280\u001b[0m, std: 0.6095\n",
      "2021-05-13 01:03:50,516 - INFO - Epoch/batch: 18/3926, ibatch: 84926, loss: \u001b[0;36m0.5370\u001b[0m, std: 0.6181\n",
      "2021-05-13 01:04:05,699 - INFO - loss: \u001b[0;32m0.5237\u001b[0m, std: 0.6099\n",
      "2021-05-13 01:04:06,676 - INFO - Epoch/batch: 18/4077, ibatch: 85077, loss: \u001b[0;36m0.5088\u001b[0m, std: 0.5856\n",
      "2021-05-13 01:04:12,210 - INFO - Epoch/batch: 18/4228, ibatch: 85228, loss: \u001b[0;36m0.5225\u001b[0m, std: 0.6019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 569: ReduceOnPlateau set learning rate to 2.9093211893625727e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 01:04:18,056 - INFO - Epoch/batch: 18/4379, ibatch: 85379, loss: \u001b[0;36m0.5223\u001b[0m, std: 0.6068\n",
      "2021-05-13 01:04:32,983 - INFO - loss: \u001b[0;32m0.5236\u001b[0m, std: 0.6069\n",
      "2021-05-13 01:04:32,998 - INFO - Saved model states in: earlystop_0.5236\n",
      "2021-05-13 01:04:33,000 - INFO - Saved net python code: earlystop_0.5236/paddle_nets.py\n",
      "2021-05-13 01:04:33,007 - INFO - Saved best model: earlystop_0.5236\n",
      "2021-05-13 01:04:33,008 - INFO - Removing earlystop model: earlystop_0.5236.1\n",
      "2021-05-13 01:04:34,408 - INFO - Epoch 18 average training loss: \u001b[0;46m0.5272\u001b[0m std: 0.6062\n",
      "2021-05-13 01:04:34,413 - INFO - Epoch 18 average validate loss: \u001b[0;46m0.5237\u001b[0m std: 0.6049\n",
      "2021-05-13 01:04:36,457 - INFO - Epoch/batch: 19/   0, ibatch: 85500, loss: \u001b[0;36m0.5341\u001b[0m, std: 0.6134\n",
      "2021-05-13 01:04:46,450 - INFO - loss: \u001b[0;32m0.5236\u001b[0m, std: 0.6069\n",
      "2021-05-13 01:04:46,522 - INFO - Saved model states in: earlystop_0.5236.1\n",
      "2021-05-13 01:04:46,524 - INFO - Saved net python code: earlystop_0.5236.1/paddle_nets.py\n",
      "2021-05-13 01:04:46,532 - INFO - Saved best model: earlystop_0.5236.1\n",
      "2021-05-13 01:04:46,533 - INFO - Removing earlystop model: earlystop_0.5236\n",
      "2021-05-13 01:04:52,144 - INFO - Epoch/batch: 19/ 151, ibatch: 85651, loss: \u001b[0;36m0.5176\u001b[0m, std: 0.5903\n",
      "2021-05-13 01:04:57,815 - INFO - Epoch/batch: 19/ 302, ibatch: 85802, loss: \u001b[0;36m0.5234\u001b[0m, std: 0.6078\n",
      "2021-05-13 01:05:13,283 - INFO - loss: \u001b[0;32m0.5236\u001b[0m, std: 0.6033\n",
      "2021-05-13 01:05:13,440 - INFO - Epoch/batch: 19/ 453, ibatch: 85953, loss: \u001b[0;36m0.5379\u001b[0m, std: 0.6215\n",
      "2021-05-13 01:05:19,266 - INFO - Epoch/batch: 19/ 604, ibatch: 86104, loss: \u001b[0;36m0.5120\u001b[0m, std: 0.5916\n",
      "2021-05-13 01:05:24,982 - INFO - Epoch/batch: 19/ 755, ibatch: 86255, loss: \u001b[0;36m0.5276\u001b[0m, std: 0.6119\n",
      "2021-05-13 01:05:40,375 - INFO - loss: \u001b[0;32m0.5236\u001b[0m, std: 0.6043\n",
      "2021-05-13 01:05:40,611 - INFO - Epoch/batch: 19/ 906, ibatch: 86406, loss: \u001b[0;36m0.5073\u001b[0m, std: 0.5937\n",
      "2021-05-13 01:05:46,412 - INFO - Epoch/batch: 19/1057, ibatch: 86557, loss: \u001b[0;36m0.5254\u001b[0m, std: 0.6055\n",
      "2021-05-13 01:05:52,381 - INFO - Epoch/batch: 19/1208, ibatch: 86708, loss: \u001b[0;36m0.5529\u001b[0m, std: 0.6262\n",
      "2021-05-13 01:06:07,225 - INFO - loss: \u001b[0;32m0.5235\u001b[0m, std: 0.6037\n",
      "2021-05-13 01:06:07,240 - INFO - Saved model states in: earlystop_0.5235\n",
      "2021-05-13 01:06:07,241 - INFO - Saved net python code: earlystop_0.5235/paddle_nets.py\n",
      "2021-05-13 01:06:07,247 - INFO - Saved best model: earlystop_0.5235\n",
      "2021-05-13 01:06:07,248 - INFO - Removing earlystop model: earlystop_0.5236.1\n",
      "2021-05-13 01:06:07,674 - INFO - Epoch/batch: 19/1359, ibatch: 86859, loss: \u001b[0;36m0.5306\u001b[0m, std: 0.6078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 580: ReduceOnPlateau set learning rate to 2.6183890704263157e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 01:06:13,444 - INFO - Epoch/batch: 19/1510, ibatch: 87010, loss: \u001b[0;36m0.5104\u001b[0m, std: 0.5897\n",
      "2021-05-13 01:06:19,224 - INFO - Epoch/batch: 19/1661, ibatch: 87161, loss: \u001b[0;36m0.5301\u001b[0m, std: 0.6105\n",
      "2021-05-13 01:06:34,234 - INFO - loss: \u001b[0;32m0.5236\u001b[0m, std: 0.6063\n",
      "2021-05-13 01:06:34,689 - INFO - Epoch/batch: 19/1812, ibatch: 87312, loss: \u001b[0;36m0.5385\u001b[0m, std: 0.6170\n",
      "2021-05-13 01:06:40,363 - INFO - Epoch/batch: 19/1963, ibatch: 87463, loss: \u001b[0;36m0.5301\u001b[0m, std: 0.6040\n",
      "2021-05-13 01:06:46,042 - INFO - Epoch/batch: 19/2114, ibatch: 87614, loss: \u001b[0;36m0.5321\u001b[0m, std: 0.6155\n",
      "2021-05-13 01:07:01,290 - INFO - loss: \u001b[0;32m0.5236\u001b[0m, std: 0.6036\n",
      "2021-05-13 01:07:01,883 - INFO - Epoch/batch: 19/2265, ibatch: 87765, loss: \u001b[0;36m0.5109\u001b[0m, std: 0.5852\n",
      "2021-05-13 01:07:07,470 - INFO - Epoch/batch: 19/2416, ibatch: 87916, loss: \u001b[0;36m0.5027\u001b[0m, std: 0.5860\n",
      "2021-05-13 01:07:13,085 - INFO - Epoch/batch: 19/2567, ibatch: 88067, loss: \u001b[0;36m0.5188\u001b[0m, std: 0.6004\n",
      "2021-05-13 01:07:27,868 - INFO - loss: \u001b[0;32m0.5237\u001b[0m, std: 0.6055\n",
      "2021-05-13 01:07:28,490 - INFO - Epoch/batch: 19/2718, ibatch: 88218, loss: \u001b[0;36m0.5470\u001b[0m, std: 0.6256\n",
      "2021-05-13 01:07:33,524 - INFO - Epoch/batch: 19/2869, ibatch: 88369, loss: \u001b[0;36m0.5108\u001b[0m, std: 0.5946\n",
      "2021-05-13 01:07:39,100 - INFO - Epoch/batch: 19/3020, ibatch: 88520, loss: \u001b[0;36m0.5388\u001b[0m, std: 0.6147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 591: ReduceOnPlateau set learning rate to 2.356550163383684e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 01:07:53,471 - INFO - loss: \u001b[0;32m0.5237\u001b[0m, std: 0.6023\n",
      "2021-05-13 01:07:54,356 - INFO - Epoch/batch: 19/3171, ibatch: 88671, loss: \u001b[0;36m0.5362\u001b[0m, std: 0.6142\n",
      "2021-05-13 01:08:00,075 - INFO - Epoch/batch: 19/3322, ibatch: 88822, loss: \u001b[0;36m0.5251\u001b[0m, std: 0.6098\n",
      "2021-05-13 01:08:05,864 - INFO - Epoch/batch: 19/3473, ibatch: 88973, loss: \u001b[0;36m0.5223\u001b[0m, std: 0.5976\n",
      "2021-05-13 01:08:20,229 - INFO - loss: \u001b[0;32m0.5236\u001b[0m, std: 0.6022\n",
      "2021-05-13 01:08:21,287 - INFO - Epoch/batch: 19/3624, ibatch: 89124, loss: \u001b[0;36m0.5383\u001b[0m, std: 0.6102\n",
      "2021-05-13 01:08:27,443 - INFO - Epoch/batch: 19/3775, ibatch: 89275, loss: \u001b[0;36m0.5370\u001b[0m, std: 0.6147\n",
      "2021-05-13 01:08:33,471 - INFO - Epoch/batch: 19/3926, ibatch: 89426, loss: \u001b[0;36m0.5331\u001b[0m, std: 0.6025\n",
      "2021-05-13 01:08:48,143 - INFO - loss: \u001b[0;32m0.5236\u001b[0m, std: 0.6026\n",
      "2021-05-13 01:08:49,317 - INFO - Epoch/batch: 19/4077, ibatch: 89577, loss: \u001b[0;36m0.5417\u001b[0m, std: 0.6253\n",
      "2021-05-13 01:08:55,081 - INFO - Epoch/batch: 19/4228, ibatch: 89728, loss: \u001b[0;36m0.5236\u001b[0m, std: 0.5970\n",
      "2021-05-13 01:09:01,157 - INFO - Epoch/batch: 19/4379, ibatch: 89879, loss: \u001b[0;36m0.5349\u001b[0m, std: 0.6186\n",
      "2021-05-13 01:09:15,887 - INFO - loss: \u001b[0;32m0.5236\u001b[0m, std: 0.6048\n",
      "2021-05-13 01:09:17,209 - INFO - Epoch 19 average training loss: \u001b[0;46m0.5273\u001b[0m std: 0.6064\n",
      "2021-05-13 01:09:17,213 - INFO - Epoch 19 average validate loss: \u001b[0;46m0.5236\u001b[0m std: 0.6041\n",
      "2021-05-13 01:09:17,217 - WARNING - Validate loss changed < 0.001 for 11 consecutive epochs, stopping!!!\n"
     ]
    }
   ],
   "source": [
    "# 训练模型， 最后的loss应该在[0.52, 0.53]区间内. \n",
    "# 每epoch需要五分钟左右(在CPU上)， 自然结束需要～20个epoch\n",
    "train_loss, valid_loss = fp.train(model, train_data, num_epochs=21, validate_callback = fp.func_partial(fp.validate_in_train, midata=valid_data, save_dir='./'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 01:09:17,352 - INFO - Loading model states from: earlystop_0.5235\n",
      "2021-05-13 01:09:17,356 - INFO - Loaded net state: earlystop_0.5235/net.state\n",
      "2021-05-13 01:09:17,360 - WARNING - Error in optim state_dict loading!\n",
      "2021-05-13 01:09:17,361 - INFO - Getting loss function: ['softmax+mse']\n",
      "2021-05-13 01:09:17,362 - INFO - Validating, data size: 500\n",
      "2021-05-13 01:09:17,362 - INFO -            batch size: 64\n",
      "2021-05-13 01:09:17,363 - INFO -               shuffle: False\n",
      "2021-05-13 01:09:17,363 - INFO -          # of batches: 8\n",
      "2021-05-13 01:09:17,363 - INFO -        recap interval: 1\n",
      "2021-05-13 01:09:17,364 - INFO -          loss padding: False\n",
      "2021-05-13 01:09:20,484 - INFO - ibatch:    0, loss: 0.2294, std: 0.3232\n",
      "2021-05-13 01:09:21,226 - INFO - ibatch:    1, loss: 0.2217, std: 0.3163\n",
      "2021-05-13 01:09:21,963 - INFO - ibatch:    2, loss: 0.2037, std: 0.2905\n",
      "2021-05-13 01:09:22,692 - INFO - ibatch:    3, loss: 0.2030, std: 0.2957\n",
      "2021-05-13 01:09:23,431 - INFO - ibatch:    4, loss: 0.1891, std: 0.2720\n",
      "2021-05-13 01:09:24,164 - INFO - ibatch:    5, loss: 0.2037, std: 0.2915\n",
      "2021-05-13 01:09:24,897 - INFO - ibatch:    6, loss: 0.2184, std: 0.3114\n",
      "2021-05-13 01:09:25,494 - INFO - ibatch:    7, loss: 0.2140, std: 0.2987\n",
      "2021-05-13 01:09:26,574 - INFO - Validate mean: \u001b[0;46m0.2103\u001b[0m, std: 0.1046\n"
     ]
    }
   ],
   "source": [
    "# 读取最后一个checkpoint目录 (忽略优化器state_dict读取错误)\n",
    "# read the last saved earlystop folder （ignore the error in optimizer state_dict loading)\n",
    "fp.state_dict_load(model, model.validate_hist.saved_dirs[-1])\n",
    "# 可以改动损失函数，检测mse损失\n",
    "args.loss_fn = ['softmax+mse']\n",
    "model.loss_fn = fp.get_loss_fn(args)\n",
    "valid_loss = fp.validate(model, valid_data, verbose=1, batch_size=64) # try a larger batch_size, should make no difference though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 01:09:26,581 - INFO - Loading data: work/data/predict.pkl\n",
      "2021-05-13 01:09:26,583 - INFO -    # of data: 112,  max seqlen: 861, user seq_length: -1\n",
      "2021-05-13 01:09:26,584 - INFO -  residue fmt: vector, nn: 0, dbn: True, attr: False, genre: upp\n",
      "2021-05-13 01:09:29,946 - INFO - Predicting, data size: 112\n",
      "2021-05-13 01:09:29,948 - INFO -            batch size: 1\n",
      "2021-05-13 01:09:29,949 - INFO -               shuffle: False\n",
      "2021-05-13 01:09:29,949 - INFO -          # of batches: 112\n",
      "2021-05-13 01:09:29,950 - INFO -        recap interval: 4\n",
      "2021-05-13 01:09:29,951 - INFO - Predicted files will be saved in: predict.files\n",
      "100%|██████████| 112/112 [00:06<00:00, 16.26it/s]\n",
      "2021-05-13 01:09:36,846 - INFO - Completed prediction of 112 samples\n"
     ]
    }
   ],
   "source": [
    "# 读取预测数据， 存储预测结果\n",
    "# 提交的结果是平均了三次运行最好checkpoint存储， 它们分别得到了0.24, 0.24, 0.242的sqrt(mse)损失， 平均后得到了0.238\n",
    "# Read in prediction data, and save the predicted results\n",
    "predict_data = fp.get_midata(args, data_name='predict', seq_length=-1)\n",
    "y_model, std_model = fp.predict(model, predict_data, save_dir='predict.files', batch_size=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
