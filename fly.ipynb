{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "# !mkdir /home/aistudio/external-libraries\n",
    "# !pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 程序在/work/code目录下， 加入路径\n",
    "import sys \n",
    "sys.path.append('/home/aistudio/work/code')\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "# fly_paddle是唯一需要的模块\n",
    "# fly_paddle is the only module users need to interact with\n",
    "import fly_paddle as fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"action\": \"train\",\n",
      "   \"argv\": \"-h\",\n",
      "   \"verbose\": 1,\n",
      "   \"resume\": false,\n",
      "   \"load_dir\": null,\n",
      "   \"save_dir\": null,\n",
      "   \"save_level\": 2,\n",
      "   \"save_grpby\": [\"epoch\", \"batch\"],\n",
      "   \"log\": \"fly_paddle-May12.log\",\n",
      "   \"data_args\": \"======= data args =======\",\n",
      "   \"data_dir\": \"data\",\n",
      "   \"data_name\": \"predict\",\n",
      "   \"data_suffix\": \".pkl\",\n",
      "   \"data_size\": 0,\n",
      "   \"test_size\": 0.1,\n",
      "   \"split_seed\": null,\n",
      "   \"input_genre\": \"Seq\",\n",
      "   \"input_fmt\": \"NLC\",\n",
      "   \"seq_length\": [0, 512, -1],\n",
      "   \"residue_fmt\": \"vector\",\n",
      "   \"residue_nn\": 0,\n",
      "   \"residue_dbn\": false,\n",
      "   \"residue_attr\": false,\n",
      "   \"residue_extra\": false,\n",
      "   \"label_genre\": \"upp\",\n",
      "   \"label_fmt\": \"NL\",\n",
      "   \"label_tone\": \"none\",\n",
      "   \"label_ntype\": 2,\n",
      "   \"label_smooth\": false,\n",
      "   \"net_args\": \"======= net args =======\",\n",
      "   \"net_src_file\": \"/home/aistudio/work/code/paddle_nets.py\",\n",
      "   \"net\": \"lazylinear\",\n",
      "   \"resnet\": false,\n",
      "   \"act_fn\": \"relu\",\n",
      "   \"norm_fn\": \"none\",\n",
      "   \"norm_axis\": -1,\n",
      "   \"dropout\": 0.2,\n",
      "   \"feature_dim\": 1,\n",
      "   \"embed_dim\": 32,\n",
      "   \"embed_num\": 1,\n",
      "   \"linear_num\": 2,\n",
      "   \"linear_dim\": [32],\n",
      "   \"linear_resnet\": false,\n",
      "   \"conv1d_num\": 1,\n",
      "   \"conv1d_dim\": [32],\n",
      "   \"conv1d_resnet\": false,\n",
      "   \"conv1d_stride\": 1,\n",
      "   \"conv2d_num\": 1,\n",
      "   \"conv2d_dim\": [32],\n",
      "   \"conv2d_resnet\": false,\n",
      "   \"attn_num\": 2,\n",
      "   \"attn_nhead\": 2,\n",
      "   \"attn_act\": \"relu\",\n",
      "   \"attn_dropout\": null,\n",
      "   \"attn_ffdim\": 32,\n",
      "   \"attn_ffdropout\": null,\n",
      "   \"lstm_num\": 2,\n",
      "   \"lstm_dim\": [32],\n",
      "   \"lstm_direct\": 2,\n",
      "   \"lstm_resnet\": false,\n",
      "   \"output_num\": 1,\n",
      "   \"output_dim\": [32, 32, 2],\n",
      "   \"output_resnet\": false,\n",
      "   \"optim_args\": \"======= optim args =======\",\n",
      "   \"optim\": \"adam\",\n",
      "   \"learning_rate\": 0.003,\n",
      "   \"beta1\": 0.9,\n",
      "   \"beta2\": 0.999,\n",
      "   \"epsilon\": 1e-08,\n",
      "   \"lr_scheduler\": \"reduced\",\n",
      "   \"lr_factor\": 0.9,\n",
      "   \"lr_patience\": 10,\n",
      "   \"weight_decay\": \"none\",\n",
      "   \"l1decay\": 0.0001,\n",
      "   \"l2decay\": 0.0001,\n",
      "   \"train_args\": \"======= train/loss args =======\",\n",
      "   \"batch_size\": 4,\n",
      "   \"num_epochs\": 777,\n",
      "   \"num_recaps_per_epoch\": 30,\n",
      "   \"num_callbacks_per_epoch\": 10,\n",
      "   \"loss_fn\": [\"mse\"],\n",
      "   \"loss_fn_scale\": [1],\n",
      "   \"loss_sqrt\": false,\n",
      "   \"loss_padding\": false,\n",
      "   \"validate_callback\": null,\n",
      "   \"trainloss_rdiff\": 0.001,\n",
      "   \"validloss_rdiff\": 0.001,\n",
      "   \"trainloss_patience\": 11,\n",
      "   \"validloss_patience\": 11,\n",
      "   \"mood_args\": \"======= mood args =======\",\n",
      "   \"debug\": false,\n",
      "   \"lucky\": false,\n",
      "   \"lazy\": false,\n",
      "   \"sharp\": false,\n",
      "   \"comfort\": false,\n",
      "   \"explore\": false,\n",
      "   \"exploit\": false,\n",
      "   \"diehard\": false,\n",
      "   \"tune\": false,\n",
      "   \"action_args\": \"======= action args =======\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# args 几乎包括所有需要的参数\n",
    "# fp.parse_args2() 根据任务初始化args, 要用到的任务包括： ‘train', 'predict'\n",
    "# args is a structure containing most (if not all) parameters\n",
    "# fp.parse_args2() initializes args based on the task to run, such as \"train\", \"predict\"\n",
    "args, _ = fp.parse_args2('train')\n",
    "fp.misc.logging_config(logging, logfile=args.log, lineno=False, level=args.verbose) # \n",
    "print(fp.gwio.json_str(args.__dict__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 注意： 根据不同的网络等等需要， args可能包含一些用不到的参数\n",
    "# Attention: some parameters in args may not be used depending on the network etc.\n",
    "# 两种更新args的方法： 1） args.update(**dict), 2) args.[key] = value\n",
    "# Two main ways to update values in args: 1) args.update(**dict), 2) args.[key] = value\n",
    "args.update(data_dir='work/data', data_name='train', residue_dbn=True, residue_extra=True)\n",
    "\n",
    "# 网络参数 （net parameters): \n",
    "# 除非特意说明， 所有的隐藏层的维度为32. \n",
    "# 训练中发现高维度和深度的网络并不能给出更好的结果！\n",
    "# The dimensions of all hidden layers are 32 unless noted otherwise\n",
    "# Larger and deeper nets gave similar, but no better, performances!\n",
    "args.net='seq2seq_attnlstmconv1d'  # the net name defined in paddle_nets.py\n",
    "args.linear_num = 1 # the number of linear feedforward layers\n",
    "args.attn_num = 1 # the number of transformer encoder layers\n",
    "args.lstm_num = 1 # the number of bidirectional lstm layers\n",
    "args.conv1d_num = 1 # the number of 1D convolutional layers\n",
    "# 输出模块由三个线性层组成， 维度分别为32, 32, 2\n",
    "# three linear layers for the final output, with dimensions of 32, 32, and 2, respectively\n",
    "args.output_dim = [32, 32, 2] \n",
    "args.norm_fn = 'layer' # layer normalization\n",
    "args.batch_size = 1 # 1 is used in consideration of the layer norm above\n",
    "# 最后递交用的损失函数选为 softmax+bce, 也可以用 softmax+mse, 结果几乎一样\n",
    "args.loss_fn = ['softmax+bce'] # softmax is needed here as the final output has a dimension of 2\n",
    "args.label_tone = 'soft'\n",
    "args.loss_sqrt = True # sqrt(loss) is only necessary for softmax+mse\n",
    "args.loss_padding = False # exclude padded residues from loss\n",
    "# 需要运行autoconfig_args()来消除参数的不一致性\n",
    "# autoconfig_args() is needed to resolve inconsistencies between parameters\n",
    "args = fp.autoconfig_args(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 22:34:38,403 - INFO - Used net definition: \u001b[0;39;46m/home/aistudio/work/code/paddle_nets.py\u001b[0m\n",
      "2021-05-12 22:34:38,487 - INFO - {'total_params': 36418, 'trainable_params': 36418}\n",
      "2021-05-12 22:34:38,488 - INFO - Optimizer method: adam\n",
      "2021-05-12 22:34:38,489 - INFO -    learning rate: 0.003\n",
      "2021-05-12 22:34:38,489 - INFO -     lr_scheduler: reduced\n",
      "2021-05-12 22:34:38,489 - INFO -     weight decay: none\n",
      "2021-05-12 22:34:38,490 - INFO -          l1decay: 0.0001\n",
      "2021-05-12 22:34:38,490 - INFO -          l2decay: 0.0001\n",
      "2021-05-12 22:34:38,490 - INFO - Getting loss function: ['softmax+bce']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "      Layer (type)                          Input Shape                                  Output Shape                   Param #    \n",
      "=====================================================================================================================================\n",
      "   MyEmbeddingLayer-1                      [[2, 512, 10]]                                [2, 512, 10]                      0       \n",
      "        Linear-1                           [[2, 512, 10]]                                [2, 512, 32]                     352      \n",
      "         ReLU-1                            [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-1                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Dropout-1                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "     MyLinearTower-1                       [[2, 512, 10]]                                [2, 512, 32]                      0       \n",
      "    PositionEncoder-1                      [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-2                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Linear-2                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Linear-3                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Linear-4                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Linear-5                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "  MultiHeadAttention-1    [[2, 512, 32], [2, 512, 32], [2, 512, 32], None]               [2, 512, 32]                      0       \n",
      "        Dropout-3                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-3                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Linear-6                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Dropout-2                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "        Linear-7                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Dropout-4                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "TransformerEncoderLayer-1                  [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "  TransformerEncoder-1                  [[2, 512, 32], None]                             [2, 512, 32]                      0       \n",
      "      MyAttnTower-1                        [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "         LSTM-1                            [[2, 512, 32]]                  [[2, 512, 64], [[2, 2, 32], [2, 2, 32]]]     16,896     \n",
      "      MyLSTMTower-1                        [[2, 512, 32]]                                [2, 512, 64]                      0       \n",
      "        Conv1D-1                           [[2, 512, 64]]                                [2, 512, 32]                   10,272     \n",
      "         ReLU-2                            [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-4                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Dropout-5                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "     MyConv1DTower-1                       [[2, 512, 64]]                                [2, 512, 32]                      0       \n",
      "        Linear-8                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "         ReLU-3                            [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-5                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Dropout-6                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "        Linear-9                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "         ReLU-4                            [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-6                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Dropout-7                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "        Linear-10                          [[2, 512, 32]]                                [2, 512, 2]                      66       \n",
      "     MyLinearTower-2                       [[2, 512, 32]]                                [2, 512, 2]                       0       \n",
      "=====================================================================================================================================\n",
      "Total params: 36,418\n",
      "Trainable params: 36,418\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 9.73\n",
      "Params size (MB): 0.14\n",
      "Estimated Total Size (MB): 9.91\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 建立和检测模型 （Get and inspect the model）\n",
    "model = fp.get_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 22:34:38,496 - INFO - Loading data: work/data/train.pkl\n",
      "2021-05-12 22:34:38,545 - INFO -    # of data: 5000,  max seqlen: 500, user seq_length: [0, 512, -1]\n",
      "2021-05-12 22:34:38,546 - INFO -  residue fmt: vector, nn: 0, dbn: True, attr: False, genre: upp\n",
      "2021-05-12 22:34:38,567 - INFO - Selected 5000 data sets with length range: [0, 512, -1]\n",
      "2021-05-12 22:34:43,111 - INFO - Processing upp data...\n"
     ]
    }
   ],
   "source": [
    "# 读取数据 （read in data)\n",
    "midata = fp.get_midata(args)\n",
    "train_data, valid_data = fp.train_test_split(midata, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 22:34:43,410 - INFO - Training, data size: 4500\n",
      "2021-05-12 22:34:43,412 - INFO -          batch size: 1\n",
      "2021-05-12 22:34:43,412 - INFO -             shuffle: True\n",
      "2021-05-12 22:34:43,413 - INFO -        # of batches: 4500\n",
      "2021-05-12 22:34:43,414 - INFO -      recap interval: 151\n",
      "2021-05-12 22:34:43,414 - INFO -   validate interval: 450\n",
      "2021-05-12 22:34:43,609 - INFO -         # of epochs: 7\n",
      "2021-05-12 22:34:43,610 - INFO -        loss padding: False\n",
      "2021-05-12 22:34:44,661 - INFO - Epoch/batch: 0/   0, ibatch:    0, loss: \u001b[0;36m0.8624\u001b[0m, std: 0.4525\n",
      "2021-05-12 22:34:52,033 - INFO - loss: \u001b[0;32m0.8656\u001b[0m, std: 0.4468\n",
      "2021-05-12 22:34:57,985 - INFO - Epoch/batch: 0/ 151, ibatch:  151, loss: \u001b[0;36m0.6957\u001b[0m, std: 0.6241\n",
      "2021-05-12 22:35:03,903 - INFO - Epoch/batch: 0/ 302, ibatch:  302, loss: \u001b[0;36m0.6100\u001b[0m, std: 0.6832\n",
      "2021-05-12 22:35:19,032 - INFO - loss: \u001b[0;32m0.5681\u001b[0m, std: 0.5923\n",
      "2021-05-12 22:35:19,050 - INFO - Saved model states in: earlystop_0.5681\n",
      "2021-05-12 22:35:19,051 - INFO - Saved net python code: earlystop_0.5681/paddle_nets.py\n",
      "2021-05-12 22:35:19,061 - INFO - Saved best model: earlystop_0.5681\n",
      "2021-05-12 22:35:19,172 - INFO - Epoch/batch: 0/ 453, ibatch:  453, loss: \u001b[0;36m0.5812\u001b[0m, std: 0.6576\n",
      "2021-05-12 22:35:24,457 - INFO - Epoch/batch: 0/ 604, ibatch:  604, loss: \u001b[0;36m0.5711\u001b[0m, std: 0.6513\n",
      "2021-05-12 22:35:29,865 - INFO - Epoch/batch: 0/ 755, ibatch:  755, loss: \u001b[0;36m0.5631\u001b[0m, std: 0.6464\n",
      "2021-05-12 22:35:44,793 - INFO - loss: \u001b[0;32m0.5646\u001b[0m, std: 0.6968\n",
      "2021-05-12 22:35:44,817 - INFO - Saved model states in: earlystop_0.5646\n",
      "2021-05-12 22:35:44,819 - INFO - Saved net python code: earlystop_0.5646/paddle_nets.py\n",
      "2021-05-12 22:35:44,826 - INFO - Saved best model: earlystop_0.5646\n",
      "2021-05-12 22:35:44,827 - INFO - Removing earlystop model: earlystop_0.5681\n",
      "2021-05-12 22:35:44,986 - INFO - Epoch/batch: 0/ 906, ibatch:  906, loss: \u001b[0;36m0.5466\u001b[0m, std: 0.6401\n",
      "2021-05-12 22:35:50,398 - INFO - Epoch/batch: 0/1057, ibatch: 1057, loss: \u001b[0;36m0.5641\u001b[0m, std: 0.6387\n",
      "2021-05-12 22:35:56,199 - INFO - Epoch/batch: 0/1208, ibatch: 1208, loss: \u001b[0;36m0.5753\u001b[0m, std: 0.6594\n",
      "2021-05-12 22:36:11,402 - INFO - loss: \u001b[0;32m0.5672\u001b[0m, std: 0.5567\n",
      "2021-05-12 22:36:11,736 - INFO - Epoch/batch: 0/1359, ibatch: 1359, loss: \u001b[0;36m0.5787\u001b[0m, std: 0.6509\n",
      "2021-05-12 22:36:17,299 - INFO - Epoch/batch: 0/1510, ibatch: 1510, loss: \u001b[0;36m0.5358\u001b[0m, std: 0.6274\n",
      "2021-05-12 22:36:23,022 - INFO - Epoch/batch: 0/1661, ibatch: 1661, loss: \u001b[0;36m0.5830\u001b[0m, std: 0.6473\n",
      "2021-05-12 22:36:37,808 - INFO - loss: \u001b[0;32m0.5475\u001b[0m, std: 0.6298\n",
      "2021-05-12 22:36:37,824 - INFO - Saved model states in: earlystop_0.5475\n",
      "2021-05-12 22:36:37,826 - INFO - Saved net python code: earlystop_0.5475/paddle_nets.py\n",
      "2021-05-12 22:36:37,833 - INFO - Saved best model: earlystop_0.5475\n",
      "2021-05-12 22:36:37,833 - INFO - Removing earlystop model: earlystop_0.5646\n",
      "2021-05-12 22:36:38,283 - INFO - Epoch/batch: 0/1812, ibatch: 1812, loss: \u001b[0;36m0.5591\u001b[0m, std: 0.6315\n",
      "2021-05-12 22:36:44,161 - INFO - Epoch/batch: 0/1963, ibatch: 1963, loss: \u001b[0;36m0.5640\u001b[0m, std: 0.6313\n",
      "2021-05-12 22:36:49,856 - INFO - Epoch/batch: 0/2114, ibatch: 2114, loss: \u001b[0;36m0.5823\u001b[0m, std: 0.6502\n",
      "2021-05-12 22:37:04,831 - INFO - loss: \u001b[0;32m0.5464\u001b[0m, std: 0.6217\n",
      "2021-05-12 22:37:04,846 - INFO - Saved model states in: earlystop_0.5464\n",
      "2021-05-12 22:37:04,847 - INFO - Saved net python code: earlystop_0.5464/paddle_nets.py\n",
      "2021-05-12 22:37:04,853 - INFO - Saved best model: earlystop_0.5464\n",
      "2021-05-12 22:37:04,854 - INFO - Removing earlystop model: earlystop_0.5475\n",
      "2021-05-12 22:37:05,438 - INFO - Epoch/batch: 0/2265, ibatch: 2265, loss: \u001b[0;36m0.5565\u001b[0m, std: 0.6387\n",
      "2021-05-12 22:37:11,208 - INFO - Epoch/batch: 0/2416, ibatch: 2416, loss: \u001b[0;36m0.5634\u001b[0m, std: 0.6418\n",
      "2021-05-12 22:37:16,943 - INFO - Epoch/batch: 0/2567, ibatch: 2567, loss: \u001b[0;36m0.5360\u001b[0m, std: 0.6169\n",
      "2021-05-12 22:37:31,674 - INFO - loss: \u001b[0;32m0.5457\u001b[0m, std: 0.6504\n",
      "2021-05-12 22:37:31,693 - INFO - Saved model states in: earlystop_0.5457\n",
      "2021-05-12 22:37:31,694 - INFO - Saved net python code: earlystop_0.5457/paddle_nets.py\n",
      "2021-05-12 22:37:31,702 - INFO - Saved best model: earlystop_0.5457\n",
      "2021-05-12 22:37:31,702 - INFO - Removing earlystop model: earlystop_0.5464\n",
      "2021-05-12 22:37:32,335 - INFO - Epoch/batch: 0/2718, ibatch: 2718, loss: \u001b[0;36m0.5594\u001b[0m, std: 0.6439\n",
      "2021-05-12 22:37:38,014 - INFO - Epoch/batch: 0/2869, ibatch: 2869, loss: \u001b[0;36m0.5681\u001b[0m, std: 0.6388\n",
      "2021-05-12 22:37:43,720 - INFO - Epoch/batch: 0/3020, ibatch: 3020, loss: \u001b[0;36m0.5529\u001b[0m, std: 0.6359\n",
      "2021-05-12 22:37:58,559 - INFO - loss: \u001b[0;32m0.5445\u001b[0m, std: 0.6428\n",
      "2021-05-12 22:37:58,574 - INFO - Saved model states in: earlystop_0.5445\n",
      "2021-05-12 22:37:58,576 - INFO - Saved net python code: earlystop_0.5445/paddle_nets.py\n",
      "2021-05-12 22:37:58,582 - INFO - Saved best model: earlystop_0.5445\n",
      "2021-05-12 22:37:58,583 - INFO - Removing earlystop model: earlystop_0.5457\n",
      "2021-05-12 22:37:59,353 - INFO - Epoch/batch: 0/3171, ibatch: 3171, loss: \u001b[0;36m0.5479\u001b[0m, std: 0.6273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: ReduceOnPlateau set learning rate to 0.0027.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 22:38:05,350 - INFO - Epoch/batch: 0/3322, ibatch: 3322, loss: \u001b[0;36m0.5692\u001b[0m, std: 0.6381\n",
      "2021-05-12 22:38:11,037 - INFO - Epoch/batch: 0/3473, ibatch: 3473, loss: \u001b[0;36m0.5547\u001b[0m, std: 0.6376\n",
      "2021-05-12 22:38:25,727 - INFO - loss: \u001b[0;32m0.5463\u001b[0m, std: 0.6225\n",
      "2021-05-12 22:38:26,682 - INFO - Epoch/batch: 0/3624, ibatch: 3624, loss: \u001b[0;36m0.5531\u001b[0m, std: 0.6339\n",
      "2021-05-12 22:38:32,134 - INFO - Epoch/batch: 0/3775, ibatch: 3775, loss: \u001b[0;36m0.5434\u001b[0m, std: 0.6169\n",
      "2021-05-12 22:38:38,093 - INFO - Epoch/batch: 0/3926, ibatch: 3926, loss: \u001b[0;36m0.5442\u001b[0m, std: 0.6187\n",
      "2021-05-12 22:38:52,639 - INFO - loss: \u001b[0;32m0.5446\u001b[0m, std: 0.5813\n",
      "2021-05-12 22:38:53,676 - INFO - Epoch/batch: 0/4077, ibatch: 4077, loss: \u001b[0;36m0.5489\u001b[0m, std: 0.6292\n",
      "2021-05-12 22:38:59,358 - INFO - Epoch/batch: 0/4228, ibatch: 4228, loss: \u001b[0;36m0.5147\u001b[0m, std: 0.6061\n",
      "2021-05-12 22:39:04,992 - INFO - Epoch/batch: 0/4379, ibatch: 4379, loss: \u001b[0;36m0.5401\u001b[0m, std: 0.6170\n",
      "2021-05-12 22:39:19,286 - INFO - loss: \u001b[0;32m0.5410\u001b[0m, std: 0.5971\n",
      "2021-05-12 22:39:19,301 - INFO - Saved model states in: earlystop_0.5410\n",
      "2021-05-12 22:39:19,303 - INFO - Saved net python code: earlystop_0.5410/paddle_nets.py\n",
      "2021-05-12 22:39:19,309 - INFO - Saved best model: earlystop_0.5410\n",
      "2021-05-12 22:39:19,309 - INFO - Removing earlystop model: earlystop_0.5445\n",
      "2021-05-12 22:39:19,746 - INFO - Epoch 0 average training loss: \u001b[0;46m0.5638\u001b[0m std: 0.6370\n",
      "2021-05-12 22:39:19,908 - INFO - Epoch 0 average validate loss: \u001b[0;46m0.5801\u001b[0m std: 0.6035\n",
      "2021-05-12 22:39:22,005 - INFO - Epoch/batch: 1/   0, ibatch: 4500, loss: \u001b[0;36m0.5430\u001b[0m, std: 0.6290\n",
      "2021-05-12 22:39:31,626 - INFO - loss: \u001b[0;32m0.5418\u001b[0m, std: 0.5964\n",
      "2021-05-12 22:39:37,439 - INFO - Epoch/batch: 1/ 151, ibatch: 4651, loss: \u001b[0;36m0.5584\u001b[0m, std: 0.6402\n",
      "2021-05-12 22:39:42,867 - INFO - Epoch/batch: 1/ 302, ibatch: 4802, loss: \u001b[0;36m0.5362\u001b[0m, std: 0.6138\n",
      "2021-05-12 22:39:57,488 - INFO - loss: \u001b[0;32m0.5413\u001b[0m, std: 0.6202\n",
      "2021-05-12 22:39:57,648 - INFO - Epoch/batch: 1/ 453, ibatch: 4953, loss: \u001b[0;36m0.5481\u001b[0m, std: 0.6253\n",
      "2021-05-12 22:40:03,404 - INFO - Epoch/batch: 1/ 604, ibatch: 5104, loss: \u001b[0;36m0.5341\u001b[0m, std: 0.6245\n",
      "2021-05-12 22:40:09,189 - INFO - Epoch/batch: 1/ 755, ibatch: 5255, loss: \u001b[0;36m0.5582\u001b[0m, std: 0.6242\n",
      "2021-05-12 22:40:24,657 - INFO - loss: \u001b[0;32m0.5385\u001b[0m, std: 0.6100\n",
      "2021-05-12 22:40:24,673 - INFO - Saved model states in: earlystop_0.5385\n",
      "2021-05-12 22:40:24,675 - INFO - Saved net python code: earlystop_0.5385/paddle_nets.py\n",
      "2021-05-12 22:40:24,681 - INFO - Saved best model: earlystop_0.5385\n",
      "2021-05-12 22:40:24,682 - INFO - Removing earlystop model: earlystop_0.5410\n",
      "2021-05-12 22:40:24,924 - INFO - Epoch/batch: 1/ 906, ibatch: 5406, loss: \u001b[0;36m0.5537\u001b[0m, std: 0.6293\n",
      "2021-05-12 22:40:30,838 - INFO - Epoch/batch: 1/1057, ibatch: 5557, loss: \u001b[0;36m0.5319\u001b[0m, std: 0.6073\n",
      "2021-05-12 22:40:36,708 - INFO - Epoch/batch: 1/1208, ibatch: 5708, loss: \u001b[0;36m0.5609\u001b[0m, std: 0.6471\n",
      "2021-05-12 22:40:51,768 - INFO - loss: \u001b[0;32m0.5400\u001b[0m, std: 0.6310\n",
      "2021-05-12 22:40:52,126 - INFO - Epoch/batch: 1/1359, ibatch: 5859, loss: \u001b[0;36m0.5643\u001b[0m, std: 0.6368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: ReduceOnPlateau set learning rate to 0.0024300000000000003.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 22:40:57,627 - INFO - Epoch/batch: 1/1510, ibatch: 6010, loss: \u001b[0;36m0.5491\u001b[0m, std: 0.6243\n",
      "2021-05-12 22:41:03,152 - INFO - Epoch/batch: 1/1661, ibatch: 6161, loss: \u001b[0;36m0.5363\u001b[0m, std: 0.6134\n",
      "2021-05-12 22:41:17,764 - INFO - loss: \u001b[0;32m0.5413\u001b[0m, std: 0.5808\n",
      "2021-05-12 22:41:18,297 - INFO - Epoch/batch: 1/1812, ibatch: 6312, loss: \u001b[0;36m0.5456\u001b[0m, std: 0.6188\n",
      "2021-05-12 22:41:24,083 - INFO - Epoch/batch: 1/1963, ibatch: 6463, loss: \u001b[0;36m0.5422\u001b[0m, std: 0.6234\n",
      "2021-05-12 22:41:29,918 - INFO - Epoch/batch: 1/2114, ibatch: 6614, loss: \u001b[0;36m0.5420\u001b[0m, std: 0.6287\n",
      "2021-05-12 22:41:44,636 - INFO - loss: \u001b[0;32m0.5363\u001b[0m, std: 0.6298\n",
      "2021-05-12 22:41:44,651 - INFO - Saved model states in: earlystop_0.5363\n",
      "2021-05-12 22:41:44,653 - INFO - Saved net python code: earlystop_0.5363/paddle_nets.py\n",
      "2021-05-12 22:41:44,659 - INFO - Saved best model: earlystop_0.5363\n",
      "2021-05-12 22:41:44,660 - INFO - Removing earlystop model: earlystop_0.5385\n",
      "2021-05-12 22:41:45,220 - INFO - Epoch/batch: 1/2265, ibatch: 6765, loss: \u001b[0;36m0.5191\u001b[0m, std: 0.5976\n",
      "2021-05-12 22:41:50,963 - INFO - Epoch/batch: 1/2416, ibatch: 6916, loss: \u001b[0;36m0.5515\u001b[0m, std: 0.6327\n",
      "2021-05-12 22:41:56,676 - INFO - Epoch/batch: 1/2567, ibatch: 7067, loss: \u001b[0;36m0.5163\u001b[0m, std: 0.6027\n",
      "2021-05-12 22:42:11,271 - INFO - loss: \u001b[0;32m0.5416\u001b[0m, std: 0.6533\n",
      "2021-05-12 22:42:12,048 - INFO - Epoch/batch: 1/2718, ibatch: 7218, loss: \u001b[0;36m0.5472\u001b[0m, std: 0.6261\n",
      "2021-05-12 22:42:18,389 - INFO - Epoch/batch: 1/2869, ibatch: 7369, loss: \u001b[0;36m0.5502\u001b[0m, std: 0.6276\n",
      "2021-05-12 22:42:24,977 - INFO - Epoch/batch: 1/3020, ibatch: 7520, loss: \u001b[0;36m0.5461\u001b[0m, std: 0.6296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: ReduceOnPlateau set learning rate to 0.002187.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 22:42:39,199 - INFO - loss: \u001b[0;32m0.5389\u001b[0m, std: 0.6478\n",
      "2021-05-12 22:42:40,065 - INFO - Epoch/batch: 1/3171, ibatch: 7671, loss: \u001b[0;36m0.5328\u001b[0m, std: 0.6250\n",
      "2021-05-12 22:42:45,757 - INFO - Epoch/batch: 1/3322, ibatch: 7822, loss: \u001b[0;36m0.5405\u001b[0m, std: 0.6152\n",
      "2021-05-12 22:42:51,493 - INFO - Epoch/batch: 1/3473, ibatch: 7973, loss: \u001b[0;36m0.5520\u001b[0m, std: 0.6275\n",
      "2021-05-12 22:43:05,953 - INFO - loss: \u001b[0;32m0.5369\u001b[0m, std: 0.6105\n",
      "2021-05-12 22:43:06,915 - INFO - Epoch/batch: 1/3624, ibatch: 8124, loss: \u001b[0;36m0.5478\u001b[0m, std: 0.6247\n",
      "2021-05-12 22:43:12,637 - INFO - Epoch/batch: 1/3775, ibatch: 8275, loss: \u001b[0;36m0.5584\u001b[0m, std: 0.6331\n",
      "2021-05-12 22:43:18,105 - INFO - Epoch/batch: 1/3926, ibatch: 8426, loss: \u001b[0;36m0.5283\u001b[0m, std: 0.6034\n",
      "2021-05-12 22:43:32,290 - INFO - loss: \u001b[0;32m0.5359\u001b[0m, std: 0.5995\n",
      "2021-05-12 22:43:32,306 - INFO - Saved model states in: earlystop_0.5359\n",
      "2021-05-12 22:43:32,307 - INFO - Saved net python code: earlystop_0.5359/paddle_nets.py\n",
      "2021-05-12 22:43:32,314 - INFO - Saved best model: earlystop_0.5359\n",
      "2021-05-12 22:43:32,315 - INFO - Removing earlystop model: earlystop_0.5363\n",
      "2021-05-12 22:43:33,289 - INFO - Epoch/batch: 1/4077, ibatch: 8577, loss: \u001b[0;36m0.5530\u001b[0m, std: 0.6224\n",
      "2021-05-12 22:43:39,020 - INFO - Epoch/batch: 1/4228, ibatch: 8728, loss: \u001b[0;36m0.5314\u001b[0m, std: 0.6104\n",
      "2021-05-12 22:43:44,795 - INFO - Epoch/batch: 1/4379, ibatch: 8879, loss: \u001b[0;36m0.5517\u001b[0m, std: 0.6295\n",
      "2021-05-12 22:43:58,871 - INFO - loss: \u001b[0;32m0.5363\u001b[0m, std: 0.6283\n",
      "2021-05-12 22:44:00,111 - INFO - Epoch 1 average training loss: \u001b[0;46m0.5446\u001b[0m std: 0.6230\n",
      "2021-05-12 22:44:00,209 - INFO - Epoch 1 average validate loss: \u001b[0;46m0.5390\u001b[0m std: 0.6189\n",
      "2021-05-12 22:44:02,159 - INFO - Epoch/batch: 2/   0, ibatch: 9000, loss: \u001b[0;36m0.5540\u001b[0m, std: 0.6299\n",
      "2021-05-12 22:44:11,837 - INFO - loss: \u001b[0;32m0.5356\u001b[0m, std: 0.6260\n",
      "2021-05-12 22:44:11,921 - INFO - Saved model states in: earlystop_0.5356\n",
      "2021-05-12 22:44:11,923 - INFO - Saved net python code: earlystop_0.5356/paddle_nets.py\n",
      "2021-05-12 22:44:11,931 - INFO - Saved best model: earlystop_0.5356\n",
      "2021-05-12 22:44:11,932 - INFO - Removing earlystop model: earlystop_0.5359\n",
      "2021-05-12 22:44:17,510 - INFO - Epoch/batch: 2/ 151, ibatch: 9151, loss: \u001b[0;36m0.5380\u001b[0m, std: 0.6174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62: ReduceOnPlateau set learning rate to 0.0019683.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 22:44:23,336 - INFO - Epoch/batch: 2/ 302, ibatch: 9302, loss: \u001b[0;36m0.5400\u001b[0m, std: 0.6194\n",
      "2021-05-12 22:44:38,529 - INFO - loss: \u001b[0;32m0.5355\u001b[0m, std: 0.6370\n",
      "2021-05-12 22:44:38,544 - INFO - Saved model states in: earlystop_0.5355\n",
      "2021-05-12 22:44:38,546 - INFO - Saved net python code: earlystop_0.5355/paddle_nets.py\n",
      "2021-05-12 22:44:38,552 - INFO - Saved best model: earlystop_0.5355\n",
      "2021-05-12 22:44:38,553 - INFO - Removing earlystop model: earlystop_0.5356\n",
      "2021-05-12 22:44:38,654 - INFO - Epoch/batch: 2/ 453, ibatch: 9453, loss: \u001b[0;36m0.5621\u001b[0m, std: 0.6413\n",
      "2021-05-12 22:44:44,153 - INFO - Epoch/batch: 2/ 604, ibatch: 9604, loss: \u001b[0;36m0.5364\u001b[0m, std: 0.6253\n",
      "2021-05-12 22:44:49,737 - INFO - Epoch/batch: 2/ 755, ibatch: 9755, loss: \u001b[0;36m0.5378\u001b[0m, std: 0.6133\n",
      "2021-05-12 22:45:04,290 - INFO - loss: \u001b[0;32m0.5357\u001b[0m, std: 0.6287\n",
      "2021-05-12 22:45:04,545 - INFO - Epoch/batch: 2/ 906, ibatch: 9906, loss: \u001b[0;36m0.5216\u001b[0m, std: 0.6036\n",
      "2021-05-12 22:45:10,518 - INFO - Epoch/batch: 2/1057, ibatch: 10057, loss: \u001b[0;36m0.5292\u001b[0m, std: 0.6165\n",
      "2021-05-12 22:45:16,260 - INFO - Epoch/batch: 2/1208, ibatch: 10208, loss: \u001b[0;36m0.5432\u001b[0m, std: 0.6278\n",
      "2021-05-12 22:45:31,789 - INFO - loss: \u001b[0;32m0.5357\u001b[0m, std: 0.6280\n",
      "2021-05-12 22:45:32,180 - INFO - Epoch/batch: 2/1359, ibatch: 10359, loss: \u001b[0;36m0.5424\u001b[0m, std: 0.6197\n",
      "2021-05-12 22:45:37,831 - INFO - Epoch/batch: 2/1510, ibatch: 10510, loss: \u001b[0;36m0.5453\u001b[0m, std: 0.6156\n",
      "2021-05-12 22:45:43,547 - INFO - Epoch/batch: 2/1661, ibatch: 10661, loss: \u001b[0;36m0.5373\u001b[0m, std: 0.6225\n",
      "2021-05-12 22:45:58,228 - INFO - loss: \u001b[0;32m0.5358\u001b[0m, std: 0.6288\n",
      "2021-05-12 22:45:58,757 - INFO - Epoch/batch: 2/1812, ibatch: 10812, loss: \u001b[0;36m0.5263\u001b[0m, std: 0.6056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73: ReduceOnPlateau set learning rate to 0.00177147.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 22:46:04,621 - INFO - Epoch/batch: 2/1963, ibatch: 10963, loss: \u001b[0;36m0.5341\u001b[0m, std: 0.6181\n",
      "2021-05-12 22:46:09,932 - INFO - Epoch/batch: 2/2114, ibatch: 11114, loss: \u001b[0;36m0.5563\u001b[0m, std: 0.6252\n",
      "2021-05-12 22:46:24,599 - INFO - loss: \u001b[0;32m0.5334\u001b[0m, std: 0.6010\n",
      "2021-05-12 22:46:24,615 - INFO - Saved model states in: earlystop_0.5334\n",
      "2021-05-12 22:46:24,617 - INFO - Saved net python code: earlystop_0.5334/paddle_nets.py\n",
      "2021-05-12 22:46:24,624 - INFO - Saved best model: earlystop_0.5334\n",
      "2021-05-12 22:46:24,625 - INFO - Removing earlystop model: earlystop_0.5355\n",
      "2021-05-12 22:46:25,214 - INFO - Epoch/batch: 2/2265, ibatch: 11265, loss: \u001b[0;36m0.5493\u001b[0m, std: 0.6244\n",
      "2021-05-12 22:46:30,781 - INFO - Epoch/batch: 2/2416, ibatch: 11416, loss: \u001b[0;36m0.5394\u001b[0m, std: 0.6214\n",
      "2021-05-12 22:46:36,532 - INFO - Epoch/batch: 2/2567, ibatch: 11567, loss: \u001b[0;36m0.5515\u001b[0m, std: 0.6238\n",
      "2021-05-12 22:46:51,380 - INFO - loss: \u001b[0;32m0.5343\u001b[0m, std: 0.5891\n",
      "2021-05-12 22:46:52,085 - INFO - Epoch/batch: 2/2718, ibatch: 11718, loss: \u001b[0;36m0.5485\u001b[0m, std: 0.6274\n",
      "2021-05-12 22:46:57,629 - INFO - Epoch/batch: 2/2869, ibatch: 11869, loss: \u001b[0;36m0.5402\u001b[0m, std: 0.6213\n",
      "2021-05-12 22:47:03,485 - INFO - Epoch/batch: 2/3020, ibatch: 12020, loss: \u001b[0;36m0.5538\u001b[0m, std: 0.6245\n",
      "2021-05-12 22:47:18,401 - INFO - loss: \u001b[0;32m0.5396\u001b[0m, std: 0.6604\n",
      "2021-05-12 22:47:19,287 - INFO - Epoch/batch: 2/3171, ibatch: 12171, loss: \u001b[0;36m0.5377\u001b[0m, std: 0.6252\n",
      "2021-05-12 22:47:25,200 - INFO - Epoch/batch: 2/3322, ibatch: 12322, loss: \u001b[0;36m0.5311\u001b[0m, std: 0.6136\n",
      "2021-05-12 22:47:31,002 - INFO - Epoch/batch: 2/3473, ibatch: 12473, loss: \u001b[0;36m0.5474\u001b[0m, std: 0.6235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84: ReduceOnPlateau set learning rate to 0.0015943230000000001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 22:47:44,870 - INFO - loss: \u001b[0;32m0.5419\u001b[0m, std: 0.6663\n",
      "2021-05-12 22:47:45,807 - INFO - Epoch/batch: 2/3624, ibatch: 12624, loss: \u001b[0;36m0.5304\u001b[0m, std: 0.6107\n",
      "2021-05-12 22:47:51,565 - INFO - Epoch/batch: 2/3775, ibatch: 12775, loss: \u001b[0;36m0.5366\u001b[0m, std: 0.6175\n",
      "2021-05-12 22:47:57,582 - INFO - Epoch/batch: 2/3926, ibatch: 12926, loss: \u001b[0;36m0.5526\u001b[0m, std: 0.6227\n",
      "2021-05-12 22:48:11,989 - INFO - loss: \u001b[0;32m0.5343\u001b[0m, std: 0.6195\n",
      "2021-05-12 22:48:12,996 - INFO - Epoch/batch: 2/4077, ibatch: 13077, loss: \u001b[0;36m0.5207\u001b[0m, std: 0.6000\n",
      "2021-05-12 22:48:18,850 - INFO - Epoch/batch: 2/4228, ibatch: 13228, loss: \u001b[0;36m0.5465\u001b[0m, std: 0.6176\n",
      "2021-05-12 22:48:24,831 - INFO - Epoch/batch: 2/4379, ibatch: 13379, loss: \u001b[0;36m0.5284\u001b[0m, std: 0.6075\n",
      "2021-05-12 22:48:39,474 - INFO - loss: \u001b[0;32m0.5319\u001b[0m, std: 0.6085\n",
      "2021-05-12 22:48:39,513 - INFO - Saved model states in: earlystop_0.5319\n",
      "2021-05-12 22:48:39,515 - INFO - Saved net python code: earlystop_0.5319/paddle_nets.py\n",
      "2021-05-12 22:48:39,522 - INFO - Saved best model: earlystop_0.5319\n",
      "2021-05-12 22:48:39,522 - INFO - Removing earlystop model: earlystop_0.5334\n",
      "2021-05-12 22:48:40,712 - INFO - Epoch 2 average training loss: \u001b[0;46m0.5401\u001b[0m std: 0.6191\n",
      "2021-05-12 22:48:40,715 - INFO - Epoch 2 average validate loss: \u001b[0;46m0.5358\u001b[0m std: 0.6267\n",
      "2021-05-12 22:48:42,458 - INFO - Epoch/batch: 3/   0, ibatch: 13500, loss: \u001b[0;36m0.5350\u001b[0m, std: 0.6186\n",
      "2021-05-12 22:48:52,336 - INFO - loss: \u001b[0;32m0.5319\u001b[0m, std: 0.6088\n",
      "2021-05-12 22:48:52,356 - INFO - Saved model states in: earlystop_0.5319.1\n",
      "2021-05-12 22:48:52,358 - INFO - Saved net python code: earlystop_0.5319.1/paddle_nets.py\n",
      "2021-05-12 22:48:52,366 - INFO - Saved best model: earlystop_0.5319.1\n",
      "2021-05-12 22:48:52,367 - INFO - Removing earlystop model: earlystop_0.5319\n",
      "2021-05-12 22:48:58,131 - INFO - Epoch/batch: 3/ 151, ibatch: 13651, loss: \u001b[0;36m0.5123\u001b[0m, std: 0.5985\n",
      "2021-05-12 22:49:04,051 - INFO - Epoch/batch: 3/ 302, ibatch: 13802, loss: \u001b[0;36m0.5424\u001b[0m, std: 0.6199\n",
      "2021-05-12 22:49:19,034 - INFO - loss: \u001b[0;32m0.5325\u001b[0m, std: 0.6159\n",
      "2021-05-12 22:49:19,141 - INFO - Epoch/batch: 3/ 453, ibatch: 13953, loss: \u001b[0;36m0.5387\u001b[0m, std: 0.6203\n",
      "2021-05-12 22:49:24,713 - INFO - Epoch/batch: 3/ 604, ibatch: 14104, loss: \u001b[0;36m0.5219\u001b[0m, std: 0.6099\n",
      "2021-05-12 22:49:30,606 - INFO - Epoch/batch: 3/ 755, ibatch: 14255, loss: \u001b[0;36m0.5373\u001b[0m, std: 0.6163\n",
      "2021-05-12 22:49:45,847 - INFO - loss: \u001b[0;32m0.5318\u001b[0m, std: 0.6162\n",
      "2021-05-12 22:49:45,863 - INFO - Saved model states in: earlystop_0.5318\n",
      "2021-05-12 22:49:45,864 - INFO - Saved net python code: earlystop_0.5318/paddle_nets.py\n",
      "2021-05-12 22:49:45,871 - INFO - Saved best model: earlystop_0.5318\n",
      "2021-05-12 22:49:45,872 - INFO - Removing earlystop model: earlystop_0.5319.1\n",
      "2021-05-12 22:49:46,121 - INFO - Epoch/batch: 3/ 906, ibatch: 14406, loss: \u001b[0;36m0.5471\u001b[0m, std: 0.6234\n",
      "2021-05-12 22:49:51,808 - INFO - Epoch/batch: 3/1057, ibatch: 14557, loss: \u001b[0;36m0.5222\u001b[0m, std: 0.6007\n",
      "2021-05-12 22:49:58,269 - INFO - Epoch/batch: 3/1208, ibatch: 14708, loss: \u001b[0;36m0.5283\u001b[0m, std: 0.6027\n",
      "2021-05-12 22:50:13,875 - INFO - loss: \u001b[0;32m0.5339\u001b[0m, std: 0.6346\n",
      "2021-05-12 22:50:14,194 - INFO - Epoch/batch: 3/1359, ibatch: 14859, loss: \u001b[0;36m0.5402\u001b[0m, std: 0.6189\n",
      "2021-05-12 22:50:20,179 - INFO - Epoch/batch: 3/1510, ibatch: 15010, loss: \u001b[0;36m0.5514\u001b[0m, std: 0.6339\n",
      "2021-05-12 22:50:25,954 - INFO - Epoch/batch: 3/1661, ibatch: 15161, loss: \u001b[0;36m0.5426\u001b[0m, std: 0.6221\n",
      "2021-05-12 22:50:40,111 - INFO - loss: \u001b[0;32m0.5337\u001b[0m, std: 0.6362\n",
      "2021-05-12 22:50:40,594 - INFO - Epoch/batch: 3/1812, ibatch: 15312, loss: \u001b[0;36m0.5337\u001b[0m, std: 0.6223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103: ReduceOnPlateau set learning rate to 0.0014348907.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 22:50:46,094 - INFO - Epoch/batch: 3/1963, ibatch: 15463, loss: \u001b[0;36m0.5399\u001b[0m, std: 0.6161\n",
      "2021-05-12 22:50:51,958 - INFO - Epoch/batch: 3/2114, ibatch: 15614, loss: \u001b[0;36m0.5416\u001b[0m, std: 0.6233\n",
      "2021-05-12 22:51:06,586 - INFO - loss: \u001b[0;32m0.5340\u001b[0m, std: 0.5928\n",
      "2021-05-12 22:51:07,123 - INFO - Epoch/batch: 3/2265, ibatch: 15765, loss: \u001b[0;36m0.5374\u001b[0m, std: 0.6136\n",
      "2021-05-12 22:51:12,779 - INFO - Epoch/batch: 3/2416, ibatch: 15916, loss: \u001b[0;36m0.5439\u001b[0m, std: 0.6263\n",
      "2021-05-12 22:51:18,316 - INFO - Epoch/batch: 3/2567, ibatch: 16067, loss: \u001b[0;36m0.5457\u001b[0m, std: 0.6162\n",
      "2021-05-12 22:51:32,066 - INFO - loss: \u001b[0;32m0.5336\u001b[0m, std: 0.5891\n",
      "2021-05-12 22:51:32,846 - INFO - Epoch/batch: 3/2718, ibatch: 16218, loss: \u001b[0;36m0.5335\u001b[0m, std: 0.6071\n",
      "2021-05-12 22:51:38,525 - INFO - Epoch/batch: 3/2869, ibatch: 16369, loss: \u001b[0;36m0.5125\u001b[0m, std: 0.5989\n",
      "2021-05-12 22:51:44,440 - INFO - Epoch/batch: 3/3020, ibatch: 16520, loss: \u001b[0;36m0.5323\u001b[0m, std: 0.6146\n",
      "2021-05-12 22:51:59,391 - INFO - loss: \u001b[0;32m0.5312\u001b[0m, std: 0.6163\n",
      "2021-05-12 22:51:59,414 - INFO - Saved model states in: earlystop_0.5312\n",
      "2021-05-12 22:51:59,416 - INFO - Saved net python code: earlystop_0.5312/paddle_nets.py\n",
      "2021-05-12 22:51:59,424 - INFO - Saved best model: earlystop_0.5312\n",
      "2021-05-12 22:51:59,425 - INFO - Removing earlystop model: earlystop_0.5318\n",
      "2021-05-12 22:52:00,272 - INFO - Epoch/batch: 3/3171, ibatch: 16671, loss: \u001b[0;36m0.5375\u001b[0m, std: 0.6163\n",
      "2021-05-12 22:52:06,137 - INFO - Epoch/batch: 3/3322, ibatch: 16822, loss: \u001b[0;36m0.5440\u001b[0m, std: 0.6249\n",
      "2021-05-12 22:52:12,167 - INFO - Epoch/batch: 3/3473, ibatch: 16973, loss: \u001b[0;36m0.5398\u001b[0m, std: 0.6216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114: ReduceOnPlateau set learning rate to 0.00129140163.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 22:52:26,689 - INFO - loss: \u001b[0;32m0.5319\u001b[0m, std: 0.5966\n",
      "2021-05-12 22:52:27,596 - INFO - Epoch/batch: 3/3624, ibatch: 17124, loss: \u001b[0;36m0.5427\u001b[0m, std: 0.6161\n",
      "2021-05-12 22:52:33,143 - INFO - Epoch/batch: 3/3775, ibatch: 17275, loss: \u001b[0;36m0.5195\u001b[0m, std: 0.6030\n",
      "2021-05-12 22:52:39,478 - INFO - Epoch/batch: 3/3926, ibatch: 17426, loss: \u001b[0;36m0.5633\u001b[0m, std: 0.6379\n",
      "2021-05-12 22:52:54,143 - INFO - loss: \u001b[0;32m0.5321\u001b[0m, std: 0.6158\n",
      "2021-05-12 22:52:55,185 - INFO - Epoch/batch: 3/4077, ibatch: 17577, loss: \u001b[0;36m0.5361\u001b[0m, std: 0.6095\n",
      "2021-05-12 22:53:01,163 - INFO - Epoch/batch: 3/4228, ibatch: 17728, loss: \u001b[0;36m0.5544\u001b[0m, std: 0.6311\n",
      "2021-05-12 22:53:06,930 - INFO - Epoch/batch: 3/4379, ibatch: 17879, loss: \u001b[0;36m0.5429\u001b[0m, std: 0.6109\n",
      "2021-05-12 22:53:21,309 - INFO - loss: \u001b[0;32m0.5325\u001b[0m, std: 0.6173\n",
      "2021-05-12 22:53:22,511 - INFO - Epoch 3 average training loss: \u001b[0;46m0.5373\u001b[0m std: 0.6164\n",
      "2021-05-12 22:53:22,515 - INFO - Epoch 3 average validate loss: \u001b[0;46m0.5326\u001b[0m std: 0.6127\n",
      "2021-05-12 22:53:24,365 - INFO - Epoch/batch: 4/   0, ibatch: 18000, loss: \u001b[0;36m0.5356\u001b[0m, std: 0.6177\n",
      "2021-05-12 22:53:34,213 - INFO - loss: \u001b[0;32m0.5327\u001b[0m, std: 0.6197\n",
      "2021-05-12 22:53:40,041 - INFO - Epoch/batch: 4/ 151, ibatch: 18151, loss: \u001b[0;36m0.5233\u001b[0m, std: 0.6068\n",
      "2021-05-12 22:53:45,788 - INFO - Epoch/batch: 4/ 302, ibatch: 18302, loss: \u001b[0;36m0.5290\u001b[0m, std: 0.6041\n",
      "2021-05-12 22:54:01,084 - INFO - loss: \u001b[0;32m0.5320\u001b[0m, std: 0.6122\n",
      "2021-05-12 22:54:01,235 - INFO - Epoch/batch: 4/ 453, ibatch: 18453, loss: \u001b[0;36m0.5311\u001b[0m, std: 0.6056\n",
      "2021-05-12 22:54:06,858 - INFO - Epoch/batch: 4/ 604, ibatch: 18604, loss: \u001b[0;36m0.5291\u001b[0m, std: 0.6153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125: ReduceOnPlateau set learning rate to 0.001162261467.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 22:54:12,662 - INFO - Epoch/batch: 4/ 755, ibatch: 18755, loss: \u001b[0;36m0.5382\u001b[0m, std: 0.6189\n",
      "2021-05-12 22:54:27,697 - INFO - loss: \u001b[0;32m0.5306\u001b[0m, std: 0.6047\n",
      "2021-05-12 22:54:27,718 - INFO - Saved model states in: earlystop_0.5306\n",
      "2021-05-12 22:54:27,720 - INFO - Saved net python code: earlystop_0.5306/paddle_nets.py\n",
      "2021-05-12 22:54:27,728 - INFO - Saved best model: earlystop_0.5306\n",
      "2021-05-12 22:54:27,728 - INFO - Removing earlystop model: earlystop_0.5312\n",
      "2021-05-12 22:54:27,961 - INFO - Epoch/batch: 4/ 906, ibatch: 18906, loss: \u001b[0;36m0.5347\u001b[0m, std: 0.6146\n",
      "2021-05-12 22:54:33,241 - INFO - Epoch/batch: 4/1057, ibatch: 19057, loss: \u001b[0;36m0.5084\u001b[0m, std: 0.5908\n",
      "2021-05-12 22:54:38,948 - INFO - Epoch/batch: 4/1208, ibatch: 19208, loss: \u001b[0;36m0.5342\u001b[0m, std: 0.6122\n",
      "2021-05-12 22:54:54,391 - INFO - loss: \u001b[0;32m0.5311\u001b[0m, std: 0.6026\n",
      "2021-05-12 22:54:54,669 - INFO - Epoch/batch: 4/1359, ibatch: 19359, loss: \u001b[0;36m0.5432\u001b[0m, std: 0.6174\n",
      "2021-05-12 22:55:00,283 - INFO - Epoch/batch: 4/1510, ibatch: 19510, loss: \u001b[0;36m0.5415\u001b[0m, std: 0.6185\n",
      "2021-05-12 22:55:06,056 - INFO - Epoch/batch: 4/1661, ibatch: 19661, loss: \u001b[0;36m0.5402\u001b[0m, std: 0.6197\n",
      "2021-05-12 22:55:20,874 - INFO - loss: \u001b[0;32m0.5330\u001b[0m, std: 0.6314\n",
      "2021-05-12 22:55:21,419 - INFO - Epoch/batch: 4/1812, ibatch: 19812, loss: \u001b[0;36m0.5267\u001b[0m, std: 0.6129\n",
      "2021-05-12 22:55:27,372 - INFO - Epoch/batch: 4/1963, ibatch: 19963, loss: \u001b[0;36m0.5301\u001b[0m, std: 0.6220\n",
      "2021-05-12 22:55:33,042 - INFO - Epoch/batch: 4/2114, ibatch: 20114, loss: \u001b[0;36m0.5163\u001b[0m, std: 0.5978\n",
      "2021-05-12 22:55:48,184 - INFO - loss: \u001b[0;32m0.5305\u001b[0m, std: 0.6078\n",
      "2021-05-12 22:55:48,198 - INFO - Saved model states in: earlystop_0.5305\n",
      "2021-05-12 22:55:48,200 - INFO - Saved net python code: earlystop_0.5305/paddle_nets.py\n",
      "2021-05-12 22:55:48,210 - INFO - Saved best model: earlystop_0.5305\n",
      "2021-05-12 22:55:48,210 - INFO - Removing earlystop model: earlystop_0.5306\n",
      "2021-05-12 22:55:48,741 - INFO - Epoch/batch: 4/2265, ibatch: 20265, loss: \u001b[0;36m0.5475\u001b[0m, std: 0.6210\n",
      "2021-05-12 22:55:54,837 - INFO - Epoch/batch: 4/2416, ibatch: 20416, loss: \u001b[0;36m0.5461\u001b[0m, std: 0.6206\n",
      "2021-05-12 22:56:00,597 - INFO - Epoch/batch: 4/2567, ibatch: 20567, loss: \u001b[0;36m0.5248\u001b[0m, std: 0.6133\n",
      "2021-05-12 22:56:15,492 - INFO - loss: \u001b[0;32m0.5301\u001b[0m, std: 0.6044\n",
      "2021-05-12 22:56:15,507 - INFO - Saved model states in: earlystop_0.5301\n",
      "2021-05-12 22:56:15,509 - INFO - Saved net python code: earlystop_0.5301/paddle_nets.py\n",
      "2021-05-12 22:56:15,517 - INFO - Saved best model: earlystop_0.5301\n",
      "2021-05-12 22:56:15,518 - INFO - Removing earlystop model: earlystop_0.5305\n",
      "2021-05-12 22:56:16,221 - INFO - Epoch/batch: 4/2718, ibatch: 20718, loss: \u001b[0;36m0.5409\u001b[0m, std: 0.6189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139: ReduceOnPlateau set learning rate to 0.0010460353203000001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 22:56:22,073 - INFO - Epoch/batch: 4/2869, ibatch: 20869, loss: \u001b[0;36m0.5318\u001b[0m, std: 0.6148\n",
      "2021-05-12 22:56:27,707 - INFO - Epoch/batch: 4/3020, ibatch: 21020, loss: \u001b[0;36m0.5436\u001b[0m, std: 0.6173\n",
      "2021-05-12 22:56:42,170 - INFO - loss: \u001b[0;32m0.5319\u001b[0m, std: 0.6138\n",
      "2021-05-12 22:56:42,953 - INFO - Epoch/batch: 4/3171, ibatch: 21171, loss: \u001b[0;36m0.5295\u001b[0m, std: 0.6053\n",
      "2021-05-12 22:56:48,719 - INFO - Epoch/batch: 4/3322, ibatch: 21322, loss: \u001b[0;36m0.5367\u001b[0m, std: 0.6170\n",
      "2021-05-12 22:56:54,397 - INFO - Epoch/batch: 4/3473, ibatch: 21473, loss: \u001b[0;36m0.5397\u001b[0m, std: 0.6215\n",
      "2021-05-12 22:57:08,572 - INFO - loss: \u001b[0;32m0.5303\u001b[0m, std: 0.6200\n",
      "2021-05-12 22:57:09,519 - INFO - Epoch/batch: 4/3624, ibatch: 21624, loss: \u001b[0;36m0.5260\u001b[0m, std: 0.6131\n",
      "2021-05-12 22:57:15,636 - INFO - Epoch/batch: 4/3775, ibatch: 21775, loss: \u001b[0;36m0.5385\u001b[0m, std: 0.6146\n",
      "2021-05-12 22:57:21,431 - INFO - Epoch/batch: 4/3926, ibatch: 21926, loss: \u001b[0;36m0.5413\u001b[0m, std: 0.6155\n",
      "2021-05-12 22:57:35,948 - INFO - loss: \u001b[0;32m0.5313\u001b[0m, std: 0.6024\n",
      "2021-05-12 22:57:36,989 - INFO - Epoch/batch: 4/4077, ibatch: 22077, loss: \u001b[0;36m0.5352\u001b[0m, std: 0.6121\n",
      "2021-05-12 22:57:42,750 - INFO - Epoch/batch: 4/4228, ibatch: 22228, loss: \u001b[0;36m0.5428\u001b[0m, std: 0.6227\n",
      "2021-05-12 22:57:48,406 - INFO - Epoch/batch: 4/4379, ibatch: 22379, loss: \u001b[0;36m0.5570\u001b[0m, std: 0.6294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150: ReduceOnPlateau set learning rate to 0.0009414317882700001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 22:58:02,552 - INFO - loss: \u001b[0;32m0.5300\u001b[0m, std: 0.6126\n",
      "2021-05-12 22:58:02,567 - INFO - Saved model states in: earlystop_0.5300\n",
      "2021-05-12 22:58:02,569 - INFO - Saved net python code: earlystop_0.5300/paddle_nets.py\n",
      "2021-05-12 22:58:02,575 - INFO - Saved best model: earlystop_0.5300\n",
      "2021-05-12 22:58:02,576 - INFO - Removing earlystop model: earlystop_0.5301\n",
      "2021-05-12 22:58:03,810 - INFO - Epoch 4 average training loss: \u001b[0;46m0.5354\u001b[0m std: 0.6147\n",
      "2021-05-12 22:58:03,815 - INFO - Epoch 4 average validate loss: \u001b[0;46m0.5312\u001b[0m std: 0.6120\n",
      "2021-05-12 22:58:05,741 - INFO - Epoch/batch: 5/   0, ibatch: 22500, loss: \u001b[0;36m0.5605\u001b[0m, std: 0.6341\n",
      "2021-05-12 22:58:15,439 - INFO - loss: \u001b[0;32m0.5300\u001b[0m, std: 0.6140\n",
      "2021-05-12 22:58:21,294 - INFO - Epoch/batch: 5/ 151, ibatch: 22651, loss: \u001b[0;36m0.5352\u001b[0m, std: 0.6076\n",
      "2021-05-12 22:58:27,108 - INFO - Epoch/batch: 5/ 302, ibatch: 22802, loss: \u001b[0;36m0.5426\u001b[0m, std: 0.6166\n",
      "2021-05-12 22:58:42,776 - INFO - loss: \u001b[0;32m0.5354\u001b[0m, std: 0.6498\n",
      "2021-05-12 22:58:42,960 - INFO - Epoch/batch: 5/ 453, ibatch: 22953, loss: \u001b[0;36m0.5566\u001b[0m, std: 0.6286\n",
      "2021-05-12 22:58:48,780 - INFO - Epoch/batch: 5/ 604, ibatch: 23104, loss: \u001b[0;36m0.5289\u001b[0m, std: 0.6049\n",
      "2021-05-12 22:58:54,594 - INFO - Epoch/batch: 5/ 755, ibatch: 23255, loss: \u001b[0;36m0.5134\u001b[0m, std: 0.5983\n",
      "2021-05-12 22:59:09,586 - INFO - loss: \u001b[0;32m0.5308\u001b[0m, std: 0.6150\n",
      "2021-05-12 22:59:09,824 - INFO - Epoch/batch: 5/ 906, ibatch: 23406, loss: \u001b[0;36m0.5382\u001b[0m, std: 0.6064\n",
      "2021-05-12 22:59:15,754 - INFO - Epoch/batch: 5/1057, ibatch: 23557, loss: \u001b[0;36m0.5245\u001b[0m, std: 0.6075\n",
      "2021-05-12 22:59:21,671 - INFO - Epoch/batch: 5/1208, ibatch: 23708, loss: \u001b[0;36m0.5498\u001b[0m, std: 0.6240\n",
      "2021-05-12 22:59:36,985 - INFO - loss: \u001b[0;32m0.5312\u001b[0m, std: 0.6278\n",
      "2021-05-12 22:59:37,307 - INFO - Epoch/batch: 5/1359, ibatch: 23859, loss: \u001b[0;36m0.5276\u001b[0m, std: 0.6156\n",
      "2021-05-12 22:59:43,654 - INFO - Epoch/batch: 5/1510, ibatch: 24010, loss: \u001b[0;36m0.5624\u001b[0m, std: 0.6289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161: ReduceOnPlateau set learning rate to 0.0008472886094430002.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 22:59:49,821 - INFO - Epoch/batch: 5/1661, ibatch: 24161, loss: \u001b[0;36m0.5292\u001b[0m, std: 0.6133\n",
      "2021-05-12 23:00:05,474 - INFO - loss: \u001b[0;32m0.5303\u001b[0m, std: 0.6105\n",
      "2021-05-12 23:00:05,921 - INFO - Epoch/batch: 5/1812, ibatch: 24312, loss: \u001b[0;36m0.5334\u001b[0m, std: 0.6103\n",
      "2021-05-12 23:00:11,718 - INFO - Epoch/batch: 5/1963, ibatch: 24463, loss: \u001b[0;36m0.5372\u001b[0m, std: 0.6126\n",
      "2021-05-12 23:00:17,637 - INFO - Epoch/batch: 5/2114, ibatch: 24614, loss: \u001b[0;36m0.5299\u001b[0m, std: 0.6148\n",
      "2021-05-12 23:00:32,543 - INFO - loss: \u001b[0;32m0.5306\u001b[0m, std: 0.6267\n",
      "2021-05-12 23:00:33,180 - INFO - Epoch/batch: 5/2265, ibatch: 24765, loss: \u001b[0;36m0.5242\u001b[0m, std: 0.6061\n",
      "2021-05-12 23:00:38,811 - INFO - Epoch/batch: 5/2416, ibatch: 24916, loss: \u001b[0;36m0.5327\u001b[0m, std: 0.6206\n",
      "2021-05-12 23:00:44,366 - INFO - Epoch/batch: 5/2567, ibatch: 25067, loss: \u001b[0;36m0.5142\u001b[0m, std: 0.5951\n",
      "2021-05-12 23:00:58,953 - INFO - loss: \u001b[0;32m0.5323\u001b[0m, std: 0.5895\n",
      "2021-05-12 23:00:59,662 - INFO - Epoch/batch: 5/2718, ibatch: 25218, loss: \u001b[0;36m0.5361\u001b[0m, std: 0.6181\n",
      "2021-05-12 23:01:05,255 - INFO - Epoch/batch: 5/2869, ibatch: 25369, loss: \u001b[0;36m0.5264\u001b[0m, std: 0.5978\n",
      "2021-05-12 23:01:10,957 - INFO - Epoch/batch: 5/3020, ibatch: 25520, loss: \u001b[0;36m0.5242\u001b[0m, std: 0.6077\n",
      "2021-05-12 23:01:25,879 - INFO - loss: \u001b[0;32m0.5296\u001b[0m, std: 0.5927\n",
      "2021-05-12 23:01:25,895 - INFO - Saved model states in: earlystop_0.5296\n",
      "2021-05-12 23:01:25,908 - INFO - Saved net python code: earlystop_0.5296/paddle_nets.py\n",
      "2021-05-12 23:01:25,916 - INFO - Saved best model: earlystop_0.5296\n",
      "2021-05-12 23:01:25,917 - INFO - Removing earlystop model: earlystop_0.5300\n",
      "2021-05-12 23:01:26,637 - INFO - Epoch/batch: 5/3171, ibatch: 25671, loss: \u001b[0;36m0.5419\u001b[0m, std: 0.6184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172: ReduceOnPlateau set learning rate to 0.0007625597484987002.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:01:32,347 - INFO - Epoch/batch: 5/3322, ibatch: 25822, loss: \u001b[0;36m0.5322\u001b[0m, std: 0.6138\n",
      "2021-05-12 23:01:38,096 - INFO - Epoch/batch: 5/3473, ibatch: 25973, loss: \u001b[0;36m0.5276\u001b[0m, std: 0.6112\n",
      "2021-05-12 23:01:52,812 - INFO - loss: \u001b[0;32m0.5303\u001b[0m, std: 0.6189\n",
      "2021-05-12 23:01:53,801 - INFO - Epoch/batch: 5/3624, ibatch: 26124, loss: \u001b[0;36m0.5366\u001b[0m, std: 0.6168\n",
      "2021-05-12 23:02:00,525 - INFO - Epoch/batch: 5/3775, ibatch: 26275, loss: \u001b[0;36m0.5466\u001b[0m, std: 0.6290\n",
      "2021-05-12 23:02:07,082 - INFO - Epoch/batch: 5/3926, ibatch: 26426, loss: \u001b[0;36m0.5382\u001b[0m, std: 0.6194\n",
      "2021-05-12 23:02:22,456 - INFO - loss: \u001b[0;32m0.5320\u001b[0m, std: 0.6348\n",
      "2021-05-12 23:02:23,638 - INFO - Epoch/batch: 5/4077, ibatch: 26577, loss: \u001b[0;36m0.5312\u001b[0m, std: 0.6098\n",
      "2021-05-12 23:02:29,010 - INFO - Epoch/batch: 5/4228, ibatch: 26728, loss: \u001b[0;36m0.5251\u001b[0m, std: 0.6045\n",
      "2021-05-12 23:02:34,873 - INFO - Epoch/batch: 5/4379, ibatch: 26879, loss: \u001b[0;36m0.5445\u001b[0m, std: 0.6217\n",
      "2021-05-12 23:02:48,681 - INFO - loss: \u001b[0;32m0.5293\u001b[0m, std: 0.6101\n",
      "2021-05-12 23:02:48,718 - INFO - Saved model states in: earlystop_0.5293\n",
      "2021-05-12 23:02:48,720 - INFO - Saved net python code: earlystop_0.5293/paddle_nets.py\n",
      "2021-05-12 23:02:48,727 - INFO - Saved best model: earlystop_0.5293\n",
      "2021-05-12 23:02:48,728 - INFO - Removing earlystop model: earlystop_0.5296\n",
      "2021-05-12 23:02:50,109 - INFO - Epoch 5 average training loss: \u001b[0;46m0.5337\u001b[0m std: 0.6127\n",
      "2021-05-12 23:02:50,115 - INFO - Epoch 5 average validate loss: \u001b[0;46m0.5311\u001b[0m std: 0.6172\n",
      "2021-05-12 23:02:52,310 - INFO - Epoch/batch: 6/   0, ibatch: 27000, loss: \u001b[0;36m0.5150\u001b[0m, std: 0.5979\n",
      "2021-05-12 23:03:02,440 - INFO - loss: \u001b[0;32m0.5293\u001b[0m, std: 0.6103\n",
      "2021-05-12 23:03:02,512 - INFO - Saved model states in: earlystop_0.5293.1\n",
      "2021-05-12 23:03:02,514 - INFO - Saved net python code: earlystop_0.5293.1/paddle_nets.py\n",
      "2021-05-12 23:03:02,523 - INFO - Saved best model: earlystop_0.5293.1\n",
      "2021-05-12 23:03:02,524 - INFO - Removing earlystop model: earlystop_0.5293\n",
      "2021-05-12 23:03:08,279 - INFO - Epoch/batch: 6/ 151, ibatch: 27151, loss: \u001b[0;36m0.5429\u001b[0m, std: 0.6186\n",
      "2021-05-12 23:03:14,350 - INFO - Epoch/batch: 6/ 302, ibatch: 27302, loss: \u001b[0;36m0.5501\u001b[0m, std: 0.6273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183: ReduceOnPlateau set learning rate to 0.0006863037736488302.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:03:29,783 - INFO - loss: \u001b[0;32m0.5285\u001b[0m, std: 0.6097\n",
      "2021-05-12 23:03:29,813 - INFO - Saved model states in: earlystop_0.5285\n",
      "2021-05-12 23:03:29,815 - INFO - Saved net python code: earlystop_0.5285/paddle_nets.py\n",
      "2021-05-12 23:03:29,822 - INFO - Saved best model: earlystop_0.5285\n",
      "2021-05-12 23:03:29,823 - INFO - Removing earlystop model: earlystop_0.5293.1\n",
      "2021-05-12 23:03:29,981 - INFO - Epoch/batch: 6/ 453, ibatch: 27453, loss: \u001b[0;36m0.5364\u001b[0m, std: 0.6101\n",
      "2021-05-12 23:03:35,779 - INFO - Epoch/batch: 6/ 604, ibatch: 27604, loss: \u001b[0;36m0.5128\u001b[0m, std: 0.5937\n",
      "2021-05-12 23:03:41,350 - INFO - Epoch/batch: 6/ 755, ibatch: 27755, loss: \u001b[0;36m0.5219\u001b[0m, std: 0.6013\n",
      "2021-05-12 23:03:56,263 - INFO - loss: \u001b[0;32m0.5288\u001b[0m, std: 0.6114\n",
      "2021-05-12 23:03:56,500 - INFO - Epoch/batch: 6/ 906, ibatch: 27906, loss: \u001b[0;36m0.5283\u001b[0m, std: 0.6148\n",
      "2021-05-12 23:04:02,263 - INFO - Epoch/batch: 6/1057, ibatch: 28057, loss: \u001b[0;36m0.5483\u001b[0m, std: 0.6198\n",
      "2021-05-12 23:04:07,861 - INFO - Epoch/batch: 6/1208, ibatch: 28208, loss: \u001b[0;36m0.5244\u001b[0m, std: 0.6086\n",
      "2021-05-12 23:04:22,903 - INFO - loss: \u001b[0;32m0.5290\u001b[0m, std: 0.6006\n",
      "2021-05-12 23:04:23,316 - INFO - Epoch/batch: 6/1359, ibatch: 28359, loss: \u001b[0;36m0.5329\u001b[0m, std: 0.6156\n",
      "2021-05-12 23:04:28,871 - INFO - Epoch/batch: 6/1510, ibatch: 28510, loss: \u001b[0;36m0.5453\u001b[0m, std: 0.6155\n",
      "2021-05-12 23:04:34,981 - INFO - Epoch/batch: 6/1661, ibatch: 28661, loss: \u001b[0;36m0.5557\u001b[0m, std: 0.6336\n",
      "2021-05-12 23:04:50,278 - INFO - loss: \u001b[0;32m0.5291\u001b[0m, std: 0.5960\n",
      "2021-05-12 23:04:50,700 - INFO - Epoch/batch: 6/1812, ibatch: 28812, loss: \u001b[0;36m0.5413\u001b[0m, std: 0.6234\n",
      "2021-05-12 23:04:56,440 - INFO - Epoch/batch: 6/1963, ibatch: 28963, loss: \u001b[0;36m0.5489\u001b[0m, std: 0.6107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194: ReduceOnPlateau set learning rate to 0.0006176733962839472.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:05:02,035 - INFO - Epoch/batch: 6/2114, ibatch: 29114, loss: \u001b[0;36m0.5168\u001b[0m, std: 0.6056\n",
      "2021-05-12 23:05:17,485 - INFO - loss: \u001b[0;32m0.5352\u001b[0m, std: 0.6549\n",
      "2021-05-12 23:05:18,220 - INFO - Epoch/batch: 6/2265, ibatch: 29265, loss: \u001b[0;36m0.5374\u001b[0m, std: 0.6219\n",
      "2021-05-12 23:05:24,624 - INFO - Epoch/batch: 6/2416, ibatch: 29416, loss: \u001b[0;36m0.5278\u001b[0m, std: 0.6099\n",
      "2021-05-12 23:05:30,786 - INFO - Epoch/batch: 6/2567, ibatch: 29567, loss: \u001b[0;36m0.5336\u001b[0m, std: 0.6082\n",
      "2021-05-12 23:05:45,355 - INFO - loss: \u001b[0;32m0.5297\u001b[0m, std: 0.6230\n",
      "2021-05-12 23:05:46,074 - INFO - Epoch/batch: 6/2718, ibatch: 29718, loss: \u001b[0;36m0.5316\u001b[0m, std: 0.6118\n",
      "2021-05-12 23:05:51,751 - INFO - Epoch/batch: 6/2869, ibatch: 29869, loss: \u001b[0;36m0.5101\u001b[0m, std: 0.5933\n",
      "2021-05-12 23:05:57,691 - INFO - Epoch/batch: 6/3020, ibatch: 30020, loss: \u001b[0;36m0.5585\u001b[0m, std: 0.6389\n",
      "2021-05-12 23:06:11,971 - INFO - loss: \u001b[0;32m0.5288\u001b[0m, std: 0.6188\n",
      "2021-05-12 23:06:12,821 - INFO - Epoch/batch: 6/3171, ibatch: 30171, loss: \u001b[0;36m0.5221\u001b[0m, std: 0.5998\n",
      "2021-05-12 23:06:18,593 - INFO - Epoch/batch: 6/3322, ibatch: 30322, loss: \u001b[0;36m0.5256\u001b[0m, std: 0.6050\n",
      "2021-05-12 23:06:24,277 - INFO - Epoch/batch: 6/3473, ibatch: 30473, loss: \u001b[0;36m0.5293\u001b[0m, std: 0.6098\n",
      "2021-05-12 23:06:38,270 - INFO - loss: \u001b[0;32m0.5289\u001b[0m, std: 0.6159\n",
      "2021-05-12 23:06:39,200 - INFO - Epoch/batch: 6/3624, ibatch: 30624, loss: \u001b[0;36m0.5123\u001b[0m, std: 0.5915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 205: ReduceOnPlateau set learning rate to 0.0005559060566555524.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:06:44,739 - INFO - Epoch/batch: 6/3775, ibatch: 30775, loss: \u001b[0;36m0.5217\u001b[0m, std: 0.6005\n",
      "2021-05-12 23:06:50,344 - INFO - Epoch/batch: 6/3926, ibatch: 30926, loss: \u001b[0;36m0.5377\u001b[0m, std: 0.6181\n",
      "2021-05-12 23:07:04,219 - INFO - loss: \u001b[0;32m0.5282\u001b[0m, std: 0.6075\n",
      "2021-05-12 23:07:04,235 - INFO - Saved model states in: earlystop_0.5282\n",
      "2021-05-12 23:07:04,236 - INFO - Saved net python code: earlystop_0.5282/paddle_nets.py\n",
      "2021-05-12 23:07:04,242 - INFO - Saved best model: earlystop_0.5282\n",
      "2021-05-12 23:07:04,243 - INFO - Removing earlystop model: earlystop_0.5285\n",
      "2021-05-12 23:07:05,204 - INFO - Epoch/batch: 6/4077, ibatch: 31077, loss: \u001b[0;36m0.5398\u001b[0m, std: 0.6215\n",
      "2021-05-12 23:07:10,938 - INFO - Epoch/batch: 6/4228, ibatch: 31228, loss: \u001b[0;36m0.5397\u001b[0m, std: 0.6125\n",
      "2021-05-12 23:07:16,603 - INFO - Epoch/batch: 6/4379, ibatch: 31379, loss: \u001b[0;36m0.5114\u001b[0m, std: 0.6015\n",
      "2021-05-12 23:07:30,435 - INFO - loss: \u001b[0;32m0.5301\u001b[0m, std: 0.6314\n",
      "2021-05-12 23:07:31,908 - INFO - Epoch 6 average training loss: \u001b[0;46m0.5323\u001b[0m std: 0.6114\n",
      "2021-05-12 23:07:31,912 - INFO - Epoch 6 average validate loss: \u001b[0;46m0.5296\u001b[0m std: 0.6163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  496    2250  0.399183  0.539422    225.0  4529.0      0   2250\r",
       "  497    2250  0.601845  0.614149    496.0  2239.0      0   2250\r",
       "  498    2250  0.701426  0.793123    426.0   333.0      0   2250\r",
       "  499    2250  0.561648  0.693080    400.0  2960.0      0   2250\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0      2700  0.462774  0.476229    483.0    36.0      0   2700\r",
       "  1      2700  0.397521  0.462351    225.0   636.0      0   2700\r",
       "  2      2700  0.312448  0.409194    169.0  3366.0      0   2700\r",
       "  3      2700  0.465621  0.527822    290.0  4100.0      0   2700\r",
       "  4      2700  0.268479  0.406346    115.0  2197.0      0   2700\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495    2700  0.561724  0.780875    481.0  4941.0      0   2700\r",
       "  496    2700  0.384986  0.533321    225.0  4529.0      0   2700\r",
       "  497    2700  0.596378  0.633761    496.0  2239.0      0   2700\r",
       "  498    2700  0.725679  0.882960    426.0   333.0      0   2700\r",
       "  499    2700  0.553263  0.741188    400.0  2960.0      0   2700\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0      3150  0.461732  0.480027    483.0    36.0      0   3150\r",
       "  1      3150  0.389858  0.456537    225.0   636.0      0   3150\r",
       "  2      3150  0.314556  0.403534    169.0  3366.0      0   3150\r",
       "  3      3150  0.469561  0.527986    290.0  4100.0      0   3150\r",
       "  4      3150  0.253366  0.374826    115.0  2197.0      0   3150\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495    3150  0.568486  0.796687    481.0  4941.0      0   3150\r",
       "  496    3150  0.391760  0.560773    225.0  4529.0      0   3150\r",
       "  497    3150  0.594951  0.632327    496.0  2239.0      0   3150\r",
       "  498    3150  0.727831  0.872676    426.0   333.0      0   3150\r",
       "  499    3150  0.547759  0.735906    400.0  2960.0      0   3150\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0      3600  0.492526  0.483461    483.0    36.0      0   3600\r",
       "  1      3600  0.412451  0.474307    225.0   636.0      0   3600\r",
       "  2      3600  0.323017  0.409337    169.0  3366.0      0   3600\r",
       "  3      3600  0.471054  0.501257    290.0  4100.0      0   3600\r",
       "  4      3600  0.258369  0.378842    115.0  2197.0      0   3600\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495    3600  0.544248  0.712871    481.0  4941.0      0   3600\r",
       "  496    3600  0.375529  0.507033    225.0  4529.0      0   3600\r",
       "  497    3600  0.595936  0.614613    496.0  2239.0      0   3600\r",
       "  498    3600  0.723405  0.882509    426.0   333.0      0   3600\r",
       "  499    3600  0.550515  0.696077    400.0  2960.0      0   3600\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0      4050  0.518307  0.474671    483.0    36.0      0   4050\r",
       "  1      4050  0.431690  0.475680    225.0   636.0      0   4050\r",
       "  2      4050  0.322284  0.400458    169.0  3366.0      0   4050\r",
       "  3      4050  0.491639  0.490276    290.0  4100.0      0   4050\r",
       "  4      4050  0.261308  0.378043    115.0  2197.0      0   4050\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495    4050  0.537092  0.653039    481.0  4941.0      0   4050\r",
       "  496    4050  0.388072  0.493072    225.0  4529.0      0   4050\r",
       "  497    4050  0.597217  0.570423    496.0  2239.0      0   4050\r",
       "  498    4050  0.688003  0.755661    426.0   333.0      0   4050\r",
       "  499    4050  0.544937  0.630066    400.0  2960.0      0   4050\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0      4499  0.507791  0.481421    483.0    36.0      0   4499\r",
       "  1      4499  0.419484  0.469173    225.0   636.0      0   4499\r",
       "  2      4499  0.320147  0.399739    169.0  3366.0      0   4499\r",
       "  3      4499  0.481009  0.509898    290.0  4100.0      0   4499\r",
       "  4      4499  0.258225  0.375330    115.0  2197.0      0   4499\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495    4499  0.546761  0.682820    481.0  4941.0      0   4499\r",
       "  496    4499  0.392911  0.521952    225.0  4529.0      0   4499\r",
       "  497    4499  0.595977  0.594820    496.0  2239.0      0   4499\r",
       "  498    4499  0.688406  0.756653    426.0   333.0      0   4499\r",
       "  499    4499  0.549993  0.669430    400.0  2960.0      0   4499\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0      4500  0.509375  0.482122    483.0    36.0      1      0\r",
       "  1      4500  0.420707  0.469186    225.0   636.0      1      0\r",
       "  2      4500  0.321434  0.400164    169.0  3366.0      1      0\r",
       "  3      4500  0.482137  0.510727    290.0  4100.0      1      0\r",
       "  4      4500  0.259360  0.375617    115.0  2197.0      1      0\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495    4500  0.547766  0.681458    481.0  4941.0      1      0\r",
       "  496    4500  0.394962  0.522902    225.0  4529.0      1      0\r",
       "  497    4500  0.596725  0.594154    496.0  2239.0      1      0\r",
       "  498    4500  0.688000  0.753668    426.0   333.0      1      0\r",
       "  499    4500  0.551266  0.669624    400.0  2960.0      1      0\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0      4950  0.495738  0.486665    483.0    36.0      1    450\r",
       "  1      4950  0.404733  0.471368    225.0   636.0      1    450\r",
       "  2      4950  0.311330  0.406496    169.0  3366.0      1    450\r",
       "  3      4950  0.470187  0.504873    290.0  4100.0      1    450\r",
       "  4      4950  0.251594  0.377988    115.0  2197.0      1    450\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495    4950  0.543644  0.717037    481.0  4941.0      1    450\r",
       "  496    4950  0.370707  0.507723    225.0  4529.0      1    450\r",
       "  497    4950  0.596523  0.602607    496.0  2239.0      1    450\r",
       "  498    4950  0.698284  0.801874    426.0   333.0      1    450\r",
       "  499    4950  0.542733  0.673735    400.0  2960.0      1    450\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0      5400  0.495364  0.488605    483.0    36.0      1    900\r",
       "  1      5400  0.405183  0.467029    225.0   636.0      1    900\r",
       "  2      5400  0.311056  0.404944    169.0  3366.0      1    900\r",
       "  3      5400  0.469018  0.508643    290.0  4100.0      1    900\r",
       "  4      5400  0.248573  0.376649    115.0  2197.0      1    900\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495    5400  0.539899  0.697604    481.0  4941.0      1    900\r",
       "  496    5400  0.373716  0.502968    225.0  4529.0      1    900\r",
       "  497    5400  0.593666  0.598321    496.0  2239.0      1    900\r",
       "  498    5400  0.704446  0.835352    426.0   333.0      1    900\r",
       "  499    5400  0.544376  0.679341    400.0  2960.0      1    900\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0      5850  0.475544  0.482856    483.0    36.0      1   1350\r",
       "  1      5850  0.397774  0.467720    225.0   636.0      1   1350\r",
       "  2      5850  0.312025  0.400541    169.0  3366.0      1   1350\r",
       "  3      5850  0.458724  0.514364    290.0  4100.0      1   1350\r",
       "  4      5850  0.252406  0.379231    115.0  2197.0      1   1350\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495    5850  0.558913  0.774384    481.0  4941.0      1   1350\r",
       "  496    5850  0.374205  0.518318    225.0  4529.0      1   1350\r",
       "  497    5850  0.592582  0.625366    496.0  2239.0      1   1350\r",
       "  498    5850  0.712212  0.851425    426.0   333.0      1   1350\r",
       "  499    5850  0.538469  0.703528    400.0  2960.0      1   1350\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0      6300  0.532617  0.490508    483.0    36.0      1   1800\r",
       "  1      6300  0.417735  0.471945    225.0   636.0      1   1800\r",
       "  2      6300  0.313254  0.398408    169.0  3366.0      1   1800\r",
       "  3      6300  0.477440  0.494341    290.0  4100.0      1   1800\r",
       "  4      6300  0.253826  0.381544    115.0  2197.0      1   1800\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495    6300  0.536174  0.646899    481.0  4941.0      1   1800\r",
       "  496    6300  0.376689  0.492959    225.0  4529.0      1   1800\r",
       "  497    6300  0.594857  0.572594    496.0  2239.0      1   1800\r",
       "  498    6300  0.683822  0.729741    426.0   333.0      1   1800\r",
       "  499    6300  0.544419  0.623417    400.0  2960.0      1   1800\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0      6750  0.469679  0.475530    483.0    36.0      1   2250\r",
       "  1      6750  0.391389  0.459491    225.0   636.0      1   2250\r",
       "  2      6750  0.306132  0.401349    169.0  3366.0      1   2250\r",
       "  3      6750  0.458081  0.518474    290.0  4100.0      1   2250\r",
       "  4      6750  0.246210  0.374009    115.0  2197.0      1   2250\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495    6750  0.546630  0.724983    481.0  4941.0      1   2250\r",
       "  496    6750  0.367056  0.511153    225.0  4529.0      1   2250\r",
       "  497    6750  0.590968  0.632580    496.0  2239.0      1   2250\r",
       "  498    6750  0.707620  0.842373    426.0   333.0      1   2250\r",
       "  499    6750  0.542818  0.700597    400.0  2960.0      1   2250\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0      7200  0.453792  0.480568    483.0    36.0      1   2700\r",
       "  1      7200  0.388173  0.462982    225.0   636.0      1   2700\r",
       "  2      7200  0.306995  0.410282    169.0  3366.0      1   2700\r",
       "  3      7200  0.458819  0.528972    290.0  4100.0      1   2700\r",
       "  4      7200  0.257718  0.399508    115.0  2197.0      1   2700\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495    7200  0.569450  0.795356    481.0  4941.0      1   2700\r",
       "  496    7200  0.384904  0.559716    225.0  4529.0      1   2700\r",
       "  497    7200  0.595977  0.656067    496.0  2239.0      1   2700\r",
       "  498    7200  0.719271  0.870230    426.0   333.0      1   2700\r",
       "  499    7200  0.547215  0.745410    400.0  2960.0      1   2700\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0      7650  0.461239  0.483389    483.0    36.0      1   3150\r",
       "  1      7650  0.386720  0.453743    225.0   636.0      1   3150\r",
       "  2      7650  0.305567  0.411557    169.0  3366.0      1   3150\r",
       "  3      7650  0.461163  0.541624    290.0  4100.0      1   3150\r",
       "  4      7650  0.251174  0.384412    115.0  2197.0      1   3150\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495    7650  0.561970  0.763092    481.0  4941.0      1   3150\r",
       "  496    7650  0.363597  0.505590    225.0  4529.0      1   3150\r",
       "  497    7650  0.593708  0.643734    496.0  2239.0      1   3150\r",
       "  498    7650  0.715628  0.861684    426.0   333.0      1   3150\r",
       "  499    7650  0.545814  0.733722    400.0  2960.0      1   3150\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0      8100  0.488653  0.479502    483.0    36.0      1   3600\r",
       "  1      8100  0.407796  0.476711    225.0   636.0      1   3600\r",
       "  2      8100  0.313511  0.401466    169.0  3366.0      1   3600\r",
       "  3      8100  0.463396  0.511553    290.0  4100.0      1   3600\r",
       "  4      8100  0.248368  0.374236    115.0  2197.0      1   3600\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495    8100  0.550873  0.741843    481.0  4941.0      1   3600\r",
       "  496    8100  0.368068  0.487416    225.0  4529.0      1   3600\r",
       "  497    8100  0.583114  0.596435    496.0  2239.0      1   3600\r",
       "  498    8100  0.689546  0.792162    426.0   333.0      1   3600\r",
       "  499    8100  0.537077  0.665628    400.0  2960.0      1   3600\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0      8550  0.507709  0.484442    483.0    36.0      1   4050\r",
       "  1      8550  0.400746  0.459606    225.0   636.0      1   4050\r",
       "  2      8550  0.310042  0.403639    169.0  3366.0      1   4050\r",
       "  3      8550  0.474786  0.502834    290.0  4100.0      1   4050\r",
       "  4      8550  0.250349  0.381264    115.0  2197.0      1   4050\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495    8550  0.540260  0.676252    481.0  4941.0      1   4050\r",
       "  496    8550  0.376014  0.512515    225.0  4529.0      1   4050\r",
       "  497    8550  0.588521  0.578948    496.0  2239.0      1   4050\r",
       "  498    8550  0.686973  0.782249    426.0   333.0      1   4050\r",
       "  499    8550  0.544976  0.656850    400.0  2960.0      1   4050\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0      8999  0.477482  0.494378    483.0    36.0      1   4499\r",
       "  1      8999  0.394810  0.465737    225.0   636.0      1   4499\r",
       "  2      8999  0.309957  0.403974    169.0  3366.0      1   4499\r",
       "  3      8999  0.463027  0.523799    290.0  4100.0      1   4499\r",
       "  4      8999  0.254704  0.387257    115.0  2197.0      1   4499\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495    8999  0.557298  0.759331    481.0  4941.0      1   4499\r",
       "  496    8999  0.368754  0.498543    225.0  4529.0      1   4499\r",
       "  497    8999  0.591816  0.625981    496.0  2239.0      1   4499\r",
       "  498    8999  0.707695  0.842075    426.0   333.0      1   4499\r",
       "  499    8999  0.545941  0.713179    400.0  2960.0      1   4499\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0      9000  0.476916  0.491513    483.0    36.0      2      0\r",
       "  1      9000  0.394425  0.464744    225.0   636.0      2      0\r",
       "  2      9000  0.309629  0.402835    169.0  3366.0      2      0\r",
       "  3      9000  0.462089  0.520203    290.0  4100.0      2      0\r",
       "  4      9000  0.253741  0.385429    115.0  2197.0      2      0\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495    9000  0.554990  0.755575    481.0  4941.0      2      0\r",
       "  496    9000  0.368733  0.498759    225.0  4529.0      2      0\r",
       "  497    9000  0.590990  0.623062    496.0  2239.0      2      0\r",
       "  498    9000  0.706764  0.840618    426.0   333.0      2      0\r",
       "  499    9000  0.544289  0.708192    400.0  2960.0      2      0\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0      9450  0.456455  0.481499    483.0    36.0      2    450\r",
       "  1      9450  0.388807  0.459924    225.0   636.0      2    450\r",
       "  2      9450  0.309119  0.403907    169.0  3366.0      2    450\r",
       "  3      9450  0.457299  0.528289    290.0  4100.0      2    450\r",
       "  4      9450  0.249941  0.381059    115.0  2197.0      2    450\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495    9450  0.558230  0.764627    481.0  4941.0      2    450\r",
       "  496    9450  0.370569  0.518595    225.0  4529.0      2    450\r",
       "  497    9450  0.592558  0.642470    496.0  2239.0      2    450\r",
       "  498    9450  0.716973  0.866511    426.0   333.0      2    450\r",
       "  499    9450  0.543299  0.725579    400.0  2960.0      2    450\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0      9900  0.476140  0.489717    483.0    36.0      2    900\r",
       "  1      9900  0.395518  0.465744    225.0   636.0      2    900\r",
       "  2      9900  0.313208  0.404893    169.0  3366.0      2    900\r",
       "  3      9900  0.451221  0.515033    290.0  4100.0      2    900\r",
       "  4      9900  0.246928  0.376413    115.0  2197.0      2    900\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495    9900  0.552280  0.758967    481.0  4941.0      2    900\r",
       "  496    9900  0.364342  0.503395    225.0  4529.0      2    900\r",
       "  497    9900  0.587511  0.622691    496.0  2239.0      2    900\r",
       "  498    9900  0.697973  0.837477    426.0   333.0      2    900\r",
       "  499    9900  0.537595  0.683156    400.0  2960.0      2    900\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     10350  0.469800  0.471326    483.0    36.0      2   1350\r",
       "  1     10350  0.392163  0.457785    225.0   636.0      2   1350\r",
       "  2     10350  0.304593  0.401047    169.0  3366.0      2   1350\r",
       "  3     10350  0.461567  0.521871    290.0  4100.0      2   1350\r",
       "  4     10350  0.252849  0.385030    115.0  2197.0      2   1350\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   10350  0.552652  0.736759    481.0  4941.0      2   1350\r",
       "  496   10350  0.382442  0.548205    225.0  4529.0      2   1350\r",
       "  497   10350  0.592706  0.634822    496.0  2239.0      2   1350\r",
       "  498   10350  0.692307  0.806735    426.0   333.0      2   1350\r",
       "  499   10350  0.543820  0.702280    400.0  2960.0      2   1350\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     10800  0.474144  0.484358    483.0    36.0      2   1800\r",
       "  1     10800  0.393254  0.468537    225.0   636.0      2   1800\r",
       "  2     10800  0.308500  0.404598    169.0  3366.0      2   1800\r",
       "  3     10800  0.456427  0.516449    290.0  4100.0      2   1800\r",
       "  4     10800  0.247290  0.377549    115.0  2197.0      2   1800\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   10800  0.550542  0.749432    481.0  4941.0      2   1800\r",
       "  496   10800  0.366460  0.508783    225.0  4529.0      2   1800\r",
       "  497   10800  0.593744  0.629607    496.0  2239.0      2   1800\r",
       "  498   10800  0.705445  0.841522    426.0   333.0      2   1800\r",
       "  499   10800  0.537048  0.692199    400.0  2960.0      2   1800\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     11250  0.489542  0.481169    483.0    36.0      2   2250\r",
       "  1     11250  0.401367  0.463128    225.0   636.0      2   2250\r",
       "  2     11250  0.316746  0.402188    169.0  3366.0      2   2250\r",
       "  3     11250  0.456705  0.509039    290.0  4100.0      2   2250\r",
       "  4     11250  0.249157  0.375822    115.0  2197.0      2   2250\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   11250  0.547090  0.719462    481.0  4941.0      2   2250\r",
       "  496   11250  0.371511  0.495327    225.0  4529.0      2   2250\r",
       "  497   11250  0.589375  0.587680    496.0  2239.0      2   2250\r",
       "  498   11250  0.689288  0.775938    426.0   333.0      2   2250\r",
       "  499   11250  0.535922  0.658051    400.0  2960.0      2   2250\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     11700  0.503036  0.478326    483.0    36.0      2   2700\r",
       "  1     11700  0.409920  0.465336    225.0   636.0      2   2700\r",
       "  2     11700  0.318097  0.403827    169.0  3366.0      2   2700\r",
       "  3     11700  0.466310  0.508558    290.0  4100.0      2   2700\r",
       "  4     11700  0.248253  0.374365    115.0  2197.0      2   2700\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   11700  0.548887  0.684918    481.0  4941.0      2   2700\r",
       "  496   11700  0.372869  0.486324    225.0  4529.0      2   2700\r",
       "  497   11700  0.587955  0.580150    496.0  2239.0      2   2700\r",
       "  498   11700  0.678932  0.739368    426.0   333.0      2   2700\r",
       "  499   11700  0.542007  0.655503    400.0  2960.0      2   2700\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     12150  0.454192  0.486211    483.0    36.0      2   3150\r",
       "  1     12150  0.384643  0.465016    225.0   636.0      2   3150\r",
       "  2     12150  0.308636  0.415532    169.0  3366.0      2   3150\r",
       "  3     12150  0.454291  0.544261    290.0  4100.0      2   3150\r",
       "  4     12150  0.255816  0.393787    115.0  2197.0      2   3150\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   12150  0.566435  0.806855    481.0  4941.0      2   3150\r",
       "  496   12150  0.363466  0.509342    225.0  4529.0      2   3150\r",
       "  497   12150  0.591811  0.642983    496.0  2239.0      2   3150\r",
       "  498   12150  0.721056  0.891513    426.0   333.0      2   3150\r",
       "  499   12150  0.545292  0.738920    400.0  2960.0      2   3150\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     12600  0.444311  0.480751    483.0    36.0      2   3600\r",
       "  1     12600  0.376709  0.455187    225.0   636.0      2   3600\r",
       "  2     12600  0.304472  0.406234    169.0  3366.0      2   3600\r",
       "  3     12600  0.461938  0.557593    290.0  4100.0      2   3600\r",
       "  4     12600  0.252621  0.392490    115.0  2197.0      2   3600\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   12600  0.572304  0.805547    481.0  4941.0      2   3600\r",
       "  496   12600  0.389187  0.582390    225.0  4529.0      2   3600\r",
       "  497   12600  0.590990  0.654786    496.0  2239.0      2   3600\r",
       "  498   12600  0.739098  0.917346    426.0   333.0      2   3600\r",
       "  499   12600  0.554112  0.748900    400.0  2960.0      2   3600\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     13050  0.484384  0.486652    483.0    36.0      2   4050\r",
       "  1     13050  0.399223  0.467211    225.0   636.0      2   4050\r",
       "  2     13050  0.315011  0.406409    169.0  3366.0      2   4050\r",
       "  3     13050  0.457570  0.516058    290.0  4100.0      2   4050\r",
       "  4     13050  0.251323  0.385805    115.0  2197.0      2   4050\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   13050  0.543054  0.726051    481.0  4941.0      2   4050\r",
       "  496   13050  0.359557  0.480316    225.0  4529.0      2   4050\r",
       "  497   13050  0.585190  0.608745    496.0  2239.0      2   4050\r",
       "  498   13050  0.692650  0.813075    426.0   333.0      2   4050\r",
       "  499   13050  0.536920  0.672676    400.0  2960.0      2   4050\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     13499  0.487485  0.487454    483.0    36.0      2   4499\r",
       "  1     13499  0.399347  0.467862    225.0   636.0      2   4499\r",
       "  2     13499  0.315469  0.405269    169.0  3366.0      2   4499\r",
       "  3     13499  0.456466  0.521649    290.0  4100.0      2   4499\r",
       "  4     13499  0.246890  0.375909    115.0  2197.0      2   4499\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   13499  0.546800  0.718663    481.0  4941.0      2   4499\r",
       "  496   13499  0.362711  0.484207    225.0  4529.0      2   4499\r",
       "  497   13499  0.588713  0.609314    496.0  2239.0      2   4499\r",
       "  498   13499  0.683132  0.777402    426.0   333.0      2   4499\r",
       "  499   13499  0.539577  0.682401    400.0  2960.0      2   4499\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     13500  0.486667  0.486733    483.0    36.0      3      0\r",
       "  1     13500  0.399072  0.467563    225.0   636.0      3      0\r",
       "  2     13500  0.315380  0.405006    169.0  3366.0      3      0\r",
       "  3     13500  0.456272  0.521262    290.0  4100.0      3      0\r",
       "  4     13500  0.246793  0.375812    115.0  2197.0      3      0\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   13500  0.546508  0.718812    481.0  4941.0      3      0\r",
       "  496   13500  0.362665  0.484663    225.0  4529.0      3      0\r",
       "  497   13500  0.588672  0.609933    496.0  2239.0      3      0\r",
       "  498   13500  0.683334  0.778514    426.0   333.0      3      0\r",
       "  499   13500  0.539222  0.682201    400.0  2960.0      3      0\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     13950  0.476492  0.480330    483.0    36.0      3    450\r",
       "  1     13950  0.393122  0.459378    225.0   636.0      3    450\r",
       "  2     13950  0.313579  0.405440    169.0  3366.0      3    450\r",
       "  3     13950  0.465123  0.521862    290.0  4100.0      3    450\r",
       "  4     13950  0.251743  0.385832    115.0  2197.0      3    450\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   13950  0.545628  0.723420    481.0  4941.0      3    450\r",
       "  496   13950  0.367525  0.498725    225.0  4529.0      3    450\r",
       "  497   13950  0.587686  0.618035    496.0  2239.0      3    450\r",
       "  498   13950  0.692318  0.806305    426.0   333.0      3    450\r",
       "  499   13950  0.541046  0.689617    400.0  2960.0      3    450\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     14400  0.470427  0.473532    483.0    36.0      3    900\r",
       "  1     14400  0.384857  0.454592    225.0   636.0      3    900\r",
       "  2     14400  0.317460  0.407184    169.0  3366.0      3    900\r",
       "  3     14400  0.463433  0.528125    290.0  4100.0      3    900\r",
       "  4     14400  0.247516  0.378503    115.0  2197.0      3    900\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   14400  0.548537  0.727741    481.0  4941.0      3    900\r",
       "  496   14400  0.367197  0.505316    225.0  4529.0      3    900\r",
       "  497   14400  0.586063  0.610417    496.0  2239.0      3    900\r",
       "  498   14400  0.699527  0.806170    426.0   333.0      3    900\r",
       "  499   14400  0.536647  0.680341    400.0  2960.0      3    900\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     14850  0.471897  0.490822    483.0    36.0      3   1350\r",
       "  1     14850  0.394278  0.469824    225.0   636.0      3   1350\r",
       "  2     14850  0.311616  0.408944    169.0  3366.0      3   1350\r",
       "  3     14850  0.456610  0.537318    290.0  4100.0      3   1350\r",
       "  4     14850  0.251995  0.388834    115.0  2197.0      3   1350\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   14850  0.554107  0.759409    481.0  4941.0      3   1350\r",
       "  496   14850  0.366016  0.508491    225.0  4529.0      3   1350\r",
       "  497   14850  0.585808  0.625601    496.0  2239.0      3   1350\r",
       "  498   14850  0.704229  0.843673    426.0   333.0      3   1350\r",
       "  499   14850  0.548052  0.717941    400.0  2960.0      3   1350\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     15300  0.468638  0.486476    483.0    36.0      3   1800\r",
       "  1     15300  0.390297  0.464898    225.0   636.0      3   1800\r",
       "  2     15300  0.304461  0.407294    169.0  3366.0      3   1800\r",
       "  3     15300  0.464686  0.528595    290.0  4100.0      3   1800\r",
       "  4     15300  0.258369  0.404838    115.0  2197.0      3   1800\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   15300  0.550169  0.747024    481.0  4941.0      3   1800\r",
       "  496   15300  0.369170  0.516276    225.0  4529.0      3   1800\r",
       "  497   15300  0.587436  0.632213    496.0  2239.0      3   1800\r",
       "  498   15300  0.703328  0.841451    426.0   333.0      3   1800\r",
       "  499   15300  0.544276  0.711814    400.0  2960.0      3   1800\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     15750  0.505202  0.470899    483.0    36.0      3   2250\r",
       "  1     15750  0.408073  0.465140    225.0   636.0      3   2250\r",
       "  2     15750  0.312020  0.400649    169.0  3366.0      3   2250\r",
       "  3     15750  0.473898  0.522562    290.0  4100.0      3   2250\r",
       "  4     15750  0.251844  0.385720    115.0  2197.0      3   2250\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   15750  0.541590  0.689120    481.0  4941.0      3   2250\r",
       "  496   15750  0.385163  0.515392    225.0  4529.0      3   2250\r",
       "  497   15750  0.583132  0.584134    496.0  2239.0      3   2250\r",
       "  498   15750  0.676476  0.738248    426.0   333.0      3   2250\r",
       "  499   15750  0.542157  0.652285    400.0  2960.0      3   2250\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     16200  0.509050  0.485100    483.0    36.0      3   2700\r",
       "  1     16200  0.406373  0.465532    225.0   636.0      3   2700\r",
       "  2     16200  0.317056  0.402074    169.0  3366.0      3   2700\r",
       "  3     16200  0.476214  0.505063    290.0  4100.0      3   2700\r",
       "  4     16200  0.247507  0.378653    115.0  2197.0      3   2700\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   16200  0.532937  0.656362    481.0  4941.0      3   2700\r",
       "  496   16200  0.375111  0.502907    225.0  4529.0      3   2700\r",
       "  497   16200  0.588373  0.594908    496.0  2239.0      3   2700\r",
       "  498   16200  0.673490  0.737736    426.0   333.0      3   2700\r",
       "  499   16200  0.541542  0.645065    400.0  2960.0      3   2700\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     16650  0.481235  0.479923    483.0    36.0      3   3150\r",
       "  1     16650  0.391287  0.462905    225.0   636.0      3   3150\r",
       "  2     16650  0.313883  0.403516    169.0  3366.0      3   3150\r",
       "  3     16650  0.458192  0.530716    290.0  4100.0      3   3150\r",
       "  4     16650  0.247235  0.379403    115.0  2197.0      3   3150\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   16650  0.544971  0.721356    481.0  4941.0      3   3150\r",
       "  496   16650  0.367342  0.507605    225.0  4529.0      3   3150\r",
       "  497   16650  0.584034  0.608926    496.0  2239.0      3   3150\r",
       "  498   16650  0.696161  0.810722    426.0   333.0      3   3150\r",
       "  499   16650  0.536260  0.675060    400.0  2960.0      3   3150\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     17100  0.500879  0.487201    483.0    36.0      3   3600\r",
       "  1     17100  0.402293  0.470433    225.0   636.0      3   3600\r",
       "  2     17100  0.315345  0.402097    169.0  3366.0      3   3600\r",
       "  3     17100  0.464542  0.513227    290.0  4100.0      3   3600\r",
       "  4     17100  0.247460  0.376301    115.0  2197.0      3   3600\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   17100  0.538150  0.684722    481.0  4941.0      3   3600\r",
       "  496   17100  0.366322  0.486864    225.0  4529.0      3   3600\r",
       "  497   17100  0.588043  0.593136    496.0  2239.0      3   3600\r",
       "  498   17100  0.681566  0.762050    426.0   333.0      3   3600\r",
       "  499   17100  0.529487  0.638013    400.0  2960.0      3   3600\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     17550  0.474711  0.476937    483.0    36.0      3   4050\r",
       "  1     17550  0.391826  0.463025    225.0   636.0      3   4050\r",
       "  2     17550  0.308204  0.398731    169.0  3366.0      3   4050\r",
       "  3     17550  0.457959  0.520933    290.0  4100.0      3   4050\r",
       "  4     17550  0.248210  0.379652    115.0  2197.0      3   4050\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   17550  0.543757  0.726528    481.0  4941.0      3   4050\r",
       "  496   17550  0.364633  0.497125    225.0  4529.0      3   4050\r",
       "  497   17550  0.586022  0.609196    496.0  2239.0      3   4050\r",
       "  498   17550  0.690525  0.798240    426.0   333.0      3   4050\r",
       "  499   17550  0.539377  0.683434    400.0  2960.0      3   4050\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     17999  0.493030  0.486599    483.0    36.0      3   4499\r",
       "  1     17999  0.396227  0.469820    225.0   636.0      3   4499\r",
       "  2     17999  0.304543  0.400029    169.0  3366.0      3   4499\r",
       "  3     17999  0.460082  0.519854    290.0  4100.0      3   4499\r",
       "  4     17999  0.252844  0.394119    115.0  2197.0      3   4499\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   17999  0.546697  0.724289    481.0  4941.0      3   4499\r",
       "  496   17999  0.364447  0.499860    225.0  4529.0      3   4499\r",
       "  497   17999  0.583215  0.604195    496.0  2239.0      3   4499\r",
       "  498   17999  0.682499  0.768813    426.0   333.0      3   4499\r",
       "  499   17999  0.535538  0.665154    400.0  2960.0      3   4499\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     18000  0.490569  0.486125    483.0    36.0      4      0\r",
       "  1     18000  0.395095  0.469442    225.0   636.0      4      0\r",
       "  2     18000  0.304437  0.400518    169.0  3366.0      4      0\r",
       "  3     18000  0.459464  0.520529    290.0  4100.0      4      0\r",
       "  4     18000  0.253204  0.395069    115.0  2197.0      4      0\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   18000  0.547309  0.727959    481.0  4941.0      4      0\r",
       "  496   18000  0.364130  0.501047    225.0  4529.0      4      0\r",
       "  497   18000  0.583250  0.606358    496.0  2239.0      4      0\r",
       "  498   18000  0.684026  0.775219    426.0   333.0      4      0\r",
       "  499   18000  0.535523  0.667791    400.0  2960.0      4      0\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     18450  0.483635  0.483260    483.0    36.0      4    450\r",
       "  1     18450  0.398501  0.465393    225.0   636.0      4    450\r",
       "  2     18450  0.309305  0.400021    169.0  3366.0      4    450\r",
       "  3     18450  0.458976  0.517115    290.0  4100.0      4    450\r",
       "  4     18450  0.249316  0.381945    115.0  2197.0      4    450\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   18450  0.543009  0.707279    481.0  4941.0      4    450\r",
       "  496   18450  0.370220  0.496431    225.0  4529.0      4    450\r",
       "  497   18450  0.586150  0.613667    496.0  2239.0      4    450\r",
       "  498   18450  0.685210  0.776176    426.0   333.0      4    450\r",
       "  499   18450  0.544957  0.679581    400.0  2960.0      4    450\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     18900  0.490217  0.477076    483.0    36.0      4    900\r",
       "  1     18900  0.396774  0.464660    225.0   636.0      4    900\r",
       "  2     18900  0.314607  0.402785    169.0  3366.0      4    900\r",
       "  3     18900  0.458041  0.522636    290.0  4100.0      4    900\r",
       "  4     18900  0.246100  0.377150    115.0  2197.0      4    900\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   18900  0.539229  0.697665    481.0  4941.0      4    900\r",
       "  496   18900  0.364291  0.489526    225.0  4529.0      4    900\r",
       "  497   18900  0.581240  0.590123    496.0  2239.0      4    900\r",
       "  498   18900  0.682449  0.758606    426.0   333.0      4    900\r",
       "  499   18900  0.531721  0.652779    400.0  2960.0      4    900\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     19350  0.495820  0.481763    483.0    36.0      4   1350\r",
       "  1     19350  0.398306  0.466976    225.0   636.0      4   1350\r",
       "  2     19350  0.313934  0.399605    169.0  3366.0      4   1350\r",
       "  3     19350  0.461451  0.523271    290.0  4100.0      4   1350\r",
       "  4     19350  0.248273  0.377616    115.0  2197.0      4   1350\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   19350  0.539161  0.690264    481.0  4941.0      4   1350\r",
       "  496   19350  0.365780  0.494157    225.0  4529.0      4   1350\r",
       "  497   19350  0.581368  0.597790    496.0  2239.0      4   1350\r",
       "  498   19350  0.678198  0.754768    426.0   333.0      4   1350\r",
       "  499   19350  0.542680  0.662831    400.0  2960.0      4   1350\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     19800  0.464606  0.482649    483.0    36.0      4   1800\r",
       "  1     19800  0.387447  0.458187    225.0   636.0      4   1800\r",
       "  2     19800  0.306831  0.399493    169.0  3366.0      4   1800\r",
       "  3     19800  0.472718  0.540961    290.0  4100.0      4   1800\r",
       "  4     19800  0.248785  0.378554    115.0  2197.0      4   1800\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   19800  0.554934  0.763076    481.0  4941.0      4   1800\r",
       "  496   19800  0.356260  0.475760    225.0  4529.0      4   1800\r",
       "  497   19800  0.584155  0.637857    496.0  2239.0      4   1800\r",
       "  498   19800  0.705317  0.842557    426.0   333.0      4   1800\r",
       "  499   19800  0.536809  0.704254    400.0  2960.0      4   1800\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     20250  0.493313  0.488251    483.0    36.0      4   2250\r",
       "  1     20250  0.392073  0.462775    225.0   636.0      4   2250\r",
       "  2     20250  0.305310  0.400741    169.0  3366.0      4   2250\r",
       "  3     20250  0.465656  0.515473    290.0  4100.0      4   2250\r",
       "  4     20250  0.248284  0.381251    115.0  2197.0      4   2250\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   20250  0.540246  0.706577    481.0  4941.0      4   2250\r",
       "  496   20250  0.365526  0.493369    225.0  4529.0      4   2250\r",
       "  497   20250  0.585295  0.599455    496.0  2239.0      4   2250\r",
       "  498   20250  0.685733  0.779599    426.0   333.0      4   2250\r",
       "  499   20250  0.534118  0.661719    400.0  2960.0      4   2250\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     20700  0.487806  0.479869    483.0    36.0      4   2700\r",
       "  1     20700  0.395635  0.461874    225.0   636.0      4   2700\r",
       "  2     20700  0.304835  0.395462    169.0  3366.0      4   2700\r",
       "  3     20700  0.459253  0.522096    290.0  4100.0      4   2700\r",
       "  4     20700  0.246742  0.379269    115.0  2197.0      4   2700\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   20700  0.540356  0.706972    481.0  4941.0      4   2700\r",
       "  496   20700  0.366300  0.494133    225.0  4529.0      4   2700\r",
       "  497   20700  0.584906  0.610572    496.0  2239.0      4   2700\r",
       "  498   20700  0.685547  0.776674    426.0   333.0      4   2700\r",
       "  499   20700  0.538145  0.672466    400.0  2960.0      4   2700\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     21150  0.493218  0.492178    483.0    36.0      4   3150\r",
       "  1     21150  0.396219  0.474393    225.0   636.0      4   3150\r",
       "  2     21150  0.307073  0.399839    169.0  3366.0      4   3150\r",
       "  3     21150  0.466922  0.515589    290.0  4100.0      4   3150\r",
       "  4     21150  0.249721  0.385227    115.0  2197.0      4   3150\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   21150  0.542057  0.727298    481.0  4941.0      4   3150\r",
       "  496   21150  0.357433  0.484668    225.0  4529.0      4   3150\r",
       "  497   21150  0.581736  0.605694    496.0  2239.0      4   3150\r",
       "  498   21150  0.695327  0.814901    426.0   333.0      4   3150\r",
       "  499   21150  0.530555  0.657285    400.0  2960.0      4   3150\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     21600  0.477941  0.484099    483.0    36.0      4   3600\r",
       "  1     21600  0.390418  0.464865    225.0   636.0      4   3600\r",
       "  2     21600  0.302103  0.397075    169.0  3366.0      4   3600\r",
       "  3     21600  0.457808  0.524305    290.0  4100.0      4   3600\r",
       "  4     21600  0.249859  0.387046    115.0  2197.0      4   3600\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   21600  0.545035  0.725808    481.0  4941.0      4   3600\r",
       "  496   21600  0.368555  0.512554    225.0  4529.0      4   3600\r",
       "  497   21600  0.587408  0.625994    496.0  2239.0      4   3600\r",
       "  498   21600  0.697443  0.814076    426.0   333.0      4   3600\r",
       "  499   21600  0.538434  0.691356    400.0  2960.0      4   3600\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     22050  0.497032  0.475669    483.0    36.0      4   4050\r",
       "  1     22050  0.399133  0.467883    225.0   636.0      4   4050\r",
       "  2     22050  0.311442  0.398202    169.0  3366.0      4   4050\r",
       "  3     22050  0.463093  0.513968    290.0  4100.0      4   4050\r",
       "  4     22050  0.245595  0.378201    115.0  2197.0      4   4050\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   22050  0.536128  0.692216    481.0  4941.0      4   4050\r",
       "  496   22050  0.369254  0.496851    225.0  4529.0      4   4050\r",
       "  497   22050  0.583426  0.593090    496.0  2239.0      4   4050\r",
       "  498   22050  0.679132  0.757340    426.0   333.0      4   4050\r",
       "  499   22050  0.540673  0.657800    400.0  2960.0      4   4050\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     22499  0.480630  0.477944    483.0    36.0      4   4499\r",
       "  1     22499  0.388452  0.459516    225.0   636.0      4   4499\r",
       "  2     22499  0.305082  0.396901    169.0  3366.0      4   4499\r",
       "  3     22499  0.458317  0.524471    290.0  4100.0      4   4499\r",
       "  4     22499  0.247882  0.381365    115.0  2197.0      4   4499\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   22499  0.541491  0.699746    481.0  4941.0      4   4499\r",
       "  496   22499  0.370486  0.518500    225.0  4529.0      4   4499\r",
       "  497   22499  0.586808  0.608210    496.0  2239.0      4   4499\r",
       "  498   22499  0.680364  0.771500    426.0   333.0      4   4499\r",
       "  499   22499  0.545650  0.691759    400.0  2960.0      4   4499\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     22500  0.479810  0.478362    483.0    36.0      5      0\r",
       "  1     22500  0.388067  0.459742    225.0   636.0      5      0\r",
       "  2     22500  0.304780  0.397185    169.0  3366.0      5      0\r",
       "  3     22500  0.457977  0.525453    290.0  4100.0      5      0\r",
       "  4     22500  0.247946  0.381661    115.0  2197.0      5      0\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   22500  0.541782  0.701784    481.0  4941.0      5      0\r",
       "  496   22500  0.370105  0.518740    225.0  4529.0      5      0\r",
       "  497   22500  0.586652  0.609075    496.0  2239.0      5      0\r",
       "  498   22500  0.680768  0.774056    426.0   333.0      5      0\r",
       "  499   22500  0.545744  0.693469    400.0  2960.0      5      0\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     22950  0.456641  0.482535    483.0    36.0      5    450\r",
       "  1     22950  0.380742  0.457790    225.0   636.0      5    450\r",
       "  2     22950  0.302410  0.407307    169.0  3366.0      5    450\r",
       "  3     22950  0.459429  0.549791    290.0  4100.0      5    450\r",
       "  4     22950  0.249480  0.387323    115.0  2197.0      5    450\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   22950  0.561032  0.779594    481.0  4941.0      5    450\r",
       "  496   22950  0.369954  0.534320    225.0  4529.0      5    450\r",
       "  497   22950  0.587997  0.651402    496.0  2239.0      5    450\r",
       "  498   22950  0.709002  0.854601    426.0   333.0      5    450\r",
       "  499   22950  0.547726  0.732108    400.0  2960.0      5    450\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     23400  0.486107  0.486726    483.0    36.0      5    900\r",
       "  1     23400  0.393278  0.469548    225.0   636.0      5    900\r",
       "  2     23400  0.308547  0.399986    169.0  3366.0      5    900\r",
       "  3     23400  0.459991  0.517714    290.0  4100.0      5    900\r",
       "  4     23400  0.247213  0.379411    115.0  2197.0      5    900\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   23400  0.548343  0.739885    481.0  4941.0      5    900\r",
       "  496   23400  0.361114  0.489369    225.0  4529.0      5    900\r",
       "  497   23400  0.583570  0.613104    496.0  2239.0      5    900\r",
       "  498   23400  0.692020  0.807057    426.0   333.0      5    900\r",
       "  499   23400  0.529817  0.667003    400.0  2960.0      5    900\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     23850  0.483884  0.493506    483.0    36.0      5   1350\r",
       "  1     23850  0.393001  0.477485    225.0   636.0      5   1350\r",
       "  2     23850  0.304010  0.401083    169.0  3366.0      5   1350\r",
       "  3     23850  0.460418  0.526783    290.0  4100.0      5   1350\r",
       "  4     23850  0.248564  0.383741    115.0  2197.0      5   1350\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   23850  0.545297  0.738005    481.0  4941.0      5   1350\r",
       "  496   23850  0.358519  0.494086    225.0  4529.0      5   1350\r",
       "  497   23850  0.583462  0.627577    496.0  2239.0      5   1350\r",
       "  498   23850  0.689914  0.815639    426.0   333.0      5   1350\r",
       "  499   23850  0.534833  0.687142    400.0  2960.0      5   1350\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     24300  0.475703  0.472215    483.0    36.0      5   1800\r",
       "  1     24300  0.387251  0.456393    225.0   636.0      5   1800\r",
       "  2     24300  0.306259  0.398022    169.0  3366.0      5   1800\r",
       "  3     24300  0.463459  0.526445    290.0  4100.0      5   1800\r",
       "  4     24300  0.247170  0.381569    115.0  2197.0      5   1800\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   24300  0.545950  0.718039    481.0  4941.0      5   1800\r",
       "  496   24300  0.374656  0.525536    225.0  4529.0      5   1800\r",
       "  497   24300  0.584879  0.613780    496.0  2239.0      5   1800\r",
       "  498   24300  0.684981  0.781699    426.0   333.0      5   1800\r",
       "  499   24300  0.538217  0.679994    400.0  2960.0      5   1800\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     24750  0.477012  0.490003    483.0    36.0      5   2250\r",
       "  1     24750  0.389324  0.470274    225.0   636.0      5   2250\r",
       "  2     24750  0.305553  0.401039    169.0  3366.0      5   2250\r",
       "  3     24750  0.455483  0.530704    290.0  4100.0      5   2250\r",
       "  4     24750  0.247113  0.381239    115.0  2197.0      5   2250\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   24750  0.546179  0.740742    481.0  4941.0      5   2250\r",
       "  496   24750  0.361522  0.501995    225.0  4529.0      5   2250\r",
       "  497   24750  0.583075  0.620044    496.0  2239.0      5   2250\r",
       "  498   24750  0.696121  0.824428    426.0   333.0      5   2250\r",
       "  499   24750  0.532339  0.687742    400.0  2960.0      5   2250\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     25200  0.499855  0.480394    483.0    36.0      5   2700\r",
       "  1     25200  0.407767  0.470672    225.0   636.0      5   2700\r",
       "  2     25200  0.329182  0.405278    169.0  3366.0      5   2700\r",
       "  3     25200  0.469294  0.512705    290.0  4100.0      5   2700\r",
       "  4     25200  0.248051  0.375948    115.0  2197.0      5   2700\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   25200  0.541357  0.696486    481.0  4941.0      5   2700\r",
       "  496   25200  0.372421  0.483871    225.0  4529.0      5   2700\r",
       "  497   25200  0.582218  0.589506    496.0  2239.0      5   2700\r",
       "  498   25200  0.675988  0.745233    426.0   333.0      5   2700\r",
       "  499   25200  0.531496  0.640689    400.0  2960.0      5   2700\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     25650  0.504388  0.487304    483.0    36.0      5   3150\r",
       "  1     25650  0.398466  0.467653    225.0   636.0      5   3150\r",
       "  2     25650  0.309175  0.395820    169.0  3366.0      5   3150\r",
       "  3     25650  0.469099  0.515970    290.0  4100.0      5   3150\r",
       "  4     25650  0.245584  0.378366    115.0  2197.0      5   3150\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   25650  0.538706  0.689139    481.0  4941.0      5   3150\r",
       "  496   25650  0.365383  0.487762    225.0  4529.0      5   3150\r",
       "  497   25650  0.583320  0.591522    496.0  2239.0      5   3150\r",
       "  498   25650  0.675524  0.741310    426.0   333.0      5   3150\r",
       "  499   25650  0.534142  0.650495    400.0  2960.0      5   3150\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     26100  0.475876  0.481344    483.0    36.0      5   3600\r",
       "  1     26100  0.391991  0.469729    225.0   636.0      5   3600\r",
       "  2     26100  0.308634  0.396725    169.0  3366.0      5   3600\r",
       "  3     26100  0.456625  0.521604    290.0  4100.0      5   3600\r",
       "  4     26100  0.246824  0.381832    115.0  2197.0      5   3600\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   26100  0.543531  0.732187    481.0  4941.0      5   3600\r",
       "  496   26100  0.361485  0.497256    225.0  4529.0      5   3600\r",
       "  497   26100  0.581991  0.618738    496.0  2239.0      5   3600\r",
       "  498   26100  0.691681  0.809638    426.0   333.0      5   3600\r",
       "  499   26100  0.529639  0.675219    400.0  2960.0      5   3600\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     26550  0.470026  0.481926    483.0    36.0      5   4050\r",
       "  1     26550  0.381075  0.460360    225.0   636.0      5   4050\r",
       "  2     26550  0.304105  0.401111    169.0  3366.0      5   4050\r",
       "  3     26550  0.450776  0.528334    290.0  4100.0      5   4050\r",
       "  4     26550  0.250967  0.388346    115.0  2197.0      5   4050\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   26550  0.547825  0.750440    481.0  4941.0      5   4050\r",
       "  496   26550  0.362895  0.512035    225.0  4529.0      5   4050\r",
       "  497   26550  0.580692  0.617310    496.0  2239.0      5   4050\r",
       "  498   26550  0.703605  0.834201    426.0   333.0      5   4050\r",
       "  499   26550  0.533631  0.685872    400.0  2960.0      5   4050\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     26999  0.484952  0.480425    483.0    36.0      5   4499\r",
       "  1     26999  0.393251  0.468809    225.0   636.0      5   4499\r",
       "  2     26999  0.308093  0.398664    169.0  3366.0      5   4499\r",
       "  3     26999  0.458493  0.525313    290.0  4100.0      5   4499\r",
       "  4     26999  0.244912  0.376860    115.0  2197.0      5   4499\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   26999  0.543964  0.725053    481.0  4941.0      5   4499\r",
       "  496   26999  0.362635  0.489998    225.0  4529.0      5   4499\r",
       "  497   26999  0.582033  0.608325    496.0  2239.0      5   4499\r",
       "  498   26999  0.688949  0.789157    426.0   333.0      5   4499\r",
       "  499   26999  0.528904  0.664959    400.0  2960.0      5   4499\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     27000  0.484847  0.480634    483.0    36.0      6      0\r",
       "  1     27000  0.392999  0.468698    225.0   636.0      6      0\r",
       "  2     27000  0.308051  0.398872    169.0  3366.0      6      0\r",
       "  3     27000  0.458496  0.525854    290.0  4100.0      6      0\r",
       "  4     27000  0.245028  0.377049    115.0  2197.0      6      0\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   27000  0.544431  0.726142    481.0  4941.0      6      0\r",
       "  496   27000  0.362880  0.490789    225.0  4529.0      6      0\r",
       "  497   27000  0.582006  0.608154    496.0  2239.0      6      0\r",
       "  498   27000  0.689279  0.789704    426.0   333.0      6      0\r",
       "  499   27000  0.528905  0.665274    400.0  2960.0      6      0\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     27450  0.478264  0.479012    483.0    36.0      6    450\r",
       "  1     27450  0.389899  0.461848    225.0   636.0      6    450\r",
       "  2     27450  0.305968  0.397985    169.0  3366.0      6    450\r",
       "  3     27450  0.459582  0.519534    290.0  4100.0      6    450\r",
       "  4     27450  0.245345  0.378851    115.0  2197.0      6    450\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   27450  0.542864  0.715727    481.0  4941.0      6    450\r",
       "  496   27450  0.366473  0.503300    225.0  4529.0      6    450\r",
       "  497   27450  0.583311  0.612531    496.0  2239.0      6    450\r",
       "  498   27450  0.690556  0.789495    426.0   333.0      6    450\r",
       "  499   27450  0.533279  0.676863    400.0  2960.0      6    450\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     27900  0.477415  0.478155    483.0    36.0      6    900\r",
       "  1     27900  0.390719  0.463299    225.0   636.0      6    900\r",
       "  2     27900  0.311488  0.398100    169.0  3366.0      6    900\r",
       "  3     27900  0.456467  0.530201    290.0  4100.0      6    900\r",
       "  4     27900  0.245529  0.377716    115.0  2197.0      6    900\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   27900  0.540382  0.713504    481.0  4941.0      6    900\r",
       "  496   27900  0.371689  0.514764    225.0  4529.0      6    900\r",
       "  497   27900  0.581215  0.607395    496.0  2239.0      6    900\r",
       "  498   27900  0.687681  0.787297    426.0   333.0      6    900\r",
       "  499   27900  0.536827  0.674460    400.0  2960.0      6    900\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     28350  0.498098  0.481267    483.0    36.0      6   1350\r",
       "  1     28350  0.397837  0.469630    225.0   636.0      6   1350\r",
       "  2     28350  0.310208  0.396566    169.0  3366.0      6   1350\r",
       "  3     28350  0.458738  0.517700    290.0  4100.0      6   1350\r",
       "  4     28350  0.246095  0.379970    115.0  2197.0      6   1350\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   28350  0.538983  0.697205    481.0  4941.0      6   1350\r",
       "  496   28350  0.363508  0.496057    225.0  4529.0      6   1350\r",
       "  497   28350  0.581545  0.599229    496.0  2239.0      6   1350\r",
       "  498   28350  0.672565  0.745825    426.0   333.0      6   1350\r",
       "  499   28350  0.531672  0.657712    400.0  2960.0      6   1350\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     28800  0.499352  0.481885    483.0    36.0      6   1800\r",
       "  1     28800  0.396187  0.463252    225.0   636.0      6   1800\r",
       "  2     28800  0.311202  0.398199    169.0  3366.0      6   1800\r",
       "  3     28800  0.466871  0.522380    290.0  4100.0      6   1800\r",
       "  4     28800  0.247231  0.381059    115.0  2197.0      6   1800\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   28800  0.538532  0.686754    481.0  4941.0      6   1800\r",
       "  496   28800  0.368664  0.497496    225.0  4529.0      6   1800\r",
       "  497   28800  0.581733  0.594015    496.0  2239.0      6   1800\r",
       "  498   28800  0.673310  0.735450    426.0   333.0      6   1800\r",
       "  499   28800  0.533649  0.657038    400.0  2960.0      6   1800\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     29250  0.445877  0.479838    483.0    36.0      6   2250\r",
       "  1     29250  0.377840  0.465716    225.0   636.0      6   2250\r",
       "  2     29250  0.300939  0.403166    169.0  3366.0      6   2250\r",
       "  3     29250  0.455555  0.547968    290.0  4100.0      6   2250\r",
       "  4     29250  0.250892  0.390912    115.0  2197.0      6   2250\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   29250  0.554565  0.772100    481.0  4941.0      6   2250\r",
       "  496   29250  0.371318  0.542631    225.0  4529.0      6   2250\r",
       "  497   29250  0.586261  0.645107    496.0  2239.0      6   2250\r",
       "  498   29250  0.716807  0.874547    426.0   333.0      6   2250\r",
       "  499   29250  0.542655  0.732656    400.0  2960.0      6   2250\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     29700  0.472874  0.481111    483.0    36.0      6   2700\r",
       "  1     29700  0.380220  0.459632    225.0   636.0      6   2700\r",
       "  2     29700  0.302729  0.399559    169.0  3366.0      6   2700\r",
       "  3     29700  0.458759  0.527247    290.0  4100.0      6   2700\r",
       "  4     29700  0.246882  0.382021    115.0  2197.0      6   2700\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   29700  0.543305  0.725027    481.0  4941.0      6   2700\r",
       "  496   29700  0.362753  0.509516    225.0  4529.0      6   2700\r",
       "  497   29700  0.579841  0.604214    496.0  2239.0      6   2700\r",
       "  498   29700  0.692516  0.799844    426.0   333.0      6   2700\r",
       "  499   29700  0.533621  0.676975    400.0  2960.0      6   2700\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     30150  0.479660  0.483856    483.0    36.0      6   3150\r",
       "  1     30150  0.387053  0.465038    225.0   636.0      6   3150\r",
       "  2     30150  0.305690  0.399283    169.0  3366.0      6   3150\r",
       "  3     30150  0.455987  0.527362    290.0  4100.0      6   3150\r",
       "  4     30150  0.246528  0.380559    115.0  2197.0      6   3150\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   30150  0.542837  0.726923    481.0  4941.0      6   3150\r",
       "  496   30150  0.362001  0.499756    225.0  4529.0      6   3150\r",
       "  497   30150  0.582180  0.617782    496.0  2239.0      6   3150\r",
       "  498   30150  0.688859  0.797463    426.0   333.0      6   3150\r",
       "  499   30150  0.532598  0.681311    400.0  2960.0      6   3150\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     30600  0.473861  0.478423    483.0    36.0      6   3600\r",
       "  1     30600  0.387120  0.462055    225.0   636.0      6   3600\r",
       "  2     30600  0.304467  0.397755    169.0  3366.0      6   3600\r",
       "  3     30600  0.456108  0.527937    290.0  4100.0      6   3600\r",
       "  4     30600  0.247183  0.380981    115.0  2197.0      6   3600\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   30600  0.543158  0.725395    481.0  4941.0      6   3600\r",
       "  496   30600  0.366319  0.506805    225.0  4529.0      6   3600\r",
       "  497   30600  0.580481  0.607309    496.0  2239.0      6   3600\r",
       "  498   30600  0.695710  0.811629    426.0   333.0      6   3600\r",
       "  499   30600  0.533329  0.679442    400.0  2960.0      6   3600\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     31050  0.482050  0.477565    483.0    36.0      6   4050\r",
       "  1     31050  0.386157  0.456186    225.0   636.0      6   4050\r",
       "  2     31050  0.305261  0.397303    169.0  3366.0      6   4050\r",
       "  3     31050  0.459525  0.521446    290.0  4100.0      6   4050\r",
       "  4     31050  0.248450  0.383526    115.0  2197.0      6   4050\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   31050  0.537388  0.701102    481.0  4941.0      6   4050\r",
       "  496   31050  0.370535  0.513491    225.0  4529.0      6   4050\r",
       "  497   31050  0.581668  0.610660    496.0  2239.0      6   4050\r",
       "  498   31050  0.689159  0.791271    426.0   333.0      6   4050\r",
       "  499   31050  0.537544  0.677603    400.0  2960.0      6   4050\r",
       "  \r",
       "  [500 rows x 7 columns],\r",
       "       ibatch      loss  loss_std  seq_len     idx  epoch  batch\r",
       "  0     31499  0.472695  0.487398    483.0    36.0      6   4499\r",
       "  1     31499  0.381268  0.463514    225.0   636.0      6   4499\r",
       "  2     31499  0.303889  0.400703    169.0  3366.0      6   4499\r",
       "  3     31499  0.452351  0.530861    290.0  4100.0      6   4499\r",
       "  4     31499  0.247544  0.382153    115.0  2197.0      6   4499\r",
       "  ..      ...       ...       ...      ...     ...    ...    ...\r",
       "  495   31499  0.547918  0.747822    481.0  4941.0      6   4499\r",
       "  496   31499  0.366523  0.521919    225.0  4529.0      6   4499\r",
       "  497   31499  0.584123  0.631734    496.0  2239.0      6   4499\r",
       "  498   31499  0.695365  0.827291    426.0   333.0      6   4499\r",
       "  499   31499  0.531615  0.690789    400.0  2960.0      6   4499\r",
       "  \r",
       "  [500 rows x 7 columns]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型， 最后的loss应该在[0.52, 0.53]区间内. \n",
    "# 每epoch需要五分钟左右(在CPU上)， 自然结束需要～20个epoch\n",
    "train_loss, valid_loss = fp.train(model, train_data, num_epochs=21, validate_callback = fp.func_partial(fp.validate_in_train, midata=valid_data, save_dir='./'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:32:46,449 - INFO - Loading model states from: earlystop_0.5282\n",
      "2021-05-12 23:32:46,454 - INFO - Loaded net state: earlystop_0.5282/net.state\n",
      "2021-05-12 23:32:46,457 - WARNING - Error in optim state_dict loading!\n",
      "2021-05-12 23:32:46,458 - INFO - Getting loss function: ['softmax+mse']\n",
      "2021-05-12 23:32:46,459 - INFO - Validating, data size: 500\n",
      "2021-05-12 23:32:46,460 - INFO -            batch size: 16\n",
      "2021-05-12 23:32:46,460 - INFO -               shuffle: False\n",
      "2021-05-12 23:32:46,461 - INFO -          # of batches: 32\n",
      "2021-05-12 23:32:46,461 - INFO -        recap interval: 2\n",
      "2021-05-12 23:32:46,461 - INFO -          loss padding: False\n",
      "2021-05-12 23:32:48,681 - INFO - ibatch:    0, loss: 0.1712, std: 0.2547\n",
      "2021-05-12 23:32:49,071 - INFO - ibatch:    2, loss: 0.2024, std: 0.2778\n",
      "2021-05-12 23:32:49,473 - INFO - ibatch:    4, loss: 0.2365, std: 0.3232\n",
      "2021-05-12 23:32:49,855 - INFO - ibatch:    6, loss: 0.1969, std: 0.2814\n",
      "2021-05-12 23:32:50,237 - INFO - ibatch:    8, loss: 0.2077, std: 0.3034\n",
      "2021-05-12 23:32:50,619 - INFO - ibatch:   10, loss: 0.2097, std: 0.2954\n",
      "2021-05-12 23:32:51,007 - INFO - ibatch:   12, loss: 0.2123, std: 0.2982\n",
      "2021-05-12 23:32:51,400 - INFO - ibatch:   14, loss: 0.1827, std: 0.2638\n",
      "2021-05-12 23:32:51,793 - INFO - ibatch:   16, loss: 0.2440, std: 0.3412\n",
      "2021-05-12 23:32:52,182 - INFO - ibatch:   18, loss: 0.2238, std: 0.3195\n",
      "2021-05-12 23:32:52,572 - INFO - ibatch:   20, loss: 0.2106, std: 0.3032\n",
      "2021-05-12 23:32:52,959 - INFO - ibatch:   22, loss: 0.2111, std: 0.3016\n",
      "2021-05-12 23:32:53,345 - INFO - ibatch:   24, loss: 0.2105, std: 0.2982\n",
      "2021-05-12 23:32:53,726 - INFO - ibatch:   26, loss: 0.2259, std: 0.3227\n",
      "2021-05-12 23:32:54,110 - INFO - ibatch:   28, loss: 0.2343, std: 0.3262\n",
      "2021-05-12 23:32:54,495 - INFO - ibatch:   30, loss: 0.2042, std: 0.2969\n",
      "2021-05-12 23:32:55,830 - INFO - Validate mean: \u001b[0;46m0.2131\u001b[0m, std: 0.1073\n"
     ]
    }
   ],
   "source": [
    "# 读取最后一个checkpoint目录 (忽略优化器state_dict读取的问题)\n",
    "# read the last saved earlystop folder （ignore the error in optimizer state_dict loading)\n",
    "fp.state_dict_load(model, model.validate_hist.saved_dirs[-1])\n",
    "# 可以改动损失函数，检测mse损失\n",
    "args.loss_fn = ['softmax+mse']\n",
    "model.loss_fn = fp.get_loss_fn(args)\n",
    "valid_loss = fp.validate(model, valid_data, verbose=1, batch_size=64) # try a larger batch_size, should make no difference though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:12:51,048 - INFO - Getting loss function: ['softmax+mse']\n",
      "2021-05-12 23:12:51,050 - INFO - Validating, data size: 500\n",
      "2021-05-12 23:12:51,051 - INFO -            batch size: 128\n",
      "2021-05-12 23:12:51,107 - INFO -               shuffle: False\n",
      "2021-05-12 23:12:51,108 - INFO -          # of batches: 4\n",
      "2021-05-12 23:12:51,109 - INFO -        recap interval: 1\n",
      "2021-05-12 23:12:51,109 - INFO -          loss padding: False\n",
      "2021-05-12 23:12:55,531 - INFO - ibatch:    0, loss: 0.2093, std: 0.2943\n",
      "2021-05-12 23:12:56,960 - INFO - ibatch:    1, loss: 0.2082, std: 0.2944\n",
      "2021-05-12 23:12:58,366 - INFO - ibatch:    2, loss: 0.2145, std: 0.3070\n",
      "2021-05-12 23:12:59,645 - INFO - ibatch:    3, loss: 0.2212, std: 0.3156\n",
      "2021-05-12 23:13:00,701 - INFO - Validate mean: \u001b[0;46m0.2131\u001b[0m, std: 0.1073\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:29:07,576 - INFO - Loading data: work/data/predict.pkl\n",
      "2021-05-12 23:29:07,579 - INFO -    # of data: 112,  max seqlen: 861, user seq_length: -1\n",
      "2021-05-12 23:29:07,608 - INFO -  residue fmt: vector, nn: 0, dbn: True, attr: False, genre: upp\n",
      "2021-05-12 23:29:10,735 - INFO - Predicting, data size: 112\n",
      "2021-05-12 23:29:10,738 - INFO -            batch size: 1\n",
      "2021-05-12 23:29:10,739 - INFO -               shuffle: False\n",
      "2021-05-12 23:29:10,739 - INFO -          # of batches: 112\n",
      "2021-05-12 23:29:10,740 - INFO -        recap interval: 4\n",
      "2021-05-12 23:29:10,740 - INFO - Predicted files will be saved in: predict.files\n",
      "100%|██████████| 112/112 [00:07<00:00, 14.20it/s]\n",
      "2021-05-12 23:29:18,639 - INFO - Completed prediction of 112 samples\n"
     ]
    }
   ],
   "source": [
    "# 读取预测数据， 存储预测结果\n",
    "# Read in prediction data, and save the predicted results\n",
    "predict_data = fp.get_midata(args, data_name='predict', seq_length=-1)\n",
    "y_model, std_model = fp.predict(model, predict_data, save_dir='predict.files', batch_size=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
