{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 螺旋桨RNA UPP预测 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "# !mkdir /home/aistudio/external-libraries\n",
    "# !pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"action\": \"train\",\n",
      "   \"argv\": \"-h\",\n",
      "   \"verbose\": 1,\n",
      "   \"resume\": false,\n",
      "   \"load_dir\": null,\n",
      "   \"save_dir\": null,\n",
      "   \"save_level\": 2,\n",
      "   \"save_grpby\": [\"epoch\", \"batch\"],\n",
      "   \"log\": \"fly_paddle-May13.log\",\n",
      "   \"data_args\": \"======= data args =======\",\n",
      "   \"data_dir\": \"data\",\n",
      "   \"data_name\": \"predict\",\n",
      "   \"data_suffix\": \".pkl\",\n",
      "   \"data_size\": 0,\n",
      "   \"test_size\": 0.1,\n",
      "   \"split_seed\": null,\n",
      "   \"input_genre\": \"Seq\",\n",
      "   \"input_fmt\": \"NLC\",\n",
      "   \"seq_length\": [0, 512, -1],\n",
      "   \"residue_fmt\": \"vector\",\n",
      "   \"residue_nn\": 0,\n",
      "   \"residue_dbn\": false,\n",
      "   \"residue_attr\": false,\n",
      "   \"residue_extra\": false,\n",
      "   \"label_genre\": \"upp\",\n",
      "   \"label_fmt\": \"NL\",\n",
      "   \"label_tone\": \"none\",\n",
      "   \"label_ntype\": 2,\n",
      "   \"label_smooth\": false,\n",
      "   \"net_args\": \"======= net args =======\",\n",
      "   \"net_src_file\": \"/home/aistudio/work/code/paddle_nets.py\",\n",
      "   \"net\": \"lazylinear\",\n",
      "   \"resnet\": false,\n",
      "   \"act_fn\": \"relu\",\n",
      "   \"norm_fn\": \"none\",\n",
      "   \"norm_axis\": -1,\n",
      "   \"dropout\": 0.2,\n",
      "   \"feature_dim\": 1,\n",
      "   \"embed_dim\": 32,\n",
      "   \"embed_num\": 1,\n",
      "   \"linear_num\": 2,\n",
      "   \"linear_dim\": [32],\n",
      "   \"linear_resnet\": false,\n",
      "   \"conv1d_num\": 1,\n",
      "   \"conv1d_dim\": [32],\n",
      "   \"conv1d_resnet\": false,\n",
      "   \"conv1d_stride\": 1,\n",
      "   \"conv2d_num\": 1,\n",
      "   \"conv2d_dim\": [32],\n",
      "   \"conv2d_resnet\": false,\n",
      "   \"attn_num\": 2,\n",
      "   \"attn_nhead\": 2,\n",
      "   \"attn_act\": \"relu\",\n",
      "   \"attn_dropout\": null,\n",
      "   \"attn_ffdim\": 32,\n",
      "   \"attn_ffdropout\": null,\n",
      "   \"lstm_num\": 2,\n",
      "   \"lstm_dim\": [32],\n",
      "   \"lstm_direct\": 2,\n",
      "   \"lstm_resnet\": false,\n",
      "   \"output_num\": 1,\n",
      "   \"output_dim\": [32, 32, 2],\n",
      "   \"output_resnet\": false,\n",
      "   \"optim_args\": \"======= optim args =======\",\n",
      "   \"optim\": \"adam\",\n",
      "   \"learning_rate\": 0.003,\n",
      "   \"beta1\": 0.9,\n",
      "   \"beta2\": 0.999,\n",
      "   \"epsilon\": 1e-08,\n",
      "   \"lr_scheduler\": \"reduced\",\n",
      "   \"lr_factor\": 0.9,\n",
      "   \"lr_patience\": 10,\n",
      "   \"weight_decay\": \"none\",\n",
      "   \"l1decay\": 0.0001,\n",
      "   \"l2decay\": 0.0001,\n",
      "   \"train_args\": \"======= train/loss args =======\",\n",
      "   \"batch_size\": 4,\n",
      "   \"num_epochs\": 777,\n",
      "   \"num_recaps_per_epoch\": 30,\n",
      "   \"num_callbacks_per_epoch\": 10,\n",
      "   \"loss_fn\": [\"mse\"],\n",
      "   \"loss_fn_scale\": [1],\n",
      "   \"loss_sqrt\": false,\n",
      "   \"loss_padding\": false,\n",
      "   \"validate_callback\": null,\n",
      "   \"trainloss_rdiff\": 0.001,\n",
      "   \"validloss_rdiff\": 0.001,\n",
      "   \"trainloss_patience\": 11,\n",
      "   \"validloss_patience\": 11,\n",
      "   \"mood_args\": \"======= mood args =======\",\n",
      "   \"debug\": false,\n",
      "   \"lucky\": false,\n",
      "   \"lazy\": false,\n",
      "   \"sharp\": false,\n",
      "   \"comfort\": false,\n",
      "   \"explore\": false,\n",
      "   \"exploit\": false,\n",
      "   \"diehard\": false,\n",
      "   \"tune\": false,\n",
      "   \"action_args\": \"======= action args =======\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 程序在/work/code目录下， 需先加入路径\n",
    "import sys \n",
    "sys.path.append('/home/aistudio/work/code')\n",
    "# fly_paddle是唯一需要直接调用的模块\n",
    "# fly_paddle is the only module required for interactive sessions\n",
    "import fly_paddle as fp\n",
    "\n",
    "# args包括几乎所有需要的参数， 贯穿于几乎所有的程序调用中\n",
    "# args由fp.parse_args2()根据任务初始化, 要用到的任务包括： ‘train', 'validate', 'predict'\n",
    "# args is a structure storing most (if not all) parameters and used for most function calls.\n",
    "# args is initialized by fp.parse_args2(), depending on the specific task, such as \"train\", \"validate\" \"predict\"\n",
    "args, _ = fp.parse_args2('train')\n",
    "print(fp.gwio.json_str(args.__dict__))\n",
    "# 注： 根据不同的网络等等需要， args可能包含一些用不到的参数\n",
    "# Attention: some parameters in args may not be used depending on the network etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 两种更新args的方法： 1） args.update(**dict), 2) args.[key] = value\n",
    "# Two main ways to update values in args: 1) args.update(**dict), 2) args.[key] = value\n",
    "args.update(data_dir='work/data', data_name='train', residue_dbn=True, residue_extra=True)\n",
    "\n",
    "# 网络参数 （net parameters): \n",
    "# 网络的设计主要考虑了三个支配RNA碱基配对的因素： \n",
    "#    1) 来自于全部序列的排列组合（配分）竞争，用Attention机制来模拟\n",
    "#    2）来自于线性大分子的一维序列限制， 用LSTM结构来模拟\n",
    "#    3）来自于局部紧邻碱基的合作（比如，一个孤立的碱基对极不稳定）， 用1D Convolution来模拟\n",
    "# 所以框架由以上三个模块组成， 并在输入和输出层加了1-3个线性层。除非特意说明， 所有的隐藏层的维度为32.\n",
    "# 训练中发现高维度和深度的网络并不能给出更好的结果！\n",
    "# Three main mechanisms directing RNA base pairing are taken into consideration for the \n",
    "# design of the network architecture. \n",
    "#   1) The combinatorial configurational space of attainable RNA base pairs, approximated by Attention Mechanism\n",
    "#   2) The quasi-1D nature of unbranched, continuous RNA polymers, approximated by LSTM\n",
    "#   3) The cooperativity of neighboring bases for stable base pairing, approximated by 1D Convolution\n",
    "# Hence the neural net comprises of three main building blocks, with additional linear layers for the input and output. \n",
    "# The dimensions of all hidden layers are 32 unless noted otherwise.\n",
    "# Larger and/or deeper nets gave similar, but no better, performances!\n",
    "args.net='seq2seq_attnlstmconv1d'  # the net name defined in paddle_nets.py\n",
    "# 输入模块由一个线性层组成\n",
    "# The input block is a single linear feedforward layer\n",
    "args.linear_num = 1 # the number of linear feedforward layers\n",
    "# 三大处理模块 (the three main data-crunching blocks)\n",
    "args.attn_num = 1 # the number of transformer encoder layers\n",
    "args.lstm_num = 1 # the number of bidirectional lstm layers\n",
    "args.conv1d_num = 1 # the number of 1D convolution layers\n",
    "# 输出模块由三个线性层组成， 维度分别为32, 32, 2\n",
    "# three linear layers for the final output, with dimensions of 32, 32, and 2, respectively\n",
    "args.output_dim = [32, 32, 2]\n",
    "# 如果序列被补长到同一长度， 对归一化的影响不清楚， 所以用batch_size=1\n",
    "# If sequences are padded to the same length, such padding may interfere with normalization, hence batch_size=1 \n",
    "args.norm_fn = 'layer' # layer normalization\n",
    "args.batch_size = 1 # 1 is used in consideration of the layer norm above\n",
    "# 最后递交用的损失函数选为softmax+bce, 也可以用 softmax+mse, 结果几乎一样\n",
    "# The submitted results were trained with softmax+bce loss function. \n",
    "# Essentially the same results were obtained with softmax+mse loss function\n",
    "args.loss_fn = ['softmax+bce'] # softmax is needed here as the final output has a dimension of 2\n",
    "args.label_tone = 'soft' # soft label\n",
    "args.loss_sqrt = True # sqrt(loss) is only necessary for softmax+mse\n",
    "args.loss_padding = False # exclude padded residues from loss\n",
    "# 需要运行fp.autoconfig_args()来消除参数的不一致性\n",
    "# fp.autoconfig_args() is needed to resolve inconsistencies between parameters\n",
    "args = fp.autoconfig_args(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 10:53:48,896 - INFO - Used net definition: \u001b[0;39;46m/home/aistudio/work/code/paddle_nets.py\u001b[0m\n",
      "2021-05-13 10:53:48,986 - INFO - {'total_params': 36418, 'trainable_params': 36418}\n",
      "2021-05-13 10:53:48,987 - INFO - Optimizer method: adam\n",
      "2021-05-13 10:53:48,988 - INFO -    learning rate: 0.003\n",
      "2021-05-13 10:53:48,988 - INFO -     lr_scheduler: reduced\n",
      "2021-05-13 10:53:48,989 - INFO -     weight decay: none\n",
      "2021-05-13 10:53:48,989 - INFO -          l1decay: 0.0001\n",
      "2021-05-13 10:53:48,991 - INFO -          l2decay: 0.0001\n",
      "2021-05-13 10:53:48,991 - INFO - Getting loss function: ['softmax+bce']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "      Layer (type)                          Input Shape                                  Output Shape                   Param #    \n",
      "=====================================================================================================================================\n",
      "   MyEmbeddingLayer-1                      [[2, 512, 10]]                                [2, 512, 10]                      0       \n",
      "        Linear-1                           [[2, 512, 10]]                                [2, 512, 32]                     352      \n",
      "         ReLU-1                            [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-1                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Dropout-1                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "     MyLinearTower-1                       [[2, 512, 10]]                                [2, 512, 32]                      0       \n",
      "    PositionEncoder-1                      [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-2                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Linear-2                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Linear-3                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Linear-4                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Linear-5                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "  MultiHeadAttention-1    [[2, 512, 32], [2, 512, 32], [2, 512, 32], None]               [2, 512, 32]                      0       \n",
      "        Dropout-3                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-3                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Linear-6                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Dropout-2                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "        Linear-7                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Dropout-4                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "TransformerEncoderLayer-1                  [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "  TransformerEncoder-1                  [[2, 512, 32], None]                             [2, 512, 32]                      0       \n",
      "      MyAttnTower-1                        [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "         LSTM-1                            [[2, 512, 32]]                  [[2, 512, 64], [[2, 2, 32], [2, 2, 32]]]     16,896     \n",
      "      MyLSTMTower-1                        [[2, 512, 32]]                                [2, 512, 64]                      0       \n",
      "        Conv1D-1                           [[2, 512, 64]]                                [2, 512, 32]                   10,272     \n",
      "         ReLU-2                            [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-4                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Dropout-5                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "     MyConv1DTower-1                       [[2, 512, 64]]                                [2, 512, 32]                      0       \n",
      "        Linear-8                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "         ReLU-3                            [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-5                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Dropout-6                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "        Linear-9                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "         ReLU-4                            [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-6                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Dropout-7                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "        Linear-10                          [[2, 512, 32]]                                [2, 512, 2]                      66       \n",
      "     MyLinearTower-2                       [[2, 512, 32]]                                [2, 512, 2]                       0       \n",
      "=====================================================================================================================================\n",
      "Total params: 36,418\n",
      "Trainable params: 36,418\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 9.73\n",
      "Params size (MB): 0.14\n",
      "Estimated Total Size (MB): 9.91\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 建立和检测模型 （Get and inspect the model）\n",
    "model = fp.get_model(args)\n",
    "# 注： 最后的输出矩阵的维度为[N, L, 2]\n",
    "# Note: the shape of the output is [N, L, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 10:53:54,736 - INFO - Loading data: work/data/train.pkl\n",
      "2021-05-13 10:53:54,799 - INFO -    # of data: 5000,  max seqlen: 500, user seq_length: [0, 512, -1]\n",
      "2021-05-13 10:53:54,800 - INFO -  residue fmt: vector, nn: 0, dbn: True, attr: False, genre: upp\n",
      "2021-05-13 10:53:54,825 - INFO - Selected 5000 data sets with length range: [0, 512, -1]\n",
      "2021-05-13 10:54:00,860 - INFO - Processing upp data...\n"
     ]
    }
   ],
   "source": [
    "# 读取数据. 提供的数据被转换成了一个dict, 存储为pickle文件. \n",
    "# 输入矩阵中最后两列的数据为linear_partition_c和linear_partition_v的预测结果\n",
    "# read in data. The provided data are transfomed into a dict, which is saved as a pickle file\n",
    "# the last two columns in the input matrix are the predictions of linear_partition_c and linear_partition_v\n",
    "midata = fp.get_midata(args)\n",
    "train_data, valid_data = fp.train_test_split(midata, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 10:54:07,183 - INFO - Training, data size: 4500\n",
      "2021-05-13 10:54:07,184 - INFO -          batch size: 1\n",
      "2021-05-13 10:54:07,185 - INFO -             shuffle: True\n",
      "2021-05-13 10:54:07,185 - INFO -        # of batches: 4500\n",
      "2021-05-13 10:54:07,186 - INFO -      recap interval: 151\n",
      "2021-05-13 10:54:07,186 - INFO -   validate interval: 450\n",
      "2021-05-13 10:54:07,187 - INFO -         # of epochs: 21\n",
      "2021-05-13 10:54:07,187 - INFO -        loss padding: False\n",
      "2021-05-13 10:54:08,028 - INFO - Epoch/batch: 0/   0, ibatch:    0, loss: \u001b[0;36m0.8334\u001b[0m, std: 0.4388\n",
      "2021-05-13 10:54:16,475 - INFO - loss: \u001b[0;32m0.8304\u001b[0m, std: 0.4166\n",
      "2021-05-13 10:54:22,838 - INFO - Epoch/batch: 0/ 151, ibatch:  151, loss: \u001b[0;36m0.6754\u001b[0m, std: 0.6512\n",
      "2021-05-13 10:54:29,126 - INFO - Epoch/batch: 0/ 302, ibatch:  302, loss: \u001b[0;36m0.5804\u001b[0m, std: 0.6641\n",
      "2021-05-13 11:01:14,115 - INFO - loss: \u001b[0;32m0.5391\u001b[0m, std: 0.6125\n",
      "2021-05-13 11:01:14,636 - INFO - Epoch/batch: 1/1812, ibatch: 6312, loss: \u001b[0;36m0.5300\u001b[0m, std: 0.6133\n",
      "2021-05-13 11:01:20,789 - INFO - Epoch/batch: 1/1963, ibatch: 6463, loss: \u001b[0;36m0.5311\u001b[0m, std: 0.6163\n",
      "2021-05-13 11:01:26,875 - INFO - Epoch/batch: 1/2114, ibatch: 6614, loss: \u001b[0;36m0.5575\u001b[0m, std: 0.6432\n",
      "2021-05-13 11:01:42,708 - INFO - loss: \u001b[0;32m0.5403\u001b[0m, std: 0.6507\n",
      "2021-05-13 11:01:43,275 - INFO - Epoch/batch: 1/2265, ibatch: 6765, loss: \u001b[0;36m0.5491\u001b[0m, std: 0.6244\n",
      "2021-05-13 11:01:49,697 - INFO - Epoch/batch: 1/2416, ibatch: 6916, loss: \u001b[0;36m0.5605\u001b[0m, std: 0.6291\n",
      "2021-05-13 11:01:55,888 - INFO - Epoch/batch: 1/2567, ibatch: 7067, loss: \u001b[0;36m0.5581\u001b[0m, std: 0.6292\n",
      "2021-05-13 11:02:11,595 - INFO - loss: \u001b[0;32m0.5414\u001b[0m, std: 0.6593\n",
      "2021-05-13 11:02:12,297 - INFO - Epoch/batch: 1/2718, ibatch: 7218, loss: \u001b[0;36m0.5312\u001b[0m, std: 0.6129\n",
      "2021-05-13 11:02:18,714 - INFO - Epoch/batch: 1/2869, ibatch: 7369, loss: \u001b[0;36m0.5505\u001b[0m, std: 0.6209\n",
      "2021-05-13 11:02:24,823 - INFO - Epoch/batch: 1/3020, ibatch: 7520, loss: \u001b[0;36m0.5339\u001b[0m, std: 0.6077\n",
      "2021-05-13 11:02:40,231 - INFO - loss: \u001b[0;32m0.5351\u001b[0m, std: 0.6097\n",
      "2021-05-13 11:02:40,255 - INFO - Saved model states in: earlystop_0.5351\n",
      "2021-05-13 11:02:40,257 - INFO - Saved net python code: earlystop_0.5351/paddle_nets.py\n",
      "2021-05-13 11:02:40,267 - INFO - Saved best model: earlystop_0.5351\n",
      "2021-05-13 11:02:40,268 - INFO - Removing earlystop model: earlystop_0.5385\n",
      "2021-05-13 11:02:41,208 - INFO - Epoch/batch: 1/3171, ibatch: 7671, loss: \u001b[0;36m0.5360\u001b[0m, std: 0.6195\n",
      "2021-05-13 11:02:47,285 - INFO - Epoch/batch: 1/3322, ibatch: 7822, loss: \u001b[0;36m0.5405\u001b[0m, std: 0.6197\n",
      "2021-05-13 11:02:53,600 - INFO - Epoch/batch: 1/3473, ibatch: 7973, loss: \u001b[0;36m0.5372\u001b[0m, std: 0.6230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: ReduceOnPlateau set learning rate to 0.0024300000000000003.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:03:08,721 - INFO - loss: \u001b[0;32m0.5360\u001b[0m, std: 0.6108\n",
      "2021-05-13 11:03:09,684 - INFO - Epoch/batch: 1/3624, ibatch: 8124, loss: \u001b[0;36m0.5452\u001b[0m, std: 0.6181\n",
      "2021-05-13 11:03:15,831 - INFO - Epoch/batch: 1/3775, ibatch: 8275, loss: \u001b[0;36m0.5598\u001b[0m, std: 0.6385\n",
      "2021-05-13 11:03:21,895 - INFO - Epoch/batch: 1/3926, ibatch: 8426, loss: \u001b[0;36m0.5343\u001b[0m, std: 0.6129\n",
      "2021-05-13 11:03:36,723 - INFO - loss: \u001b[0;32m0.5340\u001b[0m, std: 0.6057\n",
      "2021-05-13 11:03:36,741 - INFO - Saved model states in: earlystop_0.5340\n",
      "2021-05-13 11:03:36,743 - INFO - Saved net python code: earlystop_0.5340/paddle_nets.py\n",
      "2021-05-13 11:03:36,752 - INFO - Saved best model: earlystop_0.5340\n",
      "2021-05-13 11:03:36,753 - INFO - Removing earlystop model: earlystop_0.5351\n",
      "2021-05-13 11:03:37,964 - INFO - Epoch/batch: 1/4077, ibatch: 8577, loss: \u001b[0;36m0.5533\u001b[0m, std: 0.6344\n",
      "2021-05-13 11:03:44,011 - INFO - Epoch/batch: 1/4228, ibatch: 8728, loss: \u001b[0;36m0.5616\u001b[0m, std: 0.6299\n",
      "2021-05-13 11:03:50,188 - INFO - Epoch/batch: 1/4379, ibatch: 8879, loss: \u001b[0;36m0.5393\u001b[0m, std: 0.6175\n",
      "2021-05-13 11:04:04,966 - INFO - loss: \u001b[0;32m0.5368\u001b[0m, std: 0.6405\n",
      "2021-05-13 11:04:06,072 - INFO - Epoch 1 average training loss: \u001b[0;46m0.5474\u001b[0m std: 0.6261\n",
      "2021-05-13 11:04:06,076 - INFO - Epoch 1 average validate loss: \u001b[0;46m0.5402\u001b[0m std: 0.6335\n",
      "2021-05-13 11:04:07,677 - INFO - Epoch/batch: 2/   0, ibatch: 9000, loss: \u001b[0;36m0.5342\u001b[0m, std: 0.6259\n",
      "2021-05-13 11:04:17,774 - INFO - loss: \u001b[0;32m0.5362\u001b[0m, std: 0.6376\n",
      "2021-05-13 11:04:24,050 - INFO - Epoch/batch: 2/ 151, ibatch: 9151, loss: \u001b[0;36m0.5577\u001b[0m, std: 0.6348\n",
      "2021-05-13 11:04:30,012 - INFO - Epoch/batch: 2/ 302, ibatch: 9302, loss: \u001b[0;36m0.5295\u001b[0m, std: 0.6104\n",
      "2021-05-13 11:04:46,467 - INFO - loss: \u001b[0;32m0.5344\u001b[0m, std: 0.6055\n",
      "2021-05-13 11:04:46,566 - INFO - Epoch/batch: 2/ 453, ibatch: 9453, loss: \u001b[0;36m0.5493\u001b[0m, std: 0.6311\n",
      "2021-05-13 11:04:52,659 - INFO - Epoch/batch: 2/ 604, ibatch: 9604, loss: \u001b[0;36m0.5507\u001b[0m, std: 0.6251\n",
      "2021-05-13 11:04:58,628 - INFO - Epoch/batch: 2/ 755, ibatch: 9755, loss: \u001b[0;36m0.5737\u001b[0m, std: 0.6431\n",
      "2021-05-13 11:05:14,710 - INFO - loss: \u001b[0;32m0.5363\u001b[0m, std: 0.6299\n",
      "2021-05-13 11:05:15,057 - INFO - Epoch/batch: 2/ 906, ibatch: 9906, loss: \u001b[0;36m0.5441\u001b[0m, std: 0.6213\n",
      "2021-05-13 11:05:21,164 - INFO - Epoch/batch: 2/1057, ibatch: 10057, loss: \u001b[0;36m0.5445\u001b[0m, std: 0.6222\n",
      "2021-05-13 11:05:27,131 - INFO - Epoch/batch: 2/1208, ibatch: 10208, loss: \u001b[0;36m0.5318\u001b[0m, std: 0.6142\n",
      "2021-05-13 11:05:43,283 - INFO - loss: \u001b[0;32m0.5340\u001b[0m, std: 0.6046\n",
      "2021-05-13 11:05:43,648 - INFO - Epoch/batch: 2/1359, ibatch: 10359, loss: \u001b[0;36m0.5352\u001b[0m, std: 0.6196\n",
      "2021-05-13 11:05:49,879 - INFO - Epoch/batch: 2/1510, ibatch: 10510, loss: \u001b[0;36m0.5267\u001b[0m, std: 0.6076\n",
      "2021-05-13 11:05:55,822 - INFO - Epoch/batch: 2/1661, ibatch: 10661, loss: \u001b[0;36m0.5305\u001b[0m, std: 0.6146\n",
      "2021-05-13 11:06:11,105 - INFO - loss: \u001b[0;32m0.5346\u001b[0m, std: 0.6276\n",
      "2021-05-13 11:06:11,632 - INFO - Epoch/batch: 2/1812, ibatch: 10812, loss: \u001b[0;36m0.5330\u001b[0m, std: 0.6111\n",
      "2021-05-13 11:06:17,942 - INFO - Epoch/batch: 2/1963, ibatch: 10963, loss: \u001b[0;36m0.5626\u001b[0m, std: 0.6322\n",
      "2021-05-13 11:06:23,893 - INFO - Epoch/batch: 2/2114, ibatch: 11114, loss: \u001b[0;36m0.5377\u001b[0m, std: 0.6241\n",
      "2021-05-13 11:06:39,617 - INFO - loss: \u001b[0;32m0.5343\u001b[0m, std: 0.6057\n",
      "2021-05-13 11:06:40,286 - INFO - Epoch/batch: 2/2265, ibatch: 11265, loss: \u001b[0;36m0.5495\u001b[0m, std: 0.6213\n",
      "2021-05-13 11:06:46,469 - INFO - Epoch/batch: 2/2416, ibatch: 11416, loss: \u001b[0;36m0.5418\u001b[0m, std: 0.6216\n",
      "2021-05-13 11:06:52,540 - INFO - Epoch/batch: 2/2567, ibatch: 11567, loss: \u001b[0;36m0.5418\u001b[0m, std: 0.6252\n",
      "2021-05-13 11:07:07,994 - INFO - loss: \u001b[0;32m0.5315\u001b[0m, std: 0.6098\n",
      "2021-05-13 11:07:08,015 - INFO - Saved model states in: earlystop_0.5315\n",
      "2021-05-13 11:07:08,017 - INFO - Saved net python code: earlystop_0.5315/paddle_nets.py\n",
      "2021-05-13 11:07:08,052 - INFO - Saved best model: earlystop_0.5315\n",
      "2021-05-13 11:07:08,054 - INFO - Removing earlystop model: earlystop_0.5340\n",
      "2021-05-13 11:07:08,874 - INFO - Epoch/batch: 2/2718, ibatch: 11718, loss: \u001b[0;36m0.5585\u001b[0m, std: 0.6257\n",
      "2021-05-13 11:07:15,487 - INFO - Epoch/batch: 2/2869, ibatch: 11869, loss: \u001b[0;36m0.5356\u001b[0m, std: 0.6128\n",
      "2021-05-13 11:07:22,132 - INFO - Epoch/batch: 2/3020, ibatch: 12020, loss: \u001b[0;36m0.5549\u001b[0m, std: 0.6325\n",
      "2021-05-13 11:07:37,598 - INFO - loss: \u001b[0;32m0.5317\u001b[0m, std: 0.6093\n",
      "2021-05-13 11:07:38,572 - INFO - Epoch/batch: 2/3171, ibatch: 12171, loss: \u001b[0;36m0.5331\u001b[0m, std: 0.6213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82: ReduceOnPlateau set learning rate to 0.002187.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:07:44,854 - INFO - Epoch/batch: 2/3322, ibatch: 12322, loss: \u001b[0;36m0.5294\u001b[0m, std: 0.6157\n",
      "2021-05-13 11:07:50,888 - INFO - Epoch/batch: 2/3473, ibatch: 12473, loss: \u001b[0;36m0.5570\u001b[0m, std: 0.6235\n",
      "2021-05-13 11:08:05,917 - INFO - loss: \u001b[0;32m0.5335\u001b[0m, std: 0.6285\n",
      "2021-05-13 11:08:06,937 - INFO - Epoch/batch: 2/3624, ibatch: 12624, loss: \u001b[0;36m0.5400\u001b[0m, std: 0.6211\n",
      "2021-05-13 11:08:12,895 - INFO - Epoch/batch: 2/3775, ibatch: 12775, loss: \u001b[0;36m0.5449\u001b[0m, std: 0.6272\n",
      "2021-05-13 11:08:19,167 - INFO - Epoch/batch: 2/3926, ibatch: 12926, loss: \u001b[0;36m0.5416\u001b[0m, std: 0.6182\n",
      "2021-05-13 11:08:35,285 - INFO - loss: \u001b[0;32m0.5328\u001b[0m, std: 0.6016\n",
      "2021-05-13 11:08:36,422 - INFO - Epoch/batch: 2/4077, ibatch: 13077, loss: \u001b[0;36m0.5526\u001b[0m, std: 0.6340\n",
      "2021-05-13 11:08:42,406 - INFO - Epoch/batch: 2/4228, ibatch: 13228, loss: \u001b[0;36m0.5318\u001b[0m, std: 0.6078\n",
      "2021-05-13 11:08:48,446 - INFO - Epoch/batch: 2/4379, ibatch: 13379, loss: \u001b[0;36m0.5086\u001b[0m, std: 0.6004\n",
      "2021-05-13 11:09:03,491 - INFO - loss: \u001b[0;32m0.5355\u001b[0m, std: 0.6300\n",
      "2021-05-13 11:09:04,551 - INFO - Epoch 2 average training loss: \u001b[0;46m0.5424\u001b[0m std: 0.6214\n",
      "2021-05-13 11:09:04,556 - INFO - Epoch 2 average validate loss: \u001b[0;46m0.5341\u001b[0m std: 0.6173\n",
      "2021-05-13 11:09:06,475 - INFO - Epoch/batch: 3/   0, ibatch: 13500, loss: \u001b[0;36m0.5483\u001b[0m, std: 0.6236\n",
      "2021-05-13 11:09:16,682 - INFO - loss: \u001b[0;32m0.5348\u001b[0m, std: 0.6265\n",
      "2021-05-13 11:09:23,191 - INFO - Epoch/batch: 3/ 151, ibatch: 13651, loss: \u001b[0;36m0.5384\u001b[0m, std: 0.6177\n",
      "2021-05-13 11:09:29,581 - INFO - Epoch/batch: 3/ 302, ibatch: 13802, loss: \u001b[0;36m0.5492\u001b[0m, std: 0.6206\n",
      "2021-05-13 11:09:45,341 - INFO - loss: \u001b[0;32m0.5332\u001b[0m, std: 0.6164\n",
      "2021-05-13 11:09:45,451 - INFO - Epoch/batch: 3/ 453, ibatch: 13953, loss: \u001b[0;36m0.5475\u001b[0m, std: 0.6270\n",
      "2021-05-13 11:09:51,477 - INFO - Epoch/batch: 3/ 604, ibatch: 14104, loss: \u001b[0;36m0.5334\u001b[0m, std: 0.6085\n",
      "2021-05-13 11:09:58,153 - INFO - Epoch/batch: 3/ 755, ibatch: 14255, loss: \u001b[0;36m0.5664\u001b[0m, std: 0.6405\n",
      "2021-05-13 11:10:14,319 - INFO - loss: \u001b[0;32m0.5367\u001b[0m, std: 0.6403\n",
      "2021-05-13 11:10:14,559 - INFO - Epoch/batch: 3/ 906, ibatch: 14406, loss: \u001b[0;36m0.5426\u001b[0m, std: 0.6203\n",
      "2021-05-13 11:10:20,786 - INFO - Epoch/batch: 3/1057, ibatch: 14557, loss: \u001b[0;36m0.5623\u001b[0m, std: 0.6344\n",
      "2021-05-13 11:10:26,946 - INFO - Epoch/batch: 3/1208, ibatch: 14708, loss: \u001b[0;36m0.5558\u001b[0m, std: 0.6246\n",
      "2021-05-13 11:10:42,518 - INFO - loss: \u001b[0;32m0.5353\u001b[0m, std: 0.6343\n",
      "2021-05-13 11:10:42,875 - INFO - Epoch/batch: 3/1359, ibatch: 14859, loss: \u001b[0;36m0.5328\u001b[0m, std: 0.6163\n",
      "2021-05-13 11:10:49,133 - INFO - Epoch/batch: 3/1510, ibatch: 15010, loss: \u001b[0;36m0.5452\u001b[0m, std: 0.6167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101: ReduceOnPlateau set learning rate to 0.0019683.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:10:55,334 - INFO - Epoch/batch: 3/1661, ibatch: 15161, loss: \u001b[0;36m0.5343\u001b[0m, std: 0.6123\n",
      "2021-05-13 11:11:10,813 - INFO - loss: \u001b[0;32m0.5328\u001b[0m, std: 0.6166\n",
      "2021-05-13 11:11:11,292 - INFO - Epoch/batch: 3/1812, ibatch: 15312, loss: \u001b[0;36m0.5261\u001b[0m, std: 0.6101\n",
      "2021-05-13 11:11:17,743 - INFO - Epoch/batch: 3/1963, ibatch: 15463, loss: \u001b[0;36m0.5487\u001b[0m, std: 0.6273\n",
      "2021-05-13 11:11:23,712 - INFO - Epoch/batch: 3/2114, ibatch: 15614, loss: \u001b[0;36m0.5305\u001b[0m, std: 0.6057\n",
      "2021-05-13 11:11:39,424 - INFO - loss: \u001b[0;32m0.5359\u001b[0m, std: 0.6410\n",
      "2021-05-13 11:11:40,002 - INFO - Epoch/batch: 3/2265, ibatch: 15765, loss: \u001b[0;36m0.5402\u001b[0m, std: 0.6206\n",
      "2021-05-13 11:11:45,710 - INFO - Epoch/batch: 3/2416, ibatch: 15916, loss: \u001b[0;36m0.5300\u001b[0m, std: 0.6058\n",
      "2021-05-13 11:11:51,884 - INFO - Epoch/batch: 3/2567, ibatch: 16067, loss: \u001b[0;36m0.5408\u001b[0m, std: 0.6197\n",
      "2021-05-13 11:12:07,456 - INFO - loss: \u001b[0;32m0.5425\u001b[0m, std: 0.6680\n",
      "2021-05-13 11:12:08,179 - INFO - Epoch/batch: 3/2718, ibatch: 16218, loss: \u001b[0;36m0.5248\u001b[0m, std: 0.6112\n",
      "2021-05-13 11:12:14,344 - INFO - Epoch/batch: 3/2869, ibatch: 16369, loss: \u001b[0;36m0.5379\u001b[0m, std: 0.6173\n",
      "2021-05-13 11:12:20,398 - INFO - Epoch/batch: 3/3020, ibatch: 16520, loss: \u001b[0;36m0.5430\u001b[0m, std: 0.6151\n",
      "2021-05-13 11:12:36,100 - INFO - loss: \u001b[0;32m0.5329\u001b[0m, std: 0.6182\n",
      "2021-05-13 11:12:36,981 - INFO - Epoch/batch: 3/3171, ibatch: 16671, loss: \u001b[0;36m0.5364\u001b[0m, std: 0.6245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112: ReduceOnPlateau set learning rate to 0.00177147.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:12:43,371 - INFO - Epoch/batch: 3/3322, ibatch: 16822, loss: \u001b[0;36m0.5586\u001b[0m, std: 0.6344\n",
      "2021-05-13 11:12:49,513 - INFO - Epoch/batch: 3/3473, ibatch: 16973, loss: \u001b[0;36m0.5165\u001b[0m, std: 0.6051\n",
      "2021-05-13 11:13:04,611 - INFO - loss: \u001b[0;32m0.5348\u001b[0m, std: 0.6447\n",
      "2021-05-13 11:13:05,647 - INFO - Epoch/batch: 3/3624, ibatch: 17124, loss: \u001b[0;36m0.5303\u001b[0m, std: 0.6150\n",
      "2021-05-13 11:13:11,843 - INFO - Epoch/batch: 3/3775, ibatch: 17275, loss: \u001b[0;36m0.5384\u001b[0m, std: 0.6208\n",
      "2021-05-13 11:13:18,390 - INFO - Epoch/batch: 3/3926, ibatch: 17426, loss: \u001b[0;36m0.5417\u001b[0m, std: 0.6198\n",
      "2021-05-13 11:13:33,354 - INFO - loss: \u001b[0;32m0.5355\u001b[0m, std: 0.6491\n",
      "2021-05-13 11:13:34,433 - INFO - Epoch/batch: 3/4077, ibatch: 17577, loss: \u001b[0;36m0.5405\u001b[0m, std: 0.6250\n",
      "2021-05-13 11:13:40,576 - INFO - Epoch/batch: 3/4228, ibatch: 17728, loss: \u001b[0;36m0.5387\u001b[0m, std: 0.6175\n",
      "2021-05-13 11:13:46,891 - INFO - Epoch/batch: 3/4379, ibatch: 17879, loss: \u001b[0;36m0.5263\u001b[0m, std: 0.6115\n",
      "2021-05-13 11:14:01,828 - INFO - loss: \u001b[0;32m0.5350\u001b[0m, std: 0.5787\n",
      "2021-05-13 11:14:03,094 - INFO - Epoch 3 average training loss: \u001b[0;46m0.5395\u001b[0m std: 0.6187\n",
      "2021-05-13 11:14:03,099 - INFO - Epoch 3 average validate loss: \u001b[0;46m0.5354\u001b[0m std: 0.6303\n",
      "2021-05-13 11:14:04,738 - INFO - Epoch/batch: 4/   0, ibatch: 18000, loss: \u001b[0;36m0.5243\u001b[0m, std: 0.6157\n",
      "2021-05-13 11:14:14,672 - INFO - loss: \u001b[0;32m0.5357\u001b[0m, std: 0.5760\n",
      "2021-05-13 11:14:20,422 - INFO - Epoch/batch: 4/ 151, ibatch: 18151, loss: \u001b[0;36m0.5255\u001b[0m, std: 0.5958\n",
      "2021-05-13 11:14:26,428 - INFO - Epoch/batch: 4/ 302, ibatch: 18302, loss: \u001b[0;36m0.5530\u001b[0m, std: 0.6262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123: ReduceOnPlateau set learning rate to 0.0015943230000000001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:14:43,011 - INFO - loss: \u001b[0;32m0.5325\u001b[0m, std: 0.5884\n",
      "2021-05-13 11:14:43,166 - INFO - Epoch/batch: 4/ 453, ibatch: 18453, loss: \u001b[0;36m0.5415\u001b[0m, std: 0.6178\n",
      "2021-05-13 11:14:49,677 - INFO - Epoch/batch: 4/ 604, ibatch: 18604, loss: \u001b[0;36m0.5439\u001b[0m, std: 0.6268\n",
      "2021-05-13 11:14:55,896 - INFO - Epoch/batch: 4/ 755, ibatch: 18755, loss: \u001b[0;36m0.5471\u001b[0m, std: 0.6282\n",
      "2021-05-13 11:15:12,126 - INFO - loss: \u001b[0;32m0.5310\u001b[0m, std: 0.6273\n",
      "2021-05-13 11:15:12,148 - INFO - Saved model states in: earlystop_0.5310\n",
      "2021-05-13 11:15:12,150 - INFO - Saved net python code: earlystop_0.5310/paddle_nets.py\n",
      "2021-05-13 11:15:12,161 - INFO - Saved best model: earlystop_0.5310\n",
      "2021-05-13 11:15:12,163 - INFO - Removing earlystop model: earlystop_0.5315\n",
      "2021-05-13 11:15:12,443 - INFO - Epoch/batch: 4/ 906, ibatch: 18906, loss: \u001b[0;36m0.5342\u001b[0m, std: 0.6113\n",
      "2021-05-13 11:15:18,829 - INFO - Epoch/batch: 4/1057, ibatch: 19057, loss: \u001b[0;36m0.5393\u001b[0m, std: 0.6218\n",
      "2021-05-13 11:15:25,199 - INFO - Epoch/batch: 4/1208, ibatch: 19208, loss: \u001b[0;36m0.5411\u001b[0m, std: 0.6249\n",
      "2021-05-13 11:15:40,978 - INFO - loss: \u001b[0;32m0.5369\u001b[0m, std: 0.6535\n",
      "2021-05-13 11:15:41,357 - INFO - Epoch/batch: 4/1359, ibatch: 19359, loss: \u001b[0;36m0.5390\u001b[0m, std: 0.6198\n",
      "2021-05-13 11:15:47,203 - INFO - Epoch/batch: 4/1510, ibatch: 19510, loss: \u001b[0;36m0.5210\u001b[0m, std: 0.5942\n",
      "2021-05-13 11:15:53,194 - INFO - Epoch/batch: 4/1661, ibatch: 19661, loss: \u001b[0;36m0.5349\u001b[0m, std: 0.6238\n",
      "2021-05-13 11:16:08,933 - INFO - loss: \u001b[0;32m0.5301\u001b[0m, std: 0.6020\n",
      "2021-05-13 11:16:08,971 - INFO - Saved model states in: earlystop_0.5301\n",
      "2021-05-13 11:16:08,973 - INFO - Saved net python code: earlystop_0.5301/paddle_nets.py\n",
      "2021-05-13 11:16:08,981 - INFO - Saved best model: earlystop_0.5301\n",
      "2021-05-13 11:16:08,981 - INFO - Removing earlystop model: earlystop_0.5310\n",
      "2021-05-13 11:16:09,479 - INFO - Epoch/batch: 4/1812, ibatch: 19812, loss: \u001b[0;36m0.5324\u001b[0m, std: 0.6068\n",
      "2021-05-13 11:16:16,104 - INFO - Epoch/batch: 4/1963, ibatch: 19963, loss: \u001b[0;36m0.5172\u001b[0m, std: 0.6025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134: ReduceOnPlateau set learning rate to 0.0014348907.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:16:22,420 - INFO - Epoch/batch: 4/2114, ibatch: 20114, loss: \u001b[0;36m0.5516\u001b[0m, std: 0.6298\n",
      "2021-05-13 11:16:38,471 - INFO - loss: \u001b[0;32m0.5335\u001b[0m, std: 0.6292\n",
      "2021-05-13 11:16:39,075 - INFO - Epoch/batch: 4/2265, ibatch: 20265, loss: \u001b[0;36m0.5352\u001b[0m, std: 0.6149\n",
      "2021-05-13 11:16:45,222 - INFO - Epoch/batch: 4/2416, ibatch: 20416, loss: \u001b[0;36m0.5373\u001b[0m, std: 0.6130\n",
      "2021-05-13 11:16:51,466 - INFO - Epoch/batch: 4/2567, ibatch: 20567, loss: \u001b[0;36m0.5410\u001b[0m, std: 0.6187\n",
      "2021-05-13 11:17:06,922 - INFO - loss: \u001b[0;32m0.5296\u001b[0m, std: 0.6092\n",
      "2021-05-13 11:17:06,940 - INFO - Saved model states in: earlystop_0.5296\n",
      "2021-05-13 11:17:06,942 - INFO - Saved net python code: earlystop_0.5296/paddle_nets.py\n",
      "2021-05-13 11:17:06,950 - INFO - Saved best model: earlystop_0.5296\n",
      "2021-05-13 11:17:06,951 - INFO - Removing earlystop model: earlystop_0.5301\n",
      "2021-05-13 11:17:07,732 - INFO - Epoch/batch: 4/2718, ibatch: 20718, loss: \u001b[0;36m0.5375\u001b[0m, std: 0.6126\n",
      "2021-05-13 11:17:13,861 - INFO - Epoch/batch: 4/2869, ibatch: 20869, loss: \u001b[0;36m0.5351\u001b[0m, std: 0.6083\n",
      "2021-05-13 11:17:20,050 - INFO - Epoch/batch: 4/3020, ibatch: 21020, loss: \u001b[0;36m0.5546\u001b[0m, std: 0.6315\n",
      "2021-05-13 11:17:35,487 - INFO - loss: \u001b[0;32m0.5316\u001b[0m, std: 0.6261\n",
      "2021-05-13 11:17:36,453 - INFO - Epoch/batch: 4/3171, ibatch: 21171, loss: \u001b[0;36m0.5293\u001b[0m, std: 0.6143\n",
      "2021-05-13 11:17:42,981 - INFO - Epoch/batch: 4/3322, ibatch: 21322, loss: \u001b[0;36m0.5464\u001b[0m, std: 0.6257\n",
      "2021-05-13 11:17:49,276 - INFO - Epoch/batch: 4/3473, ibatch: 21473, loss: \u001b[0;36m0.5303\u001b[0m, std: 0.6152\n",
      "2021-05-13 11:18:05,021 - INFO - loss: \u001b[0;32m0.5363\u001b[0m, std: 0.6537\n",
      "2021-05-13 11:18:05,983 - INFO - Epoch/batch: 4/3624, ibatch: 21624, loss: \u001b[0;36m0.5032\u001b[0m, std: 0.5916\n",
      "2021-05-13 11:18:12,287 - INFO - Epoch/batch: 4/3775, ibatch: 21775, loss: \u001b[0;36m0.5240\u001b[0m, std: 0.6043\n",
      "2021-05-13 11:18:19,106 - INFO - Epoch/batch: 4/3926, ibatch: 21926, loss: \u001b[0;36m0.5341\u001b[0m, std: 0.6138\n",
      "2021-05-13 11:18:34,533 - INFO - loss: \u001b[0;32m0.5317\u001b[0m, std: 0.6001\n",
      "2021-05-13 11:18:35,664 - INFO - Epoch/batch: 4/4077, ibatch: 22077, loss: \u001b[0;36m0.5563\u001b[0m, std: 0.6289\n",
      "2021-05-13 11:18:41,527 - INFO - Epoch/batch: 4/4228, ibatch: 22228, loss: \u001b[0;36m0.5262\u001b[0m, std: 0.6079\n",
      "2021-05-13 11:18:48,695 - INFO - Epoch/batch: 4/4379, ibatch: 22379, loss: \u001b[0;36m0.5551\u001b[0m, std: 0.6278\n",
      "2021-05-13 11:19:04,915 - INFO - loss: \u001b[0;32m0.5306\u001b[0m, std: 0.6091\n",
      "2021-05-13 11:19:05,962 - INFO - Epoch 4 average training loss: \u001b[0;46m0.5367\u001b[0m std: 0.6155\n",
      "2021-05-13 11:19:05,967 - INFO - Epoch 4 average validate loss: \u001b[0;46m0.5327\u001b[0m std: 0.6159\n",
      "2021-05-13 11:19:07,955 - INFO - Epoch/batch: 5/   0, ibatch: 22500, loss: \u001b[0;36m0.5318\u001b[0m, std: 0.6068\n",
      "2021-05-13 11:19:18,475 - INFO - loss: \u001b[0;32m0.5307\u001b[0m, std: 0.6116\n",
      "2021-05-13 11:19:25,095 - INFO - Epoch/batch: 5/ 151, ibatch: 22651, loss: \u001b[0;36m0.5422\u001b[0m, std: 0.6230\n",
      "2021-05-13 11:19:31,239 - INFO - Epoch/batch: 5/ 302, ibatch: 22802, loss: \u001b[0;36m0.5327\u001b[0m, std: 0.6194\n",
      "2021-05-13 11:19:47,486 - INFO - loss: \u001b[0;32m0.5308\u001b[0m, std: 0.5927\n",
      "2021-05-13 11:19:47,629 - INFO - Epoch/batch: 5/ 453, ibatch: 22953, loss: \u001b[0;36m0.5445\u001b[0m, std: 0.6145\n",
      "2021-05-13 11:19:53,769 - INFO - Epoch/batch: 5/ 604, ibatch: 23104, loss: \u001b[0;36m0.5525\u001b[0m, std: 0.6240\n",
      "2021-05-13 11:19:59,965 - INFO - Epoch/batch: 5/ 755, ibatch: 23255, loss: \u001b[0;36m0.5266\u001b[0m, std: 0.6073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156: ReduceOnPlateau set learning rate to 0.00129140163.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:20:15,765 - INFO - loss: \u001b[0;32m0.5344\u001b[0m, std: 0.6451\n",
      "2021-05-13 11:20:15,996 - INFO - Epoch/batch: 5/ 906, ibatch: 23406, loss: \u001b[0;36m0.5144\u001b[0m, std: 0.6026\n",
      "2021-05-13 11:20:22,290 - INFO - Epoch/batch: 5/1057, ibatch: 23557, loss: \u001b[0;36m0.5381\u001b[0m, std: 0.6193\n",
      "2021-05-13 11:20:28,435 - INFO - Epoch/batch: 5/1208, ibatch: 23708, loss: \u001b[0;36m0.5393\u001b[0m, std: 0.6179\n",
      "2021-05-13 11:20:44,729 - INFO - loss: \u001b[0;32m0.5320\u001b[0m, std: 0.6055\n",
      "2021-05-13 11:20:45,047 - INFO - Epoch/batch: 5/1359, ibatch: 23859, loss: \u001b[0;36m0.5320\u001b[0m, std: 0.6133\n",
      "2021-05-13 11:20:51,194 - INFO - Epoch/batch: 5/1510, ibatch: 24010, loss: \u001b[0;36m0.5305\u001b[0m, std: 0.6188\n",
      "2021-05-13 11:20:57,435 - INFO - Epoch/batch: 5/1661, ibatch: 24161, loss: \u001b[0;36m0.5178\u001b[0m, std: 0.6017\n",
      "2021-05-13 11:21:13,254 - INFO - loss: \u001b[0;32m0.5309\u001b[0m, std: 0.6023\n",
      "2021-05-13 11:21:13,745 - INFO - Epoch/batch: 5/1812, ibatch: 24312, loss: \u001b[0;36m0.5276\u001b[0m, std: 0.6033\n",
      "2021-05-13 11:21:19,719 - INFO - Epoch/batch: 5/1963, ibatch: 24463, loss: \u001b[0;36m0.5516\u001b[0m, std: 0.6247\n",
      "2021-05-13 11:21:25,746 - INFO - Epoch/batch: 5/2114, ibatch: 24614, loss: \u001b[0;36m0.5345\u001b[0m, std: 0.6195\n",
      "2021-05-13 11:21:41,688 - INFO - loss: \u001b[0;32m0.5309\u001b[0m, std: 0.6196\n",
      "2021-05-13 11:21:42,280 - INFO - Epoch/batch: 5/2265, ibatch: 24765, loss: \u001b[0;36m0.5260\u001b[0m, std: 0.6055\n",
      "2021-05-13 11:21:48,747 - INFO - Epoch/batch: 5/2416, ibatch: 24916, loss: \u001b[0;36m0.5469\u001b[0m, std: 0.6248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167: ReduceOnPlateau set learning rate to 0.001162261467.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:21:55,013 - INFO - Epoch/batch: 5/2567, ibatch: 25067, loss: \u001b[0;36m0.5332\u001b[0m, std: 0.6127\n",
      "2021-05-13 11:22:11,250 - INFO - loss: \u001b[0;32m0.5298\u001b[0m, std: 0.6014\n",
      "2021-05-13 11:22:12,021 - INFO - Epoch/batch: 5/2718, ibatch: 25218, loss: \u001b[0;36m0.5518\u001b[0m, std: 0.6234\n",
      "2021-05-13 11:22:18,115 - INFO - Epoch/batch: 5/2869, ibatch: 25369, loss: \u001b[0;36m0.5381\u001b[0m, std: 0.6154\n",
      "2021-05-13 11:22:24,309 - INFO - Epoch/batch: 5/3020, ibatch: 25520, loss: \u001b[0;36m0.5292\u001b[0m, std: 0.6113\n",
      "2021-05-13 11:22:40,147 - INFO - loss: \u001b[0;32m0.5316\u001b[0m, std: 0.6178\n",
      "2021-05-13 11:22:40,935 - INFO - Epoch/batch: 5/3171, ibatch: 25671, loss: \u001b[0;36m0.5342\u001b[0m, std: 0.6165\n",
      "2021-05-13 11:22:47,119 - INFO - Epoch/batch: 5/3322, ibatch: 25822, loss: \u001b[0;36m0.5495\u001b[0m, std: 0.6194\n",
      "2021-05-13 11:22:52,932 - INFO - Epoch/batch: 5/3473, ibatch: 25973, loss: \u001b[0;36m0.5286\u001b[0m, std: 0.6140\n",
      "2021-05-13 11:23:08,955 - INFO - loss: \u001b[0;32m0.5287\u001b[0m, std: 0.5958\n",
      "2021-05-13 11:23:08,977 - INFO - Saved model states in: earlystop_0.5287\n",
      "2021-05-13 11:23:08,979 - INFO - Saved net python code: earlystop_0.5287/paddle_nets.py\n",
      "2021-05-13 11:23:08,989 - INFO - Saved best model: earlystop_0.5287\n",
      "2021-05-13 11:23:08,990 - INFO - Removing earlystop model: earlystop_0.5296\n",
      "2021-05-13 11:23:09,973 - INFO - Epoch/batch: 5/3624, ibatch: 26124, loss: \u001b[0;36m0.5303\u001b[0m, std: 0.6094\n",
      "2021-05-13 11:23:16,307 - INFO - Epoch/batch: 5/3775, ibatch: 26275, loss: \u001b[0;36m0.5432\u001b[0m, std: 0.6210\n",
      "2021-05-13 11:23:22,093 - INFO - Epoch/batch: 5/3926, ibatch: 26426, loss: \u001b[0;36m0.5373\u001b[0m, std: 0.6118\n",
      "2021-05-13 11:23:37,126 - INFO - loss: \u001b[0;32m0.5302\u001b[0m, std: 0.6009\n",
      "2021-05-13 11:23:38,307 - INFO - Epoch/batch: 5/4077, ibatch: 26577, loss: \u001b[0;36m0.5213\u001b[0m, std: 0.6083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178: ReduceOnPlateau set learning rate to 0.0010460353203000001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:23:44,104 - INFO - Epoch/batch: 5/4228, ibatch: 26728, loss: \u001b[0;36m0.5367\u001b[0m, std: 0.6049\n",
      "2021-05-13 11:23:50,317 - INFO - Epoch/batch: 5/4379, ibatch: 26879, loss: \u001b[0;36m0.5401\u001b[0m, std: 0.6200\n",
      "2021-05-13 11:24:05,806 - INFO - loss: \u001b[0;32m0.5307\u001b[0m, std: 0.6333\n",
      "2021-05-13 11:24:06,950 - INFO - Epoch 5 average training loss: \u001b[0;46m0.5351\u001b[0m std: 0.6146\n",
      "2021-05-13 11:24:06,955 - INFO - Epoch 5 average validate loss: \u001b[0;46m0.5310\u001b[0m std: 0.6115\n",
      "2021-05-13 11:24:08,780 - INFO - Epoch/batch: 6/   0, ibatch: 27000, loss: \u001b[0;36m0.5184\u001b[0m, std: 0.6090\n",
      "2021-05-13 11:24:19,183 - INFO - loss: \u001b[0;32m0.5306\u001b[0m, std: 0.6328\n",
      "2021-05-13 11:24:25,185 - INFO - Epoch/batch: 6/ 151, ibatch: 27151, loss: \u001b[0;36m0.5322\u001b[0m, std: 0.6119\n",
      "2021-05-13 11:24:31,590 - INFO - Epoch/batch: 6/ 302, ibatch: 27302, loss: \u001b[0;36m0.5397\u001b[0m, std: 0.6177\n",
      "2021-05-13 11:24:48,147 - INFO - loss: \u001b[0;32m0.5304\u001b[0m, std: 0.5910\n",
      "2021-05-13 11:24:48,323 - INFO - Epoch/batch: 6/ 453, ibatch: 27453, loss: \u001b[0;36m0.5446\u001b[0m, std: 0.6208\n",
      "2021-05-13 11:24:54,432 - INFO - Epoch/batch: 6/ 604, ibatch: 27604, loss: \u001b[0;36m0.5370\u001b[0m, std: 0.6183\n",
      "2021-05-13 11:25:00,208 - INFO - Epoch/batch: 6/ 755, ibatch: 27755, loss: \u001b[0;36m0.5145\u001b[0m, std: 0.6036\n",
      "2021-05-13 11:25:16,228 - INFO - loss: \u001b[0;32m0.5282\u001b[0m, std: 0.6144\n",
      "2021-05-13 11:25:16,249 - INFO - Saved model states in: earlystop_0.5282.1\n",
      "2021-05-13 11:25:16,252 - INFO - Saved net python code: earlystop_0.5282.1/paddle_nets.py\n",
      "2021-05-13 11:25:16,265 - INFO - Saved best model: earlystop_0.5282.1\n",
      "2021-05-13 11:25:16,266 - INFO - Removing earlystop model: earlystop_0.5287\n",
      "2021-05-13 11:25:16,526 - INFO - Epoch/batch: 6/ 906, ibatch: 27906, loss: \u001b[0;36m0.5282\u001b[0m, std: 0.6059\n",
      "2021-05-13 11:25:22,651 - INFO - Epoch/batch: 6/1057, ibatch: 28057, loss: \u001b[0;36m0.5315\u001b[0m, std: 0.6195\n",
      "2021-05-13 11:25:28,810 - INFO - Epoch/batch: 6/1208, ibatch: 28208, loss: \u001b[0;36m0.5208\u001b[0m, std: 0.6066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189: ReduceOnPlateau set learning rate to 0.0009414317882700001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:25:45,629 - INFO - loss: \u001b[0;32m0.5309\u001b[0m, std: 0.5961\n",
      "2021-05-13 11:25:46,016 - INFO - Epoch/batch: 6/1359, ibatch: 28359, loss: \u001b[0;36m0.5290\u001b[0m, std: 0.6117\n",
      "2021-05-13 11:25:52,482 - INFO - Epoch/batch: 6/1510, ibatch: 28510, loss: \u001b[0;36m0.5483\u001b[0m, std: 0.6182\n",
      "2021-05-13 11:25:58,345 - INFO - Epoch/batch: 6/1661, ibatch: 28661, loss: \u001b[0;36m0.5233\u001b[0m, std: 0.6159\n",
      "2021-05-13 11:26:14,406 - INFO - loss: \u001b[0;32m0.5296\u001b[0m, std: 0.6153\n",
      "2021-05-13 11:26:14,932 - INFO - Epoch/batch: 6/1812, ibatch: 28812, loss: \u001b[0;36m0.5222\u001b[0m, std: 0.6061\n",
      "2021-05-13 11:26:21,022 - INFO - Epoch/batch: 6/1963, ibatch: 28963, loss: \u001b[0;36m0.5185\u001b[0m, std: 0.6054\n",
      "2021-05-13 11:26:26,839 - INFO - Epoch/batch: 6/2114, ibatch: 29114, loss: \u001b[0;36m0.5356\u001b[0m, std: 0.6108\n",
      "2021-05-13 11:26:42,964 - INFO - loss: \u001b[0;32m0.5324\u001b[0m, std: 0.6383\n",
      "2021-05-13 11:26:43,557 - INFO - Epoch/batch: 6/2265, ibatch: 29265, loss: \u001b[0;36m0.5366\u001b[0m, std: 0.6178\n",
      "2021-05-13 11:26:50,007 - INFO - Epoch/batch: 6/2416, ibatch: 29416, loss: \u001b[0;36m0.5308\u001b[0m, std: 0.6070\n",
      "2021-05-13 11:26:56,040 - INFO - Epoch/batch: 6/2567, ibatch: 29567, loss: \u001b[0;36m0.5454\u001b[0m, std: 0.6277\n",
      "2021-05-13 11:27:11,976 - INFO - loss: \u001b[0;32m0.5295\u001b[0m, std: 0.6047\n",
      "2021-05-13 11:27:12,868 - INFO - Epoch/batch: 6/2718, ibatch: 29718, loss: \u001b[0;36m0.5431\u001b[0m, std: 0.6154\n",
      "2021-05-13 11:27:18,609 - INFO - Epoch/batch: 6/2869, ibatch: 29869, loss: \u001b[0;36m0.5336\u001b[0m, std: 0.6049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200: ReduceOnPlateau set learning rate to 0.0008472886094430002.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:27:24,533 - INFO - Epoch/batch: 6/3020, ibatch: 30020, loss: \u001b[0;36m0.5231\u001b[0m, std: 0.6095\n",
      "2021-05-13 11:27:40,259 - INFO - loss: \u001b[0;32m0.5283\u001b[0m, std: 0.6141\n",
      "2021-05-13 11:27:41,328 - INFO - Epoch/batch: 6/3171, ibatch: 30171, loss: \u001b[0;36m0.5479\u001b[0m, std: 0.6252\n",
      "2021-05-13 11:27:48,198 - INFO - Epoch/batch: 6/3322, ibatch: 30322, loss: \u001b[0;36m0.5316\u001b[0m, std: 0.6091\n",
      "2021-05-13 11:27:55,397 - INFO - Epoch/batch: 6/3473, ibatch: 30473, loss: \u001b[0;36m0.5374\u001b[0m, std: 0.6164\n",
      "2021-05-13 11:28:12,219 - INFO - loss: \u001b[0;32m0.5293\u001b[0m, std: 0.6127\n",
      "2021-05-13 11:28:13,188 - INFO - Epoch/batch: 6/3624, ibatch: 30624, loss: \u001b[0;36m0.5342\u001b[0m, std: 0.6066\n",
      "2021-05-13 11:28:19,267 - INFO - Epoch/batch: 6/3775, ibatch: 30775, loss: \u001b[0;36m0.5336\u001b[0m, std: 0.6107\n",
      "2021-05-13 11:28:25,360 - INFO - Epoch/batch: 6/3926, ibatch: 30926, loss: \u001b[0;36m0.5432\u001b[0m, std: 0.6095\n",
      "2021-05-13 11:28:41,147 - INFO - loss: \u001b[0;32m0.5319\u001b[0m, std: 0.6336\n",
      "2021-05-13 11:28:42,368 - INFO - Epoch/batch: 6/4077, ibatch: 31077, loss: \u001b[0;36m0.5222\u001b[0m, std: 0.6070\n",
      "2021-05-13 11:28:48,233 - INFO - Epoch/batch: 6/4228, ibatch: 31228, loss: \u001b[0;36m0.5457\u001b[0m, std: 0.6129\n",
      "2021-05-13 11:28:54,720 - INFO - Epoch/batch: 6/4379, ibatch: 31379, loss: \u001b[0;36m0.5482\u001b[0m, std: 0.6224\n",
      "2021-05-13 11:29:10,316 - INFO - loss: \u001b[0;32m0.5306\u001b[0m, std: 0.6129\n",
      "2021-05-13 11:29:11,428 - INFO - Epoch 6 average training loss: \u001b[0;46m0.5332\u001b[0m std: 0.6124\n",
      "2021-05-13 11:29:11,452 - INFO - Epoch 6 average validate loss: \u001b[0;46m0.5302\u001b[0m std: 0.6151\n",
      "2021-05-13 11:29:12,988 - INFO - Epoch/batch: 7/   0, ibatch: 31500, loss: \u001b[0;36m0.5124\u001b[0m, std: 0.5992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 211: ReduceOnPlateau set learning rate to 0.0007625597484987002.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:29:23,073 - INFO - loss: \u001b[0;32m0.5306\u001b[0m, std: 0.6124\n",
      "2021-05-13 11:29:29,575 - INFO - Epoch/batch: 7/ 151, ibatch: 31651, loss: \u001b[0;36m0.5510\u001b[0m, std: 0.6256\n",
      "2021-05-13 11:29:35,788 - INFO - Epoch/batch: 7/ 302, ibatch: 31802, loss: \u001b[0;36m0.5297\u001b[0m, std: 0.6131\n",
      "2021-05-13 11:29:52,229 - INFO - loss: \u001b[0;32m0.5292\u001b[0m, std: 0.6160\n",
      "2021-05-13 11:29:52,349 - INFO - Epoch/batch: 7/ 453, ibatch: 31953, loss: \u001b[0;36m0.5481\u001b[0m, std: 0.6206\n",
      "2021-05-13 11:29:58,483 - INFO - Epoch/batch: 7/ 604, ibatch: 32104, loss: \u001b[0;36m0.5373\u001b[0m, std: 0.6134\n",
      "2021-05-13 11:30:04,538 - INFO - Epoch/batch: 7/ 755, ibatch: 32255, loss: \u001b[0;36m0.5161\u001b[0m, std: 0.6004\n",
      "2021-05-13 11:30:20,312 - INFO - loss: \u001b[0;32m0.5288\u001b[0m, std: 0.6139\n",
      "2021-05-13 11:30:20,571 - INFO - Epoch/batch: 7/ 906, ibatch: 32406, loss: \u001b[0;36m0.5331\u001b[0m, std: 0.6166\n",
      "2021-05-13 11:30:26,284 - INFO - Epoch/batch: 7/1057, ibatch: 32557, loss: \u001b[0;36m0.5268\u001b[0m, std: 0.6039\n",
      "2021-05-13 11:30:32,373 - INFO - Epoch/batch: 7/1208, ibatch: 32708, loss: \u001b[0;36m0.5347\u001b[0m, std: 0.6140\n",
      "2021-05-13 11:30:48,185 - INFO - loss: \u001b[0;32m0.5294\u001b[0m, std: 0.6052\n",
      "2021-05-13 11:30:48,541 - INFO - Epoch/batch: 7/1359, ibatch: 32859, loss: \u001b[0;36m0.5344\u001b[0m, std: 0.6089\n",
      "2021-05-13 11:30:55,124 - INFO - Epoch/batch: 7/1510, ibatch: 33010, loss: \u001b[0;36m0.5189\u001b[0m, std: 0.6038\n",
      "2021-05-13 11:31:01,582 - INFO - Epoch/batch: 7/1661, ibatch: 33161, loss: \u001b[0;36m0.5450\u001b[0m, std: 0.6198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222: ReduceOnPlateau set learning rate to 0.0006863037736488302.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:31:17,601 - INFO - loss: \u001b[0;32m0.5290\u001b[0m, std: 0.6157\n",
      "2021-05-13 11:31:18,175 - INFO - Epoch/batch: 7/1812, ibatch: 33312, loss: \u001b[0;36m0.5230\u001b[0m, std: 0.6102\n",
      "2021-05-13 11:31:24,937 - INFO - Epoch/batch: 7/1963, ibatch: 33463, loss: \u001b[0;36m0.5269\u001b[0m, std: 0.6043\n",
      "2021-05-13 11:31:32,103 - INFO - Epoch/batch: 7/2114, ibatch: 33614, loss: \u001b[0;36m0.5427\u001b[0m, std: 0.6097\n",
      "2021-05-13 11:31:48,951 - INFO - loss: \u001b[0;32m0.5295\u001b[0m, std: 0.6220\n",
      "2021-05-13 11:31:49,546 - INFO - Epoch/batch: 7/2265, ibatch: 33765, loss: \u001b[0;36m0.5254\u001b[0m, std: 0.6121\n",
      "2021-05-13 11:31:55,714 - INFO - Epoch/batch: 7/2416, ibatch: 33916, loss: \u001b[0;36m0.5388\u001b[0m, std: 0.6223\n",
      "2021-05-13 11:32:01,765 - INFO - Epoch/batch: 7/2567, ibatch: 34067, loss: \u001b[0;36m0.5343\u001b[0m, std: 0.6126\n",
      "2021-05-13 11:32:17,215 - INFO - loss: \u001b[0;32m0.5277\u001b[0m, std: 0.5989\n",
      "2021-05-13 11:32:17,255 - INFO - Saved model states in: earlystop_0.5277\n",
      "2021-05-13 11:32:17,258 - INFO - Saved net python code: earlystop_0.5277/paddle_nets.py\n",
      "2021-05-13 11:32:17,267 - INFO - Saved best model: earlystop_0.5277\n",
      "2021-05-13 11:32:17,268 - INFO - Removing earlystop model: earlystop_0.5282.1\n",
      "2021-05-13 11:32:17,997 - INFO - Epoch/batch: 7/2718, ibatch: 34218, loss: \u001b[0;36m0.5276\u001b[0m, std: 0.6011\n",
      "2021-05-13 11:32:23,852 - INFO - Epoch/batch: 7/2869, ibatch: 34369, loss: \u001b[0;36m0.5236\u001b[0m, std: 0.5980\n",
      "2021-05-13 11:32:29,511 - INFO - Epoch/batch: 7/3020, ibatch: 34520, loss: \u001b[0;36m0.5178\u001b[0m, std: 0.6009\n",
      "2021-05-13 11:32:44,725 - INFO - loss: \u001b[0;32m0.5281\u001b[0m, std: 0.6129\n",
      "2021-05-13 11:32:45,616 - INFO - Epoch/batch: 7/3171, ibatch: 34671, loss: \u001b[0;36m0.5290\u001b[0m, std: 0.6175\n",
      "2021-05-13 11:32:51,333 - INFO - Epoch/batch: 7/3322, ibatch: 34822, loss: \u001b[0;36m0.5267\u001b[0m, std: 0.6056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233: ReduceOnPlateau set learning rate to 0.0006176733962839472.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:32:57,239 - INFO - Epoch/batch: 7/3473, ibatch: 34973, loss: \u001b[0;36m0.5323\u001b[0m, std: 0.6052\n",
      "2021-05-13 11:33:12,889 - INFO - loss: \u001b[0;32m0.5289\u001b[0m, std: 0.6235\n",
      "2021-05-13 11:33:13,790 - INFO - Epoch/batch: 7/3624, ibatch: 35124, loss: \u001b[0;36m0.5341\u001b[0m, std: 0.6137\n",
      "2021-05-13 11:33:19,960 - INFO - Epoch/batch: 7/3775, ibatch: 35275, loss: \u001b[0;36m0.5428\u001b[0m, std: 0.6214\n",
      "2021-05-13 11:33:25,797 - INFO - Epoch/batch: 7/3926, ibatch: 35426, loss: \u001b[0;36m0.5135\u001b[0m, std: 0.5948\n",
      "2021-05-13 11:33:40,901 - INFO - loss: \u001b[0;32m0.5292\u001b[0m, std: 0.6237\n",
      "2021-05-13 11:33:42,042 - INFO - Epoch/batch: 7/4077, ibatch: 35577, loss: \u001b[0;36m0.5366\u001b[0m, std: 0.6131\n",
      "2021-05-13 11:33:48,016 - INFO - Epoch/batch: 7/4228, ibatch: 35728, loss: \u001b[0;36m0.5240\u001b[0m, std: 0.6079\n",
      "2021-05-13 11:33:53,977 - INFO - Epoch/batch: 7/4379, ibatch: 35879, loss: \u001b[0;36m0.5339\u001b[0m, std: 0.6111\n",
      "2021-05-13 11:34:08,653 - INFO - loss: \u001b[0;32m0.5278\u001b[0m, std: 0.5942\n",
      "2021-05-13 11:34:09,717 - INFO - Epoch 7 average training loss: \u001b[0;46m0.5320\u001b[0m std: 0.6110\n",
      "2021-05-13 11:34:09,754 - INFO - Epoch 7 average validate loss: \u001b[0;46m0.5289\u001b[0m std: 0.6126\n",
      "2021-05-13 11:34:11,524 - INFO - Epoch/batch: 8/   0, ibatch: 36000, loss: \u001b[0;36m0.5553\u001b[0m, std: 0.6292\n",
      "2021-05-13 11:34:21,576 - INFO - loss: \u001b[0;32m0.5279\u001b[0m, std: 0.5935\n",
      "2021-05-13 11:34:27,412 - INFO - Epoch/batch: 8/ 151, ibatch: 36151, loss: \u001b[0;36m0.5325\u001b[0m, std: 0.6066\n",
      "2021-05-13 11:34:33,263 - INFO - Epoch/batch: 8/ 302, ibatch: 36302, loss: \u001b[0;36m0.5177\u001b[0m, std: 0.6085\n",
      "2021-05-13 11:34:49,026 - INFO - loss: \u001b[0;32m0.5306\u001b[0m, std: 0.6289\n",
      "2021-05-13 11:34:49,174 - INFO - Epoch/batch: 8/ 453, ibatch: 36453, loss: \u001b[0;36m0.5215\u001b[0m, std: 0.6071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 244: ReduceOnPlateau set learning rate to 0.0005559060566555524.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:34:55,463 - INFO - Epoch/batch: 8/ 604, ibatch: 36604, loss: \u001b[0;36m0.5252\u001b[0m, std: 0.6011\n",
      "2021-05-13 11:35:01,696 - INFO - Epoch/batch: 8/ 755, ibatch: 36755, loss: \u001b[0;36m0.5264\u001b[0m, std: 0.6077\n",
      "2021-05-13 11:35:17,407 - INFO - loss: \u001b[0;32m0.5278\u001b[0m, std: 0.6116\n",
      "2021-05-13 11:35:17,669 - INFO - Epoch/batch: 8/ 906, ibatch: 36906, loss: \u001b[0;36m0.5341\u001b[0m, std: 0.6136\n",
      "2021-05-13 11:35:23,884 - INFO - Epoch/batch: 8/1057, ibatch: 37057, loss: \u001b[0;36m0.5323\u001b[0m, std: 0.6157\n",
      "2021-05-13 11:35:30,053 - INFO - Epoch/batch: 8/1208, ibatch: 37208, loss: \u001b[0;36m0.5374\u001b[0m, std: 0.6234\n",
      "2021-05-13 11:35:45,906 - INFO - loss: \u001b[0;32m0.5293\u001b[0m, std: 0.6269\n",
      "2021-05-13 11:35:46,247 - INFO - Epoch/batch: 8/1359, ibatch: 37359, loss: \u001b[0;36m0.5292\u001b[0m, std: 0.6098\n",
      "2021-05-13 11:35:52,412 - INFO - Epoch/batch: 8/1510, ibatch: 37510, loss: \u001b[0;36m0.5361\u001b[0m, std: 0.6153\n",
      "2021-05-13 11:35:58,656 - INFO - Epoch/batch: 8/1661, ibatch: 37661, loss: \u001b[0;36m0.5502\u001b[0m, std: 0.6238\n",
      "2021-05-13 11:36:14,501 - INFO - loss: \u001b[0;32m0.5285\u001b[0m, std: 0.6138\n",
      "2021-05-13 11:36:14,996 - INFO - Epoch/batch: 8/1812, ibatch: 37812, loss: \u001b[0;36m0.5316\u001b[0m, std: 0.5968\n",
      "2021-05-13 11:36:21,172 - INFO - Epoch/batch: 8/1963, ibatch: 37963, loss: \u001b[0;36m0.5097\u001b[0m, std: 0.5982\n",
      "2021-05-13 11:36:27,371 - INFO - Epoch/batch: 8/2114, ibatch: 38114, loss: \u001b[0;36m0.5264\u001b[0m, std: 0.6090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 255: ReduceOnPlateau set learning rate to 0.0005003154509899972.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:36:43,402 - INFO - loss: \u001b[0;32m0.5279\u001b[0m, std: 0.6093\n",
      "2021-05-13 11:36:44,056 - INFO - Epoch/batch: 8/2265, ibatch: 38265, loss: \u001b[0;36m0.5195\u001b[0m, std: 0.5992\n",
      "2021-05-13 11:36:50,188 - INFO - Epoch/batch: 8/2416, ibatch: 38416, loss: \u001b[0;36m0.5108\u001b[0m, std: 0.6008\n",
      "2021-05-13 11:36:56,386 - INFO - Epoch/batch: 8/2567, ibatch: 38567, loss: \u001b[0;36m0.5483\u001b[0m, std: 0.6222\n",
      "2021-05-13 11:37:12,021 - INFO - loss: \u001b[0;32m0.5275\u001b[0m, std: 0.6075\n",
      "2021-05-13 11:37:12,042 - INFO - Saved model states in: earlystop_0.5275\n",
      "2021-05-13 11:37:12,044 - INFO - Saved net python code: earlystop_0.5275/paddle_nets.py\n",
      "2021-05-13 11:37:12,055 - INFO - Saved best model: earlystop_0.5275\n",
      "2021-05-13 11:37:12,057 - INFO - Removing earlystop model: earlystop_0.5277\n",
      "2021-05-13 11:37:12,868 - INFO - Epoch/batch: 8/2718, ibatch: 38718, loss: \u001b[0;36m0.5315\u001b[0m, std: 0.6225\n",
      "2021-05-13 11:37:18,802 - INFO - Epoch/batch: 8/2869, ibatch: 38869, loss: \u001b[0;36m0.5282\u001b[0m, std: 0.6083\n",
      "2021-05-13 11:37:24,949 - INFO - Epoch/batch: 8/3020, ibatch: 39020, loss: \u001b[0;36m0.5421\u001b[0m, std: 0.6148\n",
      "2021-05-13 11:37:39,705 - INFO - loss: \u001b[0;32m0.5321\u001b[0m, std: 0.6389\n",
      "2021-05-13 11:37:40,454 - INFO - Epoch/batch: 8/3171, ibatch: 39171, loss: \u001b[0;36m0.5033\u001b[0m, std: 0.5832\n",
      "2021-05-13 11:37:46,717 - INFO - Epoch/batch: 8/3322, ibatch: 39322, loss: \u001b[0;36m0.5431\u001b[0m, std: 0.6194\n",
      "2021-05-13 11:37:52,942 - INFO - Epoch/batch: 8/3473, ibatch: 39473, loss: \u001b[0;36m0.5433\u001b[0m, std: 0.6158\n",
      "2021-05-13 11:38:08,525 - INFO - loss: \u001b[0;32m0.5276\u001b[0m, std: 0.6055\n",
      "2021-05-13 11:38:09,518 - INFO - Epoch/batch: 8/3624, ibatch: 39624, loss: \u001b[0;36m0.5271\u001b[0m, std: 0.6016\n",
      "2021-05-13 11:38:15,707 - INFO - Epoch/batch: 8/3775, ibatch: 39775, loss: \u001b[0;36m0.5256\u001b[0m, std: 0.6079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 266: ReduceOnPlateau set learning rate to 0.00045028390589099747.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:38:21,761 - INFO - Epoch/batch: 8/3926, ibatch: 39926, loss: \u001b[0;36m0.5516\u001b[0m, std: 0.6232\n",
      "2021-05-13 11:38:37,370 - INFO - loss: \u001b[0;32m0.5287\u001b[0m, std: 0.6097\n",
      "2021-05-13 11:38:38,708 - INFO - Epoch/batch: 8/4077, ibatch: 40077, loss: \u001b[0;36m0.5368\u001b[0m, std: 0.6148\n",
      "2021-05-13 11:38:44,803 - INFO - Epoch/batch: 8/4228, ibatch: 40228, loss: \u001b[0;36m0.5332\u001b[0m, std: 0.6045\n",
      "2021-05-13 11:38:51,111 - INFO - Epoch/batch: 8/4379, ibatch: 40379, loss: \u001b[0;36m0.5315\u001b[0m, std: 0.6137\n",
      "2021-05-13 11:39:05,508 - INFO - loss: \u001b[0;32m0.5274\u001b[0m, std: 0.6073\n",
      "2021-05-13 11:39:05,527 - INFO - Saved model states in: earlystop_0.5274\n",
      "2021-05-13 11:39:05,528 - INFO - Saved net python code: earlystop_0.5274/paddle_nets.py\n",
      "2021-05-13 11:39:05,536 - INFO - Saved best model: earlystop_0.5274\n",
      "2021-05-13 11:39:05,537 - INFO - Removing earlystop model: earlystop_0.5275\n",
      "2021-05-13 11:39:06,650 - INFO - Epoch 8 average training loss: \u001b[0;46m0.5309\u001b[0m std: 0.6100\n",
      "2021-05-13 11:39:06,655 - INFO - Epoch 8 average validate loss: \u001b[0;46m0.5287\u001b[0m std: 0.6139\n",
      "2021-05-13 11:39:08,312 - INFO - Epoch/batch: 9/   0, ibatch: 40500, loss: \u001b[0;36m0.5438\u001b[0m, std: 0.6113\n",
      "2021-05-13 11:39:17,980 - INFO - loss: \u001b[0;32m0.5274\u001b[0m, std: 0.6076\n",
      "2021-05-13 11:39:18,004 - INFO - Saved model states in: earlystop_0.5274.1\n",
      "2021-05-13 11:39:18,006 - INFO - Saved net python code: earlystop_0.5274.1/paddle_nets.py\n",
      "2021-05-13 11:39:18,057 - INFO - Saved best model: earlystop_0.5274.1\n",
      "2021-05-13 11:39:18,059 - INFO - Removing earlystop model: earlystop_0.5274\n",
      "2021-05-13 11:39:24,153 - INFO - Epoch/batch: 9/ 151, ibatch: 40651, loss: \u001b[0;36m0.5385\u001b[0m, std: 0.6178\n",
      "2021-05-13 11:39:30,420 - INFO - Epoch/batch: 9/ 302, ibatch: 40802, loss: \u001b[0;36m0.5349\u001b[0m, std: 0.6176\n",
      "2021-05-13 11:39:46,419 - INFO - loss: \u001b[0;32m0.5276\u001b[0m, std: 0.6126\n",
      "2021-05-13 11:39:46,592 - INFO - Epoch/batch: 9/ 453, ibatch: 40953, loss: \u001b[0;36m0.5193\u001b[0m, std: 0.5971\n",
      "2021-05-13 11:39:53,087 - INFO - Epoch/batch: 9/ 604, ibatch: 41104, loss: \u001b[0;36m0.5369\u001b[0m, std: 0.6173\n",
      "2021-05-13 11:39:59,470 - INFO - Epoch/batch: 9/ 755, ibatch: 41255, loss: \u001b[0;36m0.5199\u001b[0m, std: 0.5979\n",
      "2021-05-13 11:40:15,398 - INFO - loss: \u001b[0;32m0.5276\u001b[0m, std: 0.6132\n",
      "2021-05-13 11:40:15,667 - INFO - Epoch/batch: 9/ 906, ibatch: 41406, loss: \u001b[0;36m0.5351\u001b[0m, std: 0.6238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 277: ReduceOnPlateau set learning rate to 0.0004052555153018977.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:40:22,184 - INFO - Epoch/batch: 9/1057, ibatch: 41557, loss: \u001b[0;36m0.5424\u001b[0m, std: 0.6199\n",
      "2021-05-13 11:40:28,520 - INFO - Epoch/batch: 9/1208, ibatch: 41708, loss: \u001b[0;36m0.5179\u001b[0m, std: 0.5975\n",
      "2021-05-13 11:40:44,329 - INFO - loss: \u001b[0;32m0.5280\u001b[0m, std: 0.6187\n",
      "2021-05-13 11:40:44,744 - INFO - Epoch/batch: 9/1359, ibatch: 41859, loss: \u001b[0;36m0.5396\u001b[0m, std: 0.6165\n",
      "2021-05-13 11:40:51,050 - INFO - Epoch/batch: 9/1510, ibatch: 42010, loss: \u001b[0;36m0.5358\u001b[0m, std: 0.6128\n",
      "2021-05-13 11:40:57,870 - INFO - Epoch/batch: 9/1661, ibatch: 42161, loss: \u001b[0;36m0.5461\u001b[0m, std: 0.6147\n",
      "2021-05-13 11:41:14,087 - INFO - loss: \u001b[0;32m0.5297\u001b[0m, std: 0.6293\n",
      "2021-05-13 11:41:14,641 - INFO - Epoch/batch: 9/1812, ibatch: 42312, loss: \u001b[0;36m0.5329\u001b[0m, std: 0.6124\n",
      "2021-05-13 11:41:20,745 - INFO - Epoch/batch: 9/1963, ibatch: 42463, loss: \u001b[0;36m0.5206\u001b[0m, std: 0.6011\n",
      "2021-05-13 11:41:26,743 - INFO - Epoch/batch: 9/2114, ibatch: 42614, loss: \u001b[0;36m0.5282\u001b[0m, std: 0.6049\n",
      "2021-05-13 11:41:42,272 - INFO - loss: \u001b[0;32m0.5273\u001b[0m, std: 0.6115\n",
      "2021-05-13 11:41:42,290 - INFO - Saved model states in: earlystop_0.5273\n",
      "2021-05-13 11:41:42,291 - INFO - Saved net python code: earlystop_0.5273/paddle_nets.py\n",
      "2021-05-13 11:41:42,299 - INFO - Saved best model: earlystop_0.5273\n",
      "2021-05-13 11:41:42,300 - INFO - Removing earlystop model: earlystop_0.5274.1\n",
      "2021-05-13 11:41:42,994 - INFO - Epoch/batch: 9/2265, ibatch: 42765, loss: \u001b[0;36m0.5237\u001b[0m, std: 0.6046\n",
      "2021-05-13 11:41:49,114 - INFO - Epoch/batch: 9/2416, ibatch: 42916, loss: \u001b[0;36m0.5397\u001b[0m, std: 0.6178\n",
      "2021-05-13 11:41:55,609 - INFO - Epoch/batch: 9/2567, ibatch: 43067, loss: \u001b[0;36m0.5289\u001b[0m, std: 0.6069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288: ReduceOnPlateau set learning rate to 0.00036472996377170795.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:42:11,587 - INFO - loss: \u001b[0;32m0.5274\u001b[0m, std: 0.6123\n",
      "2021-05-13 11:42:12,480 - INFO - Epoch/batch: 9/2718, ibatch: 43218, loss: \u001b[0;36m0.5435\u001b[0m, std: 0.6221\n",
      "2021-05-13 11:42:19,283 - INFO - Epoch/batch: 9/2869, ibatch: 43369, loss: \u001b[0;36m0.5196\u001b[0m, std: 0.6030\n",
      "2021-05-13 11:42:26,321 - INFO - Epoch/batch: 9/3020, ibatch: 43520, loss: \u001b[0;36m0.5413\u001b[0m, std: 0.6143\n",
      "2021-05-13 11:42:41,838 - INFO - loss: \u001b[0;32m0.5324\u001b[0m, std: 0.6456\n",
      "2021-05-13 11:42:42,670 - INFO - Epoch/batch: 9/3171, ibatch: 43671, loss: \u001b[0;36m0.5199\u001b[0m, std: 0.6046\n",
      "2021-05-13 11:42:48,653 - INFO - Epoch/batch: 9/3322, ibatch: 43822, loss: \u001b[0;36m0.5185\u001b[0m, std: 0.5880\n",
      "2021-05-13 11:42:54,709 - INFO - Epoch/batch: 9/3473, ibatch: 43973, loss: \u001b[0;36m0.5350\u001b[0m, std: 0.6204\n",
      "2021-05-13 11:43:10,507 - INFO - loss: \u001b[0;32m0.5292\u001b[0m, std: 0.6235\n",
      "2021-05-13 11:43:11,719 - INFO - Epoch/batch: 9/3624, ibatch: 44124, loss: \u001b[0;36m0.5166\u001b[0m, std: 0.6047\n",
      "2021-05-13 11:43:18,359 - INFO - Epoch/batch: 9/3775, ibatch: 44275, loss: \u001b[0;36m0.5537\u001b[0m, std: 0.6263\n",
      "2021-05-13 11:43:24,876 - INFO - Epoch/batch: 9/3926, ibatch: 44426, loss: \u001b[0;36m0.5173\u001b[0m, std: 0.6004\n",
      "2021-05-13 11:43:40,752 - INFO - loss: \u001b[0;32m0.5285\u001b[0m, std: 0.6065\n",
      "2021-05-13 11:43:41,949 - INFO - Epoch/batch: 9/4077, ibatch: 44577, loss: \u001b[0;36m0.5323\u001b[0m, std: 0.6148\n",
      "2021-05-13 11:43:48,000 - INFO - Epoch/batch: 9/4228, ibatch: 44728, loss: \u001b[0;36m0.5239\u001b[0m, std: 0.6046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299: ReduceOnPlateau set learning rate to 0.00032825696739453717.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:43:53,887 - INFO - Epoch/batch: 9/4379, ibatch: 44879, loss: \u001b[0;36m0.5167\u001b[0m, std: 0.5919\n",
      "2021-05-13 11:44:08,350 - INFO - loss: \u001b[0;32m0.5274\u001b[0m, std: 0.6139\n",
      "2021-05-13 11:44:09,453 - INFO - Epoch 9 average training loss: \u001b[0;46m0.5301\u001b[0m std: 0.6094\n",
      "2021-05-13 11:44:09,458 - INFO - Epoch 9 average validate loss: \u001b[0;46m0.5284\u001b[0m std: 0.6177\n",
      "2021-05-13 11:44:11,339 - INFO - Epoch/batch: 10/   0, ibatch: 45000, loss: \u001b[0;36m0.5243\u001b[0m, std: 0.6075\n",
      "2021-05-13 11:44:21,370 - INFO - loss: \u001b[0;32m0.5274\u001b[0m, std: 0.6147\n",
      "2021-05-13 11:44:27,512 - INFO - Epoch/batch: 10/ 151, ibatch: 45151, loss: \u001b[0;36m0.5381\u001b[0m, std: 0.6153\n",
      "2021-05-13 11:44:33,465 - INFO - Epoch/batch: 10/ 302, ibatch: 45302, loss: \u001b[0;36m0.5085\u001b[0m, std: 0.5913\n",
      "2021-05-13 11:44:49,325 - INFO - loss: \u001b[0;32m0.5275\u001b[0m, std: 0.6173\n",
      "2021-05-13 11:44:49,484 - INFO - Epoch/batch: 10/ 453, ibatch: 45453, loss: \u001b[0;36m0.5221\u001b[0m, std: 0.6026\n",
      "2021-05-13 11:44:55,492 - INFO - Epoch/batch: 10/ 604, ibatch: 45604, loss: \u001b[0;36m0.5119\u001b[0m, std: 0.5988\n",
      "2021-05-13 11:45:01,485 - INFO - Epoch/batch: 10/ 755, ibatch: 45755, loss: \u001b[0;36m0.5113\u001b[0m, std: 0.5981\n",
      "2021-05-13 11:45:17,008 - INFO - loss: \u001b[0;32m0.5278\u001b[0m, std: 0.6113\n",
      "2021-05-13 11:45:17,271 - INFO - Epoch/batch: 10/ 906, ibatch: 45906, loss: \u001b[0;36m0.5359\u001b[0m, std: 0.6124\n",
      "2021-05-13 11:45:23,892 - INFO - Epoch/batch: 10/1057, ibatch: 46057, loss: \u001b[0;36m0.5330\u001b[0m, std: 0.6166\n",
      "2021-05-13 11:45:30,333 - INFO - Epoch/batch: 10/1208, ibatch: 46208, loss: \u001b[0;36m0.5342\u001b[0m, std: 0.6193\n",
      "2021-05-13 11:45:46,617 - INFO - loss: \u001b[0;32m0.5276\u001b[0m, std: 0.6116\n",
      "2021-05-13 11:45:46,985 - INFO - Epoch/batch: 10/1359, ibatch: 46359, loss: \u001b[0;36m0.5430\u001b[0m, std: 0.6167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310: ReduceOnPlateau set learning rate to 0.00029543127065508344.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:45:52,904 - INFO - Epoch/batch: 10/1510, ibatch: 46510, loss: \u001b[0;36m0.5580\u001b[0m, std: 0.6268\n",
      "2021-05-13 11:45:58,935 - INFO - Epoch/batch: 10/1661, ibatch: 46661, loss: \u001b[0;36m0.5300\u001b[0m, std: 0.6046\n",
      "2021-05-13 11:46:14,598 - INFO - loss: \u001b[0;32m0.5278\u001b[0m, std: 0.6165\n",
      "2021-05-13 11:46:15,166 - INFO - Epoch/batch: 10/1812, ibatch: 46812, loss: \u001b[0;36m0.5200\u001b[0m, std: 0.5972\n",
      "2021-05-13 11:46:21,408 - INFO - Epoch/batch: 10/1963, ibatch: 46963, loss: \u001b[0;36m0.5207\u001b[0m, std: 0.6051\n",
      "2021-05-13 11:46:27,859 - INFO - Epoch/batch: 10/2114, ibatch: 47114, loss: \u001b[0;36m0.5292\u001b[0m, std: 0.6060\n",
      "2021-05-13 11:46:44,167 - INFO - loss: \u001b[0;32m0.5276\u001b[0m, std: 0.6184\n",
      "2021-05-13 11:46:44,680 - INFO - Epoch/batch: 10/2265, ibatch: 47265, loss: \u001b[0;36m0.5077\u001b[0m, std: 0.5905\n",
      "2021-05-13 11:46:50,559 - INFO - Epoch/batch: 10/2416, ibatch: 47416, loss: \u001b[0;36m0.5111\u001b[0m, std: 0.6008\n",
      "2021-05-13 11:46:56,560 - INFO - Epoch/batch: 10/2567, ibatch: 47567, loss: \u001b[0;36m0.5298\u001b[0m, std: 0.6141\n",
      "2021-05-13 11:47:11,961 - INFO - loss: \u001b[0;32m0.5292\u001b[0m, std: 0.6270\n",
      "2021-05-13 11:47:12,597 - INFO - Epoch/batch: 10/2718, ibatch: 47718, loss: \u001b[0;36m0.5285\u001b[0m, std: 0.6061\n",
      "2021-05-13 11:47:18,808 - INFO - Epoch/batch: 10/2869, ibatch: 47869, loss: \u001b[0;36m0.5403\u001b[0m, std: 0.6182\n",
      "2021-05-13 11:47:25,085 - INFO - Epoch/batch: 10/3020, ibatch: 48020, loss: \u001b[0;36m0.5284\u001b[0m, std: 0.6082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321: ReduceOnPlateau set learning rate to 0.0002658881435895751.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:47:41,154 - INFO - loss: \u001b[0;32m0.5280\u001b[0m, std: 0.6130\n",
      "2021-05-13 11:47:42,059 - INFO - Epoch/batch: 10/3171, ibatch: 48171, loss: \u001b[0;36m0.5399\u001b[0m, std: 0.6131\n",
      "2021-05-13 11:47:48,455 - INFO - Epoch/batch: 10/3322, ibatch: 48322, loss: \u001b[0;36m0.5160\u001b[0m, std: 0.5983\n",
      "2021-05-13 11:47:54,489 - INFO - Epoch/batch: 10/3473, ibatch: 48473, loss: \u001b[0;36m0.5326\u001b[0m, std: 0.6142\n",
      "2021-05-13 11:48:09,610 - INFO - loss: \u001b[0;32m0.5276\u001b[0m, std: 0.6158\n",
      "2021-05-13 11:48:10,426 - INFO - Epoch/batch: 10/3624, ibatch: 48624, loss: \u001b[0;36m0.5328\u001b[0m, std: 0.6066\n",
      "2021-05-13 11:48:16,458 - INFO - Epoch/batch: 10/3775, ibatch: 48775, loss: \u001b[0;36m0.5385\u001b[0m, std: 0.6149\n",
      "2021-05-13 11:48:22,655 - INFO - Epoch/batch: 10/3926, ibatch: 48926, loss: \u001b[0;36m0.5431\u001b[0m, std: 0.6127\n",
      "2021-05-13 11:48:37,313 - INFO - loss: \u001b[0;32m0.5281\u001b[0m, std: 0.6193\n",
      "2021-05-13 11:48:38,400 - INFO - Epoch/batch: 10/4077, ibatch: 49077, loss: \u001b[0;36m0.5261\u001b[0m, std: 0.6043\n",
      "2021-05-13 11:48:44,497 - INFO - Epoch/batch: 10/4228, ibatch: 49228, loss: \u001b[0;36m0.5468\u001b[0m, std: 0.6239\n",
      "2021-05-13 11:48:50,768 - INFO - Epoch/batch: 10/4379, ibatch: 49379, loss: \u001b[0;36m0.5306\u001b[0m, std: 0.6129\n",
      "2021-05-13 11:49:05,610 - INFO - loss: \u001b[0;32m0.5282\u001b[0m, std: 0.6229\n",
      "2021-05-13 11:49:06,749 - INFO - Epoch 10 average training loss: \u001b[0;46m0.5294\u001b[0m std: 0.6087\n",
      "2021-05-13 11:49:06,757 - INFO - Epoch 10 average validate loss: \u001b[0;46m0.5279\u001b[0m std: 0.6171\n",
      "2021-05-13 11:49:08,539 - INFO - Epoch/batch: 11/   0, ibatch: 49500, loss: \u001b[0;36m0.5360\u001b[0m, std: 0.6109\n",
      "2021-05-13 11:49:18,480 - INFO - loss: \u001b[0;32m0.5282\u001b[0m, std: 0.6226\n",
      "2021-05-13 11:49:24,494 - INFO - Epoch/batch: 11/ 151, ibatch: 49651, loss: \u001b[0;36m0.5320\u001b[0m, std: 0.6165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 332: ReduceOnPlateau set learning rate to 0.0002392993292306176.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:49:30,400 - INFO - Epoch/batch: 11/ 302, ibatch: 49802, loss: \u001b[0;36m0.5159\u001b[0m, std: 0.5955\n",
      "2021-05-13 11:49:46,670 - INFO - loss: \u001b[0;32m0.5288\u001b[0m, std: 0.6276\n",
      "2021-05-13 11:49:46,784 - INFO - Epoch/batch: 11/ 453, ibatch: 49953, loss: \u001b[0;36m0.5215\u001b[0m, std: 0.6003\n",
      "2021-05-13 11:49:52,871 - INFO - Epoch/batch: 11/ 604, ibatch: 50104, loss: \u001b[0;36m0.5241\u001b[0m, std: 0.6031\n",
      "2021-05-13 11:49:58,860 - INFO - Epoch/batch: 11/ 755, ibatch: 50255, loss: \u001b[0;36m0.5152\u001b[0m, std: 0.5911\n",
      "2021-05-13 11:50:14,604 - INFO - loss: \u001b[0;32m0.5282\u001b[0m, std: 0.6196\n",
      "2021-05-13 11:50:14,825 - INFO - Epoch/batch: 11/ 906, ibatch: 50406, loss: \u001b[0;36m0.5092\u001b[0m, std: 0.5953\n",
      "2021-05-13 11:50:21,113 - INFO - Epoch/batch: 11/1057, ibatch: 50557, loss: \u001b[0;36m0.5230\u001b[0m, std: 0.6075\n",
      "2021-05-13 11:50:27,683 - INFO - Epoch/batch: 11/1208, ibatch: 50708, loss: \u001b[0;36m0.5321\u001b[0m, std: 0.6097\n",
      "2021-05-13 11:50:43,230 - INFO - loss: \u001b[0;32m0.5274\u001b[0m, std: 0.5997\n",
      "2021-05-13 11:50:43,607 - INFO - Epoch/batch: 11/1359, ibatch: 50859, loss: \u001b[0;36m0.5327\u001b[0m, std: 0.6107\n",
      "2021-05-13 11:50:49,974 - INFO - Epoch/batch: 11/1510, ibatch: 51010, loss: \u001b[0;36m0.5357\u001b[0m, std: 0.6147\n",
      "2021-05-13 11:50:56,236 - INFO - Epoch/batch: 11/1661, ibatch: 51161, loss: \u001b[0;36m0.5339\u001b[0m, std: 0.6195\n",
      "2021-05-13 11:51:11,663 - INFO - loss: \u001b[0;32m0.5275\u001b[0m, std: 0.6163\n",
      "2021-05-13 11:51:12,129 - INFO - Epoch/batch: 11/1812, ibatch: 51312, loss: \u001b[0;36m0.5284\u001b[0m, std: 0.6095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 343: ReduceOnPlateau set learning rate to 0.00021536939630755584.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:51:18,404 - INFO - Epoch/batch: 11/1963, ibatch: 51463, loss: \u001b[0;36m0.5177\u001b[0m, std: 0.5964\n",
      "2021-05-13 11:51:24,671 - INFO - Epoch/batch: 11/2114, ibatch: 51614, loss: \u001b[0;36m0.5161\u001b[0m, std: 0.6001\n",
      "2021-05-13 11:51:40,470 - INFO - loss: \u001b[0;32m0.5275\u001b[0m, std: 0.6097\n",
      "2021-05-13 11:51:41,102 - INFO - Epoch/batch: 11/2265, ibatch: 51765, loss: \u001b[0;36m0.5248\u001b[0m, std: 0.6079\n",
      "2021-05-13 11:51:47,548 - INFO - Epoch/batch: 11/2416, ibatch: 51916, loss: \u001b[0;36m0.5395\u001b[0m, std: 0.6079\n",
      "2021-05-13 11:51:53,503 - INFO - Epoch/batch: 11/2567, ibatch: 52067, loss: \u001b[0;36m0.5178\u001b[0m, std: 0.6063\n",
      "2021-05-13 11:52:09,115 - INFO - loss: \u001b[0;32m0.5292\u001b[0m, std: 0.6274\n",
      "2021-05-13 11:52:09,861 - INFO - Epoch/batch: 11/2718, ibatch: 52218, loss: \u001b[0;36m0.5281\u001b[0m, std: 0.6064\n",
      "2021-05-13 11:52:16,135 - INFO - Epoch/batch: 11/2869, ibatch: 52369, loss: \u001b[0;36m0.5343\u001b[0m, std: 0.6168\n",
      "2021-05-13 11:52:22,042 - INFO - Epoch/batch: 11/3020, ibatch: 52520, loss: \u001b[0;36m0.5255\u001b[0m, std: 0.6075\n",
      "2021-05-13 11:52:37,511 - INFO - loss: \u001b[0;32m0.5274\u001b[0m, std: 0.6063\n",
      "2021-05-13 11:52:38,370 - INFO - Epoch/batch: 11/3171, ibatch: 52671, loss: \u001b[0;36m0.5176\u001b[0m, std: 0.5952\n",
      "2021-05-13 11:52:44,618 - INFO - Epoch/batch: 11/3322, ibatch: 52822, loss: \u001b[0;36m0.5433\u001b[0m, std: 0.6168\n",
      "2021-05-13 11:52:50,674 - INFO - Epoch/batch: 11/3473, ibatch: 52973, loss: \u001b[0;36m0.5278\u001b[0m, std: 0.6089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 354: ReduceOnPlateau set learning rate to 0.00019383245667680025.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:53:05,806 - INFO - loss: \u001b[0;32m0.5272\u001b[0m, std: 0.6033\n",
      "2021-05-13 11:53:05,827 - INFO - Saved model states in: earlystop_0.5272\n",
      "2021-05-13 11:53:05,829 - INFO - Saved net python code: earlystop_0.5272/paddle_nets.py\n",
      "2021-05-13 11:53:05,839 - INFO - Saved best model: earlystop_0.5272\n",
      "2021-05-13 11:53:05,839 - INFO - Removing earlystop model: earlystop_0.5273\n",
      "2021-05-13 11:53:06,867 - INFO - Epoch/batch: 11/3624, ibatch: 53124, loss: \u001b[0;36m0.5458\u001b[0m, std: 0.6286\n",
      "2021-05-13 11:53:12,804 - INFO - Epoch/batch: 11/3775, ibatch: 53275, loss: \u001b[0;36m0.5177\u001b[0m, std: 0.6034\n",
      "2021-05-13 11:53:18,923 - INFO - Epoch/batch: 11/3926, ibatch: 53426, loss: \u001b[0;36m0.5414\u001b[0m, std: 0.6077\n",
      "2021-05-13 11:53:33,855 - INFO - loss: \u001b[0;32m0.5273\u001b[0m, std: 0.6082\n",
      "2021-05-13 11:53:34,979 - INFO - Epoch/batch: 11/4077, ibatch: 53577, loss: \u001b[0;36m0.5346\u001b[0m, std: 0.6128\n",
      "2021-05-13 11:53:41,186 - INFO - Epoch/batch: 11/4228, ibatch: 53728, loss: \u001b[0;36m0.5430\u001b[0m, std: 0.6122\n",
      "2021-05-13 11:53:47,439 - INFO - Epoch/batch: 11/4379, ibatch: 53879, loss: \u001b[0;36m0.5533\u001b[0m, std: 0.6161\n",
      "2021-05-13 11:54:02,311 - INFO - loss: \u001b[0;32m0.5270\u001b[0m, std: 0.6103\n",
      "2021-05-13 11:54:02,329 - INFO - Saved model states in: earlystop_0.5270\n",
      "2021-05-13 11:54:02,331 - INFO - Saved net python code: earlystop_0.5270/paddle_nets.py\n",
      "2021-05-13 11:54:02,338 - INFO - Saved best model: earlystop_0.5270\n",
      "2021-05-13 11:54:02,339 - INFO - Removing earlystop model: earlystop_0.5272\n",
      "2021-05-13 11:54:03,455 - INFO - Epoch 11 average training loss: \u001b[0;46m0.5286\u001b[0m std: 0.6076\n",
      "2021-05-13 11:54:03,462 - INFO - Epoch 11 average validate loss: \u001b[0;46m0.5278\u001b[0m std: 0.6137\n",
      "2021-05-13 11:54:05,300 - INFO - Epoch/batch: 12/   0, ibatch: 54000, loss: \u001b[0;36m0.5251\u001b[0m, std: 0.6012\n",
      "2021-05-13 11:54:15,488 - INFO - loss: \u001b[0;32m0.5270\u001b[0m, std: 0.6101\n",
      "2021-05-13 11:54:15,512 - INFO - Saved model states in: earlystop_0.5270.1\n",
      "2021-05-13 11:54:15,514 - INFO - Saved net python code: earlystop_0.5270.1/paddle_nets.py\n",
      "2021-05-13 11:54:15,524 - INFO - Saved best model: earlystop_0.5270.1\n",
      "2021-05-13 11:54:15,525 - INFO - Removing earlystop model: earlystop_0.5270\n",
      "2021-05-13 11:54:22,114 - INFO - Epoch/batch: 12/ 151, ibatch: 54151, loss: \u001b[0;36m0.5463\u001b[0m, std: 0.6172\n",
      "2021-05-13 11:54:28,445 - INFO - Epoch/batch: 12/ 302, ibatch: 54302, loss: \u001b[0;36m0.5304\u001b[0m, std: 0.6015\n",
      "2021-05-13 11:54:44,310 - INFO - loss: \u001b[0;32m0.5279\u001b[0m, std: 0.6186\n",
      "2021-05-13 11:54:44,451 - INFO - Epoch/batch: 12/ 453, ibatch: 54453, loss: \u001b[0;36m0.5417\u001b[0m, std: 0.6188\n",
      "2021-05-13 11:54:50,527 - INFO - Epoch/batch: 12/ 604, ibatch: 54604, loss: \u001b[0;36m0.5155\u001b[0m, std: 0.6033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 365: ReduceOnPlateau set learning rate to 0.00017444921100912022.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:54:56,360 - INFO - Epoch/batch: 12/ 755, ibatch: 54755, loss: \u001b[0;36m0.5198\u001b[0m, std: 0.6011\n",
      "2021-05-13 11:55:12,321 - INFO - loss: \u001b[0;32m0.5268\u001b[0m, std: 0.6163\n",
      "2021-05-13 11:55:12,339 - INFO - Saved model states in: earlystop_0.5268\n",
      "2021-05-13 11:55:12,340 - INFO - Saved net python code: earlystop_0.5268/paddle_nets.py\n",
      "2021-05-13 11:55:12,348 - INFO - Saved best model: earlystop_0.5268\n",
      "2021-05-13 11:55:12,349 - INFO - Removing earlystop model: earlystop_0.5270.1\n",
      "2021-05-13 11:55:12,544 - INFO - Epoch/batch: 12/ 906, ibatch: 54906, loss: \u001b[0;36m0.5275\u001b[0m, std: 0.6025\n",
      "2021-05-13 11:55:18,703 - INFO - Epoch/batch: 12/1057, ibatch: 55057, loss: \u001b[0;36m0.5260\u001b[0m, std: 0.6000\n",
      "2021-05-13 11:55:24,307 - INFO - Epoch/batch: 12/1208, ibatch: 55208, loss: \u001b[0;36m0.5353\u001b[0m, std: 0.6050\n",
      "2021-05-13 11:55:39,667 - INFO - loss: \u001b[0;32m0.5276\u001b[0m, std: 0.6190\n",
      "2021-05-13 11:55:40,019 - INFO - Epoch/batch: 12/1359, ibatch: 55359, loss: \u001b[0;36m0.5220\u001b[0m, std: 0.5952\n",
      "2021-05-13 11:55:46,041 - INFO - Epoch/batch: 12/1510, ibatch: 55510, loss: \u001b[0;36m0.5387\u001b[0m, std: 0.6188\n",
      "2021-05-13 11:55:52,030 - INFO - Epoch/batch: 12/1661, ibatch: 55661, loss: \u001b[0;36m0.5149\u001b[0m, std: 0.5983\n",
      "2021-05-13 11:56:07,492 - INFO - loss: \u001b[0;32m0.5273\u001b[0m, std: 0.6174\n",
      "2021-05-13 11:56:07,982 - INFO - Epoch/batch: 12/1812, ibatch: 55812, loss: \u001b[0;36m0.5204\u001b[0m, std: 0.5976\n",
      "2021-05-13 11:56:14,347 - INFO - Epoch/batch: 12/1963, ibatch: 55963, loss: \u001b[0;36m0.5224\u001b[0m, std: 0.6126\n",
      "2021-05-13 11:56:20,685 - INFO - Epoch/batch: 12/2114, ibatch: 56114, loss: \u001b[0;36m0.5365\u001b[0m, std: 0.6147\n",
      "2021-05-13 11:56:36,726 - INFO - loss: \u001b[0;32m0.5267\u001b[0m, std: 0.6113\n",
      "2021-05-13 11:56:36,745 - INFO - Saved model states in: earlystop_0.5267\n",
      "2021-05-13 11:56:36,747 - INFO - Saved net python code: earlystop_0.5267/paddle_nets.py\n",
      "2021-05-13 11:56:36,755 - INFO - Saved best model: earlystop_0.5267\n",
      "2021-05-13 11:56:36,756 - INFO - Removing earlystop model: earlystop_0.5268\n",
      "2021-05-13 11:56:37,434 - INFO - Epoch/batch: 12/2265, ibatch: 56265, loss: \u001b[0;36m0.5312\u001b[0m, std: 0.6129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 376: ReduceOnPlateau set learning rate to 0.0001570042899082082.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:56:43,904 - INFO - Epoch/batch: 12/2416, ibatch: 56416, loss: \u001b[0;36m0.5297\u001b[0m, std: 0.6095\n",
      "2021-05-13 11:56:50,166 - INFO - Epoch/batch: 12/2567, ibatch: 56567, loss: \u001b[0;36m0.5262\u001b[0m, std: 0.6066\n",
      "2021-05-13 11:57:05,482 - INFO - loss: \u001b[0;32m0.5265\u001b[0m, std: 0.6096\n",
      "2021-05-13 11:57:05,500 - INFO - Saved model states in: earlystop_0.5265\n",
      "2021-05-13 11:57:05,502 - INFO - Saved net python code: earlystop_0.5265/paddle_nets.py\n",
      "2021-05-13 11:57:05,509 - INFO - Saved best model: earlystop_0.5265\n",
      "2021-05-13 11:57:05,510 - INFO - Removing earlystop model: earlystop_0.5267\n",
      "2021-05-13 11:57:06,253 - INFO - Epoch/batch: 12/2718, ibatch: 56718, loss: \u001b[0;36m0.5324\u001b[0m, std: 0.6187\n",
      "2021-05-13 11:57:12,537 - INFO - Epoch/batch: 12/2869, ibatch: 56869, loss: \u001b[0;36m0.5371\u001b[0m, std: 0.6172\n",
      "2021-05-13 11:57:18,509 - INFO - Epoch/batch: 12/3020, ibatch: 57020, loss: \u001b[0;36m0.5273\u001b[0m, std: 0.5988\n",
      "2021-05-13 11:57:33,927 - INFO - loss: \u001b[0;32m0.5269\u001b[0m, std: 0.5963\n",
      "2021-05-13 11:57:34,877 - INFO - Epoch/batch: 12/3171, ibatch: 57171, loss: \u001b[0;36m0.5445\u001b[0m, std: 0.6384\n",
      "2021-05-13 11:57:40,960 - INFO - Epoch/batch: 12/3322, ibatch: 57322, loss: \u001b[0;36m0.5153\u001b[0m, std: 0.5917\n",
      "2021-05-13 11:57:46,639 - INFO - Epoch/batch: 12/3473, ibatch: 57473, loss: \u001b[0;36m0.5136\u001b[0m, std: 0.5962\n",
      "2021-05-13 11:58:01,513 - INFO - loss: \u001b[0;32m0.5273\u001b[0m, std: 0.6153\n",
      "2021-05-13 11:58:02,455 - INFO - Epoch/batch: 12/3624, ibatch: 57624, loss: \u001b[0;36m0.5325\u001b[0m, std: 0.6098\n",
      "2021-05-13 11:58:08,599 - INFO - Epoch/batch: 12/3775, ibatch: 57775, loss: \u001b[0;36m0.5311\u001b[0m, std: 0.6101\n",
      "2021-05-13 11:58:14,570 - INFO - Epoch/batch: 12/3926, ibatch: 57926, loss: \u001b[0;36m0.5363\u001b[0m, std: 0.6090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 387: ReduceOnPlateau set learning rate to 0.0001413038609173874.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 11:58:29,587 - INFO - loss: \u001b[0;32m0.5266\u001b[0m, std: 0.6081\n",
      "2021-05-13 11:58:30,865 - INFO - Epoch/batch: 12/4077, ibatch: 58077, loss: \u001b[0;36m0.5413\u001b[0m, std: 0.6159\n",
      "2021-05-13 11:58:37,169 - INFO - Epoch/batch: 12/4228, ibatch: 58228, loss: \u001b[0;36m0.5187\u001b[0m, std: 0.5941\n",
      "2021-05-13 11:58:43,218 - INFO - Epoch/batch: 12/4379, ibatch: 58379, loss: \u001b[0;36m0.5123\u001b[0m, std: 0.6018\n",
      "2021-05-13 11:58:57,957 - INFO - loss: \u001b[0;32m0.5280\u001b[0m, std: 0.6244\n",
      "2021-05-13 11:58:59,165 - INFO - Epoch 12 average training loss: \u001b[0;46m0.5283\u001b[0m std: 0.6073\n",
      "2021-05-13 11:58:59,170 - INFO - Epoch 12 average validate loss: \u001b[0;46m0.5271\u001b[0m std: 0.6133\n",
      "2021-05-13 11:59:00,896 - INFO - Epoch/batch: 13/   0, ibatch: 58500, loss: \u001b[0;36m0.5191\u001b[0m, std: 0.6008\n",
      "2021-05-13 11:59:10,783 - INFO - loss: \u001b[0;32m0.5281\u001b[0m, std: 0.6248\n",
      "2021-05-13 11:59:17,067 - INFO - Epoch/batch: 13/ 151, ibatch: 58651, loss: \u001b[0;36m0.5426\u001b[0m, std: 0.6293\n",
      "2021-05-13 11:59:23,359 - INFO - Epoch/batch: 13/ 302, ibatch: 58802, loss: \u001b[0;36m0.5271\u001b[0m, std: 0.6083\n",
      "2021-05-13 11:59:39,418 - INFO - loss: \u001b[0;32m0.5266\u001b[0m, std: 0.6091\n",
      "2021-05-13 11:59:39,564 - INFO - Epoch/batch: 13/ 453, ibatch: 58953, loss: \u001b[0;36m0.5305\u001b[0m, std: 0.6110\n",
      "2021-05-13 11:59:45,378 - INFO - Epoch/batch: 13/ 604, ibatch: 59104, loss: \u001b[0;36m0.5391\u001b[0m, std: 0.6139\n",
      "2021-05-13 11:59:51,280 - INFO - Epoch/batch: 13/ 755, ibatch: 59255, loss: \u001b[0;36m0.5288\u001b[0m, std: 0.6037\n",
      "2021-05-13 12:00:06,523 - INFO - loss: \u001b[0;32m0.5283\u001b[0m, std: 0.6244\n",
      "2021-05-13 12:00:06,794 - INFO - Epoch/batch: 13/ 906, ibatch: 59406, loss: \u001b[0;36m0.4898\u001b[0m, std: 0.5748\n",
      "2021-05-13 12:00:13,607 - INFO - Epoch/batch: 13/1057, ibatch: 59557, loss: \u001b[0;36m0.5304\u001b[0m, std: 0.6133\n",
      "2021-05-13 12:00:20,102 - INFO - Epoch/batch: 13/1208, ibatch: 59708, loss: \u001b[0;36m0.5356\u001b[0m, std: 0.6150\n",
      "2021-05-13 12:00:35,975 - INFO - loss: \u001b[0;32m0.5270\u001b[0m, std: 0.6107\n",
      "2021-05-13 12:00:36,348 - INFO - Epoch/batch: 13/1359, ibatch: 59859, loss: \u001b[0;36m0.5216\u001b[0m, std: 0.6063\n",
      "2021-05-13 12:00:42,292 - INFO - Epoch/batch: 13/1510, ibatch: 60010, loss: \u001b[0;36m0.5178\u001b[0m, std: 0.5991\n",
      "2021-05-13 12:00:48,346 - INFO - Epoch/batch: 13/1661, ibatch: 60161, loss: \u001b[0;36m0.5116\u001b[0m, std: 0.5914\n",
      "2021-05-13 12:01:04,226 - INFO - loss: \u001b[0;32m0.5274\u001b[0m, std: 0.6185\n",
      "2021-05-13 12:01:04,706 - INFO - Epoch/batch: 13/1812, ibatch: 60312, loss: \u001b[0;36m0.5296\u001b[0m, std: 0.6013\n",
      "2021-05-13 12:01:11,028 - INFO - Epoch/batch: 13/1963, ibatch: 60463, loss: \u001b[0;36m0.5225\u001b[0m, std: 0.6141\n",
      "2021-05-13 12:01:17,211 - INFO - Epoch/batch: 13/2114, ibatch: 60614, loss: \u001b[0;36m0.5389\u001b[0m, std: 0.6087\n",
      "2021-05-13 12:01:32,935 - INFO - loss: \u001b[0;32m0.5279\u001b[0m, std: 0.6245\n",
      "2021-05-13 12:01:33,543 - INFO - Epoch/batch: 13/2265, ibatch: 60765, loss: \u001b[0;36m0.5144\u001b[0m, std: 0.6001\n",
      "2021-05-13 12:01:39,818 - INFO - Epoch/batch: 13/2416, ibatch: 60916, loss: \u001b[0;36m0.5263\u001b[0m, std: 0.6046\n",
      "2021-05-13 12:01:46,880 - INFO - Epoch/batch: 13/2567, ibatch: 61067, loss: \u001b[0;36m0.5197\u001b[0m, std: 0.6061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 408: ReduceOnPlateau set learning rate to 0.00012717347482564865.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 12:02:04,198 - INFO - loss: \u001b[0;32m0.5272\u001b[0m, std: 0.6139\n",
      "2021-05-13 12:02:04,984 - INFO - Epoch/batch: 13/2718, ibatch: 61218, loss: \u001b[0;36m0.5393\u001b[0m, std: 0.6207\n",
      "2021-05-13 12:02:11,555 - INFO - Epoch/batch: 13/2869, ibatch: 61369, loss: \u001b[0;36m0.5449\u001b[0m, std: 0.6204\n",
      "2021-05-13 12:02:17,590 - INFO - Epoch/batch: 13/3020, ibatch: 61520, loss: \u001b[0;36m0.5290\u001b[0m, std: 0.6145\n",
      "2021-05-13 12:02:33,612 - INFO - loss: \u001b[0;32m0.5275\u001b[0m, std: 0.6159\n",
      "2021-05-13 12:02:34,402 - INFO - Epoch/batch: 13/3171, ibatch: 61671, loss: \u001b[0;36m0.5225\u001b[0m, std: 0.5964\n",
      "2021-05-13 12:02:40,358 - INFO - Epoch/batch: 13/3322, ibatch: 61822, loss: \u001b[0;36m0.5256\u001b[0m, std: 0.6078\n",
      "2021-05-13 12:02:46,324 - INFO - Epoch/batch: 13/3473, ibatch: 61973, loss: \u001b[0;36m0.5371\u001b[0m, std: 0.6092\n",
      "2021-05-13 12:03:01,568 - INFO - loss: \u001b[0;32m0.5273\u001b[0m, std: 0.6155\n",
      "2021-05-13 12:03:02,616 - INFO - Epoch/batch: 13/3624, ibatch: 62124, loss: \u001b[0;36m0.5274\u001b[0m, std: 0.6054\n",
      "2021-05-13 12:03:08,821 - INFO - Epoch/batch: 13/3775, ibatch: 62275, loss: \u001b[0;36m0.5283\u001b[0m, std: 0.6031\n",
      "2021-05-13 12:03:14,938 - INFO - Epoch/batch: 13/3926, ibatch: 62426, loss: \u001b[0;36m0.5390\u001b[0m, std: 0.6159\n",
      "2021-05-13 12:03:29,843 - INFO - loss: \u001b[0;32m0.5270\u001b[0m, std: 0.6149\n",
      "2021-05-13 12:03:30,956 - INFO - Epoch/batch: 13/4077, ibatch: 62577, loss: \u001b[0;36m0.5268\u001b[0m, std: 0.6049\n",
      "2021-05-13 12:03:37,221 - INFO - Epoch/batch: 13/4228, ibatch: 62728, loss: \u001b[0;36m0.5313\u001b[0m, std: 0.6149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 419: ReduceOnPlateau set learning rate to 0.00011445612734308378.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 12:03:43,124 - INFO - Epoch/batch: 13/4379, ibatch: 62879, loss: \u001b[0;36m0.5330\u001b[0m, std: 0.6075\n",
      "2021-05-13 12:03:57,901 - INFO - loss: \u001b[0;32m0.5266\u001b[0m, std: 0.6080\n",
      "2021-05-13 12:03:58,950 - INFO - Epoch 13 average training loss: \u001b[0;46m0.5282\u001b[0m std: 0.6077\n",
      "2021-05-13 12:03:58,955 - INFO - Epoch 13 average validate loss: \u001b[0;46m0.5273\u001b[0m std: 0.6164\n",
      "2021-05-13 12:04:00,805 - INFO - Epoch/batch: 14/   0, ibatch: 63000, loss: \u001b[0;36m0.5374\u001b[0m, std: 0.6115\n",
      "2021-05-13 12:04:11,091 - INFO - loss: \u001b[0;32m0.5266\u001b[0m, std: 0.6083\n",
      "2021-05-13 12:04:17,528 - INFO - Epoch/batch: 14/ 151, ibatch: 63151, loss: \u001b[0;36m0.5306\u001b[0m, std: 0.6145\n",
      "2021-05-13 12:04:24,427 - INFO - Epoch/batch: 14/ 302, ibatch: 63302, loss: \u001b[0;36m0.5192\u001b[0m, std: 0.6012\n",
      "2021-05-13 12:04:42,115 - INFO - loss: \u001b[0;32m0.5270\u001b[0m, std: 0.6159\n",
      "2021-05-13 12:04:42,245 - INFO - Epoch/batch: 14/ 453, ibatch: 63453, loss: \u001b[0;36m0.5470\u001b[0m, std: 0.6146\n",
      "2021-05-13 12:04:47,885 - INFO - Epoch/batch: 14/ 604, ibatch: 63604, loss: \u001b[0;36m0.5182\u001b[0m, std: 0.5975\n",
      "2021-05-13 12:04:54,668 - INFO - Epoch/batch: 14/ 755, ibatch: 63755, loss: \u001b[0;36m0.5192\u001b[0m, std: 0.5987\n",
      "2021-05-13 12:05:10,414 - INFO - loss: \u001b[0;32m0.5266\u001b[0m, std: 0.6047\n",
      "2021-05-13 12:05:10,689 - INFO - Epoch/batch: 14/ 906, ibatch: 63906, loss: \u001b[0;36m0.5381\u001b[0m, std: 0.6217\n",
      "2021-05-13 12:05:16,563 - INFO - Epoch/batch: 14/1057, ibatch: 64057, loss: \u001b[0;36m0.5208\u001b[0m, std: 0.5988\n",
      "2021-05-13 12:05:22,317 - INFO - Epoch/batch: 14/1208, ibatch: 64208, loss: \u001b[0;36m0.5112\u001b[0m, std: 0.5939\n",
      "2021-05-13 12:05:38,205 - INFO - loss: \u001b[0;32m0.5272\u001b[0m, std: 0.6157\n",
      "2021-05-13 12:05:38,670 - INFO - Epoch/batch: 14/1359, ibatch: 64359, loss: \u001b[0;36m0.5388\u001b[0m, std: 0.6160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 430: ReduceOnPlateau set learning rate to 0.00010301051460877541.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 12:05:44,884 - INFO - Epoch/batch: 14/1510, ibatch: 64510, loss: \u001b[0;36m0.5412\u001b[0m, std: 0.6159\n",
      "2021-05-13 12:05:50,811 - INFO - Epoch/batch: 14/1661, ibatch: 64661, loss: \u001b[0;36m0.5237\u001b[0m, std: 0.5978\n",
      "2021-05-13 12:06:06,219 - INFO - loss: \u001b[0;32m0.5276\u001b[0m, std: 0.6183\n",
      "2021-05-13 12:06:06,705 - INFO - Epoch/batch: 14/1812, ibatch: 64812, loss: \u001b[0;36m0.5198\u001b[0m, std: 0.5996\n",
      "2021-05-13 12:06:12,560 - INFO - Epoch/batch: 14/1963, ibatch: 64963, loss: \u001b[0;36m0.5072\u001b[0m, std: 0.5924\n",
      "2021-05-13 12:06:18,245 - INFO - Epoch/batch: 14/2114, ibatch: 65114, loss: \u001b[0;36m0.5323\u001b[0m, std: 0.6056\n",
      "2021-05-13 12:06:34,208 - INFO - loss: \u001b[0;32m0.5268\u001b[0m, std: 0.6081\n",
      "2021-05-13 12:06:34,805 - INFO - Epoch/batch: 14/2265, ibatch: 65265, loss: \u001b[0;36m0.5342\u001b[0m, std: 0.6070\n",
      "2021-05-13 12:06:41,017 - INFO - Epoch/batch: 14/2416, ibatch: 65416, loss: \u001b[0;36m0.5411\u001b[0m, std: 0.6180\n",
      "2021-05-13 12:06:47,179 - INFO - Epoch/batch: 14/2567, ibatch: 65567, loss: \u001b[0;36m0.5330\u001b[0m, std: 0.6068\n",
      "2021-05-13 12:07:01,818 - INFO - loss: \u001b[0;32m0.5265\u001b[0m, std: 0.6120\n",
      "2021-05-13 12:07:02,589 - INFO - Epoch/batch: 14/2718, ibatch: 65718, loss: \u001b[0;36m0.5244\u001b[0m, std: 0.6018\n",
      "2021-05-13 12:07:09,119 - INFO - Epoch/batch: 14/2869, ibatch: 65869, loss: \u001b[0;36m0.5345\u001b[0m, std: 0.6086\n",
      "2021-05-13 12:07:15,296 - INFO - Epoch/batch: 14/3020, ibatch: 66020, loss: \u001b[0;36m0.5381\u001b[0m, std: 0.6233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 441: ReduceOnPlateau set learning rate to 9.270946314789788e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 12:07:30,500 - INFO - loss: \u001b[0;32m0.5268\u001b[0m, std: 0.6153\n",
      "2021-05-13 12:07:31,386 - INFO - Epoch/batch: 14/3171, ibatch: 66171, loss: \u001b[0;36m0.5180\u001b[0m, std: 0.5996\n",
      "2021-05-13 12:07:37,904 - INFO - Epoch/batch: 14/3322, ibatch: 66322, loss: \u001b[0;36m0.5358\u001b[0m, std: 0.6064\n",
      "2021-05-13 12:07:43,807 - INFO - Epoch/batch: 14/3473, ibatch: 66473, loss: \u001b[0;36m0.5432\u001b[0m, std: 0.6276\n",
      "2021-05-13 12:07:59,000 - INFO - loss: \u001b[0;32m0.5269\u001b[0m, std: 0.6114\n",
      "2021-05-13 12:07:59,919 - INFO - Epoch/batch: 14/3624, ibatch: 66624, loss: \u001b[0;36m0.5245\u001b[0m, std: 0.6023\n",
      "2021-05-13 12:08:06,113 - INFO - Epoch/batch: 14/3775, ibatch: 66775, loss: \u001b[0;36m0.4950\u001b[0m, std: 0.5821\n",
      "2021-05-13 12:08:12,583 - INFO - Epoch/batch: 14/3926, ibatch: 66926, loss: \u001b[0;36m0.5399\u001b[0m, std: 0.6233\n",
      "2021-05-13 12:08:27,698 - INFO - loss: \u001b[0;32m0.5272\u001b[0m, std: 0.6114\n",
      "2021-05-13 12:08:28,938 - INFO - Epoch/batch: 14/4077, ibatch: 67077, loss: \u001b[0;36m0.5265\u001b[0m, std: 0.6105\n",
      "2021-05-13 12:08:35,102 - INFO - Epoch/batch: 14/4228, ibatch: 67228, loss: \u001b[0;36m0.5286\u001b[0m, std: 0.6044\n",
      "2021-05-13 12:08:41,327 - INFO - Epoch/batch: 14/4379, ibatch: 67379, loss: \u001b[0;36m0.5385\u001b[0m, std: 0.6184\n",
      "2021-05-13 12:08:55,701 - INFO - loss: \u001b[0;32m0.5265\u001b[0m, std: 0.6113\n",
      "2021-05-13 12:08:55,722 - INFO - Saved model states in: earlystop_0.5265.1\n",
      "2021-05-13 12:08:55,724 - INFO - Saved net python code: earlystop_0.5265.1/paddle_nets.py\n",
      "2021-05-13 12:08:55,733 - INFO - Saved best model: earlystop_0.5265.1\n",
      "2021-05-13 12:08:55,733 - INFO - Removing earlystop model: earlystop_0.5265\n",
      "2021-05-13 12:08:56,754 - INFO - Epoch 14 average training loss: \u001b[0;46m0.5281\u001b[0m std: 0.6071\n",
      "2021-05-13 12:08:56,758 - INFO - Epoch 14 average validate loss: \u001b[0;46m0.5269\u001b[0m std: 0.6120\n",
      "2021-05-13 12:08:58,563 - INFO - Epoch/batch: 15/   0, ibatch: 67500, loss: \u001b[0;36m0.5162\u001b[0m, std: 0.6006\n",
      "2021-05-13 12:09:08,875 - INFO - loss: \u001b[0;32m0.5265\u001b[0m, std: 0.6112\n",
      "2021-05-13 12:09:08,898 - INFO - Saved model states in: earlystop_0.5265\n",
      "2021-05-13 12:09:08,900 - INFO - Saved net python code: earlystop_0.5265/paddle_nets.py\n",
      "2021-05-13 12:09:08,909 - INFO - Saved best model: earlystop_0.5265\n",
      "2021-05-13 12:09:08,910 - INFO - Removing earlystop model: earlystop_0.5265.1\n",
      "2021-05-13 12:09:15,076 - INFO - Epoch/batch: 15/ 151, ibatch: 67651, loss: \u001b[0;36m0.5227\u001b[0m, std: 0.5981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 452: ReduceOnPlateau set learning rate to 8.343851683310809e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 12:09:21,155 - INFO - Epoch/batch: 15/ 302, ibatch: 67802, loss: \u001b[0;36m0.5174\u001b[0m, std: 0.6015\n",
      "2021-05-13 12:09:37,010 - INFO - loss: \u001b[0;32m0.5273\u001b[0m, std: 0.6177\n",
      "2021-05-13 12:09:37,124 - INFO - Epoch/batch: 15/ 453, ibatch: 67953, loss: \u001b[0;36m0.5179\u001b[0m, std: 0.5923\n",
      "2021-05-13 12:09:43,075 - INFO - Epoch/batch: 15/ 604, ibatch: 68104, loss: \u001b[0;36m0.5408\u001b[0m, std: 0.6145\n",
      "2021-05-13 12:09:49,337 - INFO - Epoch/batch: 15/ 755, ibatch: 68255, loss: \u001b[0;36m0.5324\u001b[0m, std: 0.6048\n",
      "2021-05-13 12:10:05,863 - INFO - loss: \u001b[0;32m0.5267\u001b[0m, std: 0.6006\n",
      "2021-05-13 12:10:06,110 - INFO - Epoch/batch: 15/ 906, ibatch: 68406, loss: \u001b[0;36m0.5455\u001b[0m, std: 0.6161\n",
      "2021-05-13 12:10:12,023 - INFO - Epoch/batch: 15/1057, ibatch: 68557, loss: \u001b[0;36m0.5156\u001b[0m, std: 0.5900\n",
      "2021-05-13 12:10:18,313 - INFO - Epoch/batch: 15/1208, ibatch: 68708, loss: \u001b[0;36m0.5474\u001b[0m, std: 0.6230\n",
      "2021-05-13 12:10:34,002 - INFO - loss: \u001b[0;32m0.5266\u001b[0m, std: 0.6093\n",
      "2021-05-13 12:10:34,377 - INFO - Epoch/batch: 15/1359, ibatch: 68859, loss: \u001b[0;36m0.5320\u001b[0m, std: 0.6086\n",
      "2021-05-13 12:10:40,847 - INFO - Epoch/batch: 15/1510, ibatch: 69010, loss: \u001b[0;36m0.5246\u001b[0m, std: 0.6065\n",
      "2021-05-13 12:10:47,006 - INFO - Epoch/batch: 15/1661, ibatch: 69161, loss: \u001b[0;36m0.5148\u001b[0m, std: 0.5922\n",
      "2021-05-13 12:11:02,430 - INFO - loss: \u001b[0;32m0.5266\u001b[0m, std: 0.6081\n",
      "2021-05-13 12:11:02,978 - INFO - Epoch/batch: 15/1812, ibatch: 69312, loss: \u001b[0;36m0.5286\u001b[0m, std: 0.6116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 463: ReduceOnPlateau set learning rate to 7.509466514979728e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 12:11:09,125 - INFO - Epoch/batch: 15/1963, ibatch: 69463, loss: \u001b[0;36m0.5200\u001b[0m, std: 0.5965\n",
      "2021-05-13 12:11:15,737 - INFO - Epoch/batch: 15/2114, ibatch: 69614, loss: \u001b[0;36m0.5309\u001b[0m, std: 0.6096\n",
      "2021-05-13 12:11:31,889 - INFO - loss: \u001b[0;32m0.5268\u001b[0m, std: 0.6156\n",
      "2021-05-13 12:11:32,457 - INFO - Epoch/batch: 15/2265, ibatch: 69765, loss: \u001b[0;36m0.5309\u001b[0m, std: 0.6141\n",
      "2021-05-13 12:11:38,630 - INFO - Epoch/batch: 15/2416, ibatch: 69916, loss: \u001b[0;36m0.5261\u001b[0m, std: 0.6056\n",
      "2021-05-13 12:11:44,699 - INFO - Epoch/batch: 15/2567, ibatch: 70067, loss: \u001b[0;36m0.5218\u001b[0m, std: 0.6021\n",
      "2021-05-13 12:11:59,726 - INFO - loss: \u001b[0;32m0.5268\u001b[0m, std: 0.6135\n",
      "2021-05-13 12:12:00,582 - INFO - Epoch/batch: 15/2718, ibatch: 70218, loss: \u001b[0;36m0.5306\u001b[0m, std: 0.6165\n",
      "2021-05-13 12:12:06,775 - INFO - Epoch/batch: 15/2869, ibatch: 70369, loss: \u001b[0;36m0.5340\u001b[0m, std: 0.6135\n",
      "2021-05-13 12:12:13,029 - INFO - Epoch/batch: 15/3020, ibatch: 70520, loss: \u001b[0;36m0.5340\u001b[0m, std: 0.6185\n",
      "2021-05-13 12:12:28,302 - INFO - loss: \u001b[0;32m0.5269\u001b[0m, std: 0.6175\n",
      "2021-05-13 12:12:29,186 - INFO - Epoch/batch: 15/3171, ibatch: 70671, loss: \u001b[0;36m0.5254\u001b[0m, std: 0.5961\n",
      "2021-05-13 12:12:35,331 - INFO - Epoch/batch: 15/3322, ibatch: 70822, loss: \u001b[0;36m0.5385\u001b[0m, std: 0.6217\n",
      "2021-05-13 12:12:41,667 - INFO - Epoch/batch: 15/3473, ibatch: 70973, loss: \u001b[0;36m0.5273\u001b[0m, std: 0.6134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 474: ReduceOnPlateau set learning rate to 6.758519863481756e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 12:12:57,414 - INFO - loss: \u001b[0;32m0.5272\u001b[0m, std: 0.6177\n",
      "2021-05-13 12:12:58,404 - INFO - Epoch/batch: 15/3624, ibatch: 71124, loss: \u001b[0;36m0.5188\u001b[0m, std: 0.6026\n",
      "2021-05-13 12:13:04,570 - INFO - Epoch/batch: 15/3775, ibatch: 71275, loss: \u001b[0;36m0.5355\u001b[0m, std: 0.6118\n",
      "2021-05-13 12:13:10,624 - INFO - Epoch/batch: 15/3926, ibatch: 71426, loss: \u001b[0;36m0.5322\u001b[0m, std: 0.6063\n",
      "2021-05-13 12:13:25,722 - INFO - loss: \u001b[0;32m0.5272\u001b[0m, std: 0.6166\n",
      "2021-05-13 12:13:26,894 - INFO - Epoch/batch: 15/4077, ibatch: 71577, loss: \u001b[0;36m0.5095\u001b[0m, std: 0.5894\n",
      "2021-05-13 12:13:33,034 - INFO - Epoch/batch: 15/4228, ibatch: 71728, loss: \u001b[0;36m0.5118\u001b[0m, std: 0.5973\n",
      "2021-05-13 12:13:39,486 - INFO - Epoch/batch: 15/4379, ibatch: 71879, loss: \u001b[0;36m0.5391\u001b[0m, std: 0.6221\n",
      "2021-05-13 12:13:54,165 - INFO - loss: \u001b[0;32m0.5270\u001b[0m, std: 0.6123\n",
      "2021-05-13 12:13:55,274 - INFO - Epoch 15 average training loss: \u001b[0;46m0.5278\u001b[0m std: 0.6068\n",
      "2021-05-13 12:13:55,278 - INFO - Epoch 15 average validate loss: \u001b[0;46m0.5269\u001b[0m std: 0.6127\n",
      "2021-05-13 12:13:57,002 - INFO - Epoch/batch: 16/   0, ibatch: 72000, loss: \u001b[0;36m0.5250\u001b[0m, std: 0.6080\n",
      "2021-05-13 12:14:07,069 - INFO - loss: \u001b[0;32m0.5270\u001b[0m, std: 0.6124\n",
      "2021-05-13 12:14:13,360 - INFO - Epoch/batch: 16/ 151, ibatch: 72151, loss: \u001b[0;36m0.5441\u001b[0m, std: 0.6206\n",
      "2021-05-13 12:14:19,058 - INFO - Epoch/batch: 16/ 302, ibatch: 72302, loss: \u001b[0;36m0.5160\u001b[0m, std: 0.5974\n",
      "2021-05-13 12:14:35,263 - INFO - loss: \u001b[0;32m0.5268\u001b[0m, std: 0.6125\n",
      "2021-05-13 12:14:35,394 - INFO - Epoch/batch: 16/ 453, ibatch: 72453, loss: \u001b[0;36m0.5303\u001b[0m, std: 0.6019\n",
      "2021-05-13 12:14:41,458 - INFO - Epoch/batch: 16/ 604, ibatch: 72604, loss: \u001b[0;36m0.4956\u001b[0m, std: 0.5808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 485: ReduceOnPlateau set learning rate to 6.0826678771335806e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 12:14:47,610 - INFO - Epoch/batch: 16/ 755, ibatch: 72755, loss: \u001b[0;36m0.5327\u001b[0m, std: 0.6172\n",
      "2021-05-13 12:15:03,384 - INFO - loss: \u001b[0;32m0.5264\u001b[0m, std: 0.6102\n",
      "2021-05-13 12:15:03,404 - INFO - Saved model states in: earlystop_0.5264\n",
      "2021-05-13 12:15:03,405 - INFO - Saved net python code: earlystop_0.5264/paddle_nets.py\n",
      "2021-05-13 12:15:03,415 - INFO - Saved best model: earlystop_0.5264\n",
      "2021-05-13 12:15:03,416 - INFO - Removing earlystop model: earlystop_0.5265\n",
      "2021-05-13 12:15:03,666 - INFO - Epoch/batch: 16/ 906, ibatch: 72906, loss: \u001b[0;36m0.5323\u001b[0m, std: 0.6194\n",
      "2021-05-13 12:15:10,121 - INFO - Epoch/batch: 16/1057, ibatch: 73057, loss: \u001b[0;36m0.5324\u001b[0m, std: 0.6079\n",
      "2021-05-13 12:15:16,444 - INFO - Epoch/batch: 16/1208, ibatch: 73208, loss: \u001b[0;36m0.5343\u001b[0m, std: 0.6094\n",
      "2021-05-13 12:15:32,816 - INFO - loss: \u001b[0;32m0.5272\u001b[0m, std: 0.6179\n",
      "2021-05-13 12:15:33,247 - INFO - Epoch/batch: 16/1359, ibatch: 73359, loss: \u001b[0;36m0.5284\u001b[0m, std: 0.5978\n",
      "2021-05-13 12:15:39,392 - INFO - Epoch/batch: 16/1510, ibatch: 73510, loss: \u001b[0;36m0.5295\u001b[0m, std: 0.6076\n",
      "2021-05-13 12:15:45,873 - INFO - Epoch/batch: 16/1661, ibatch: 73661, loss: \u001b[0;36m0.5372\u001b[0m, std: 0.6215\n",
      "2021-05-13 12:16:01,177 - INFO - loss: \u001b[0;32m0.5267\u001b[0m, std: 0.6116\n",
      "2021-05-13 12:16:01,710 - INFO - Epoch/batch: 16/1812, ibatch: 73812, loss: \u001b[0;36m0.5253\u001b[0m, std: 0.6039\n",
      "2021-05-13 12:16:07,971 - INFO - Epoch/batch: 16/1963, ibatch: 73963, loss: \u001b[0;36m0.5509\u001b[0m, std: 0.6245\n",
      "2021-05-13 12:16:14,240 - INFO - Epoch/batch: 16/2114, ibatch: 74114, loss: \u001b[0;36m0.5366\u001b[0m, std: 0.6042\n",
      "2021-05-13 12:16:29,720 - INFO - loss: \u001b[0;32m0.5269\u001b[0m, std: 0.6120\n",
      "2021-05-13 12:16:30,344 - INFO - Epoch/batch: 16/2265, ibatch: 74265, loss: \u001b[0;36m0.5225\u001b[0m, std: 0.6137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 496: ReduceOnPlateau set learning rate to 5.4744010894202224e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 12:16:36,682 - INFO - Epoch/batch: 16/2416, ibatch: 74416, loss: \u001b[0;36m0.5329\u001b[0m, std: 0.6073\n",
      "2021-05-13 12:16:42,669 - INFO - Epoch/batch: 16/2567, ibatch: 74567, loss: \u001b[0;36m0.5104\u001b[0m, std: 0.5913\n",
      "2021-05-13 12:16:57,783 - INFO - loss: \u001b[0;32m0.5273\u001b[0m, std: 0.6189\n",
      "2021-05-13 12:16:58,464 - INFO - Epoch/batch: 16/2718, ibatch: 74718, loss: \u001b[0;36m0.5124\u001b[0m, std: 0.5946\n",
      "2021-05-13 12:17:04,470 - INFO - Epoch/batch: 16/2869, ibatch: 74869, loss: \u001b[0;36m0.5195\u001b[0m, std: 0.6108\n",
      "2021-05-13 12:17:10,648 - INFO - Epoch/batch: 16/3020, ibatch: 75020, loss: \u001b[0;36m0.5395\u001b[0m, std: 0.6178\n",
      "2021-05-13 12:17:25,893 - INFO - loss: \u001b[0;32m0.5265\u001b[0m, std: 0.6107\n",
      "2021-05-13 12:17:26,826 - INFO - Epoch/batch: 16/3171, ibatch: 75171, loss: \u001b[0;36m0.5350\u001b[0m, std: 0.6154\n",
      "2021-05-13 12:17:32,704 - INFO - Epoch/batch: 16/3322, ibatch: 75322, loss: \u001b[0;36m0.5282\u001b[0m, std: 0.6066\n",
      "2021-05-13 12:17:38,780 - INFO - Epoch/batch: 16/3473, ibatch: 75473, loss: \u001b[0;36m0.5279\u001b[0m, std: 0.6065\n",
      "2021-05-13 12:17:53,905 - INFO - loss: \u001b[0;32m0.5266\u001b[0m, std: 0.6088\n",
      "2021-05-13 12:17:55,041 - INFO - Epoch/batch: 16/3624, ibatch: 75624, loss: \u001b[0;36m0.5320\u001b[0m, std: 0.6051\n",
      "2021-05-13 12:18:01,009 - INFO - Epoch/batch: 16/3775, ibatch: 75775, loss: \u001b[0;36m0.5265\u001b[0m, std: 0.6056\n",
      "2021-05-13 12:18:07,144 - INFO - Epoch/batch: 16/3926, ibatch: 75926, loss: \u001b[0;36m0.5099\u001b[0m, std: 0.5925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 507: ReduceOnPlateau set learning rate to 4.9269609804782e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 12:18:22,421 - INFO - loss: \u001b[0;32m0.5266\u001b[0m, std: 0.6096\n",
      "2021-05-13 12:18:23,644 - INFO - Epoch/batch: 16/4077, ibatch: 76077, loss: \u001b[0;36m0.5278\u001b[0m, std: 0.6093\n",
      "2021-05-13 12:18:29,749 - INFO - Epoch/batch: 16/4228, ibatch: 76228, loss: \u001b[0;36m0.5316\u001b[0m, std: 0.6083\n",
      "2021-05-13 12:18:35,870 - INFO - Epoch/batch: 16/4379, ibatch: 76379, loss: \u001b[0;36m0.5133\u001b[0m, std: 0.6013\n",
      "2021-05-13 12:18:50,355 - INFO - loss: \u001b[0;32m0.5267\u001b[0m, std: 0.6126\n",
      "2021-05-13 12:18:51,450 - INFO - Epoch 16 average training loss: \u001b[0;46m0.5275\u001b[0m std: 0.6069\n",
      "2021-05-13 12:18:51,455 - INFO - Epoch 16 average validate loss: \u001b[0;46m0.5268\u001b[0m std: 0.6125\n",
      "2021-05-13 12:18:53,241 - INFO - Epoch/batch: 17/   0, ibatch: 76500, loss: \u001b[0;36m0.5348\u001b[0m, std: 0.6073\n",
      "2021-05-13 12:19:03,452 - INFO - loss: \u001b[0;32m0.5267\u001b[0m, std: 0.6126\n",
      "2021-05-13 12:19:09,656 - INFO - Epoch/batch: 17/ 151, ibatch: 76651, loss: \u001b[0;36m0.5412\u001b[0m, std: 0.6231\n",
      "2021-05-13 12:19:15,707 - INFO - Epoch/batch: 17/ 302, ibatch: 76802, loss: \u001b[0;36m0.5276\u001b[0m, std: 0.6002\n",
      "2021-05-13 12:19:31,625 - INFO - loss: \u001b[0;32m0.5272\u001b[0m, std: 0.6153\n",
      "2021-05-13 12:19:31,811 - INFO - Epoch/batch: 17/ 453, ibatch: 76953, loss: \u001b[0;36m0.5260\u001b[0m, std: 0.6113\n",
      "2021-05-13 12:19:37,938 - INFO - Epoch/batch: 17/ 604, ibatch: 77104, loss: \u001b[0;36m0.5374\u001b[0m, std: 0.6160\n",
      "2021-05-13 12:19:43,788 - INFO - Epoch/batch: 17/ 755, ibatch: 77255, loss: \u001b[0;36m0.5236\u001b[0m, std: 0.5984\n",
      "2021-05-13 12:19:58,953 - INFO - loss: \u001b[0;32m0.5268\u001b[0m, std: 0.6104\n",
      "2021-05-13 12:19:59,191 - INFO - Epoch/batch: 17/ 906, ibatch: 77406, loss: \u001b[0;36m0.5203\u001b[0m, std: 0.6029\n",
      "2021-05-13 12:20:05,029 - INFO - Epoch/batch: 17/1057, ibatch: 77557, loss: \u001b[0;36m0.5163\u001b[0m, std: 0.5986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 518: ReduceOnPlateau set learning rate to 4.43426488243038e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 12:20:11,178 - INFO - Epoch/batch: 17/1208, ibatch: 77708, loss: \u001b[0;36m0.5272\u001b[0m, std: 0.6045\n",
      "2021-05-13 12:20:27,155 - INFO - loss: \u001b[0;32m0.5273\u001b[0m, std: 0.6138\n",
      "2021-05-13 12:20:27,536 - INFO - Epoch/batch: 17/1359, ibatch: 77859, loss: \u001b[0;36m0.5256\u001b[0m, std: 0.6047\n",
      "2021-05-13 12:20:33,445 - INFO - Epoch/batch: 17/1510, ibatch: 78010, loss: \u001b[0;36m0.5164\u001b[0m, std: 0.5847\n",
      "2021-05-13 12:20:39,529 - INFO - Epoch/batch: 17/1661, ibatch: 78161, loss: \u001b[0;36m0.5238\u001b[0m, std: 0.6095\n",
      "2021-05-13 12:20:54,720 - INFO - loss: \u001b[0;32m0.5268\u001b[0m, std: 0.6100\n",
      "2021-05-13 12:20:55,312 - INFO - Epoch/batch: 17/1812, ibatch: 78312, loss: \u001b[0;36m0.5321\u001b[0m, std: 0.6153\n",
      "2021-05-13 12:21:01,833 - INFO - Epoch/batch: 17/1963, ibatch: 78463, loss: \u001b[0;36m0.5334\u001b[0m, std: 0.6141\n",
      "2021-05-13 12:21:08,242 - INFO - Epoch/batch: 17/2114, ibatch: 78614, loss: \u001b[0;36m0.5393\u001b[0m, std: 0.6178\n",
      "2021-05-13 12:21:23,633 - INFO - loss: \u001b[0;32m0.5267\u001b[0m, std: 0.6092\n",
      "2021-05-13 12:21:24,271 - INFO - Epoch/batch: 17/2265, ibatch: 78765, loss: \u001b[0;36m0.5294\u001b[0m, std: 0.6078\n",
      "2021-05-13 12:21:30,446 - INFO - Epoch/batch: 17/2416, ibatch: 78916, loss: \u001b[0;36m0.5193\u001b[0m, std: 0.5990\n",
      "2021-05-13 12:21:36,463 - INFO - Epoch/batch: 17/2567, ibatch: 79067, loss: \u001b[0;36m0.5268\u001b[0m, std: 0.6017\n",
      "2021-05-13 12:21:51,727 - INFO - loss: \u001b[0;32m0.5272\u001b[0m, std: 0.6165\n",
      "2021-05-13 12:21:52,527 - INFO - Epoch/batch: 17/2718, ibatch: 79218, loss: \u001b[0;36m0.5190\u001b[0m, std: 0.5990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 529: ReduceOnPlateau set learning rate to 3.990838394187342e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 12:21:58,797 - INFO - Epoch/batch: 17/2869, ibatch: 79369, loss: \u001b[0;36m0.5292\u001b[0m, std: 0.6077\n",
      "2021-05-13 12:22:04,841 - INFO - Epoch/batch: 17/3020, ibatch: 79520, loss: \u001b[0;36m0.5394\u001b[0m, std: 0.6178\n",
      "2021-05-13 12:22:20,122 - INFO - loss: \u001b[0;32m0.5268\u001b[0m, std: 0.6144\n",
      "2021-05-13 12:22:21,014 - INFO - Epoch/batch: 17/3171, ibatch: 79671, loss: \u001b[0;36m0.5252\u001b[0m, std: 0.5975\n",
      "2021-05-13 12:22:28,070 - INFO - Epoch/batch: 17/3322, ibatch: 79822, loss: \u001b[0;36m0.5443\u001b[0m, std: 0.6171\n",
      "2021-05-13 12:22:34,635 - INFO - Epoch/batch: 17/3473, ibatch: 79973, loss: \u001b[0;36m0.5155\u001b[0m, std: 0.5952\n",
      "2021-05-13 12:22:49,871 - INFO - loss: \u001b[0;32m0.5266\u001b[0m, std: 0.6102\n",
      "2021-05-13 12:22:50,738 - INFO - Epoch/batch: 17/3624, ibatch: 80124, loss: \u001b[0;36m0.5310\u001b[0m, std: 0.6110\n",
      "2021-05-13 12:22:56,991 - INFO - Epoch/batch: 17/3775, ibatch: 80275, loss: \u001b[0;36m0.5163\u001b[0m, std: 0.5962\n",
      "2021-05-13 12:23:03,891 - INFO - Epoch/batch: 17/3926, ibatch: 80426, loss: \u001b[0;36m0.5108\u001b[0m, std: 0.5880\n",
      "2021-05-13 12:23:18,739 - INFO - loss: \u001b[0;32m0.5264\u001b[0m, std: 0.6065\n",
      "2021-05-13 12:23:19,923 - INFO - Epoch/batch: 17/4077, ibatch: 80577, loss: \u001b[0;36m0.5492\u001b[0m, std: 0.6279\n",
      "2021-05-13 12:23:25,693 - INFO - Epoch/batch: 17/4228, ibatch: 80728, loss: \u001b[0;36m0.5303\u001b[0m, std: 0.6100\n",
      "2021-05-13 12:23:31,502 - INFO - Epoch/batch: 17/4379, ibatch: 80879, loss: \u001b[0;36m0.5164\u001b[0m, std: 0.5959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 540: ReduceOnPlateau set learning rate to 3.591754554768608e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 12:23:46,101 - INFO - loss: \u001b[0;32m0.5269\u001b[0m, std: 0.6148\n",
      "2021-05-13 12:23:47,208 - INFO - Epoch 17 average training loss: \u001b[0;46m0.5273\u001b[0m std: 0.6061\n",
      "2021-05-13 12:23:47,255 - INFO - Epoch 17 average validate loss: \u001b[0;46m0.5269\u001b[0m std: 0.6122\n",
      "2021-05-13 12:23:49,130 - INFO - Epoch/batch: 18/   0, ibatch: 81000, loss: \u001b[0;36m0.5255\u001b[0m, std: 0.6096\n",
      "2021-05-13 12:23:59,574 - INFO - loss: \u001b[0;32m0.5269\u001b[0m, std: 0.6147\n",
      "2021-05-13 12:24:05,992 - INFO - Epoch/batch: 18/ 151, ibatch: 81151, loss: \u001b[0;36m0.5405\u001b[0m, std: 0.6207\n",
      "2021-05-13 12:24:12,096 - INFO - Epoch/batch: 18/ 302, ibatch: 81302, loss: \u001b[0;36m0.5284\u001b[0m, std: 0.6090\n",
      "2021-05-13 12:24:28,194 - INFO - loss: \u001b[0;32m0.5265\u001b[0m, std: 0.6094\n",
      "2021-05-13 12:24:28,297 - INFO - Epoch/batch: 18/ 453, ibatch: 81453, loss: \u001b[0;36m0.5179\u001b[0m, std: 0.5999\n",
      "2021-05-13 12:24:34,186 - INFO - Epoch/batch: 18/ 604, ibatch: 81604, loss: \u001b[0;36m0.5056\u001b[0m, std: 0.5789\n",
      "2021-05-13 12:24:40,281 - INFO - Epoch/batch: 18/ 755, ibatch: 81755, loss: \u001b[0;36m0.5209\u001b[0m, std: 0.6025\n",
      "2021-05-13 12:24:55,663 - INFO - loss: \u001b[0;32m0.5268\u001b[0m, std: 0.6152\n",
      "2021-05-13 12:24:55,973 - INFO - Epoch/batch: 18/ 906, ibatch: 81906, loss: \u001b[0;36m0.5111\u001b[0m, std: 0.5935\n",
      "2021-05-13 12:25:02,281 - INFO - Epoch/batch: 18/1057, ibatch: 82057, loss: \u001b[0;36m0.5222\u001b[0m, std: 0.6037\n",
      "2021-05-13 12:25:08,441 - INFO - Epoch/batch: 18/1208, ibatch: 82208, loss: \u001b[0;36m0.5213\u001b[0m, std: 0.6009\n",
      "2021-05-13 12:25:24,463 - INFO - loss: \u001b[0;32m0.5274\u001b[0m, std: 0.6203\n",
      "2021-05-13 12:25:24,866 - INFO - Epoch/batch: 18/1359, ibatch: 82359, loss: \u001b[0;36m0.5188\u001b[0m, std: 0.5931\n",
      "2021-05-13 12:25:30,860 - INFO - Epoch/batch: 18/1510, ibatch: 82510, loss: \u001b[0;36m0.5193\u001b[0m, std: 0.6032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 551: ReduceOnPlateau set learning rate to 3.232579099291747e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 12:25:36,732 - INFO - Epoch/batch: 18/1661, ibatch: 82661, loss: \u001b[0;36m0.5312\u001b[0m, std: 0.6174\n",
      "2021-05-13 12:25:51,722 - INFO - loss: \u001b[0;32m0.5270\u001b[0m, std: 0.6171\n",
      "2021-05-13 12:25:52,264 - INFO - Epoch/batch: 18/1812, ibatch: 82812, loss: \u001b[0;36m0.5102\u001b[0m, std: 0.5861\n",
      "2021-05-13 12:25:58,559 - INFO - Epoch/batch: 18/1963, ibatch: 82963, loss: \u001b[0;36m0.5394\u001b[0m, std: 0.6177\n",
      "2021-05-13 12:26:04,922 - INFO - Epoch/batch: 18/2114, ibatch: 83114, loss: \u001b[0;36m0.5352\u001b[0m, std: 0.6130\n",
      "2021-05-13 12:26:21,222 - INFO - loss: \u001b[0;32m0.5266\u001b[0m, std: 0.6112\n",
      "2021-05-13 12:26:21,952 - INFO - Epoch/batch: 18/2265, ibatch: 83265, loss: \u001b[0;36m0.5347\u001b[0m, std: 0.6119\n",
      "2021-05-13 12:26:28,627 - INFO - Epoch/batch: 18/2416, ibatch: 83416, loss: \u001b[0;36m0.5216\u001b[0m, std: 0.5965\n",
      "2021-05-13 12:26:35,214 - INFO - Epoch/batch: 18/2567, ibatch: 83567, loss: \u001b[0;36m0.5506\u001b[0m, std: 0.6319\n",
      "2021-05-13 12:26:50,215 - INFO - loss: \u001b[0;32m0.5265\u001b[0m, std: 0.6105\n",
      "2021-05-13 12:26:51,128 - INFO - Epoch/batch: 18/2718, ibatch: 83718, loss: \u001b[0;36m0.5427\u001b[0m, std: 0.6207\n",
      "2021-05-13 12:26:58,341 - INFO - Epoch/batch: 18/2869, ibatch: 83869, loss: \u001b[0;36m0.5226\u001b[0m, std: 0.5988\n",
      "2021-05-13 12:27:05,187 - INFO - Epoch/batch: 18/3020, ibatch: 84020, loss: \u001b[0;36m0.5424\u001b[0m, std: 0.6222\n",
      "2021-05-13 12:27:21,019 - INFO - loss: \u001b[0;32m0.5266\u001b[0m, std: 0.6119\n",
      "2021-05-13 12:27:21,780 - INFO - Epoch/batch: 18/3171, ibatch: 84171, loss: \u001b[0;36m0.5344\u001b[0m, std: 0.6079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 562: ReduceOnPlateau set learning rate to 2.9093211893625727e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 12:27:27,872 - INFO - Epoch/batch: 18/3322, ibatch: 84322, loss: \u001b[0;36m0.5324\u001b[0m, std: 0.6140\n",
      "2021-05-13 12:27:34,304 - INFO - Epoch/batch: 18/3473, ibatch: 84473, loss: \u001b[0;36m0.5337\u001b[0m, std: 0.6124\n",
      "2021-05-13 12:27:49,303 - INFO - loss: \u001b[0;32m0.5266\u001b[0m, std: 0.6103\n",
      "2021-05-13 12:27:50,337 - INFO - Epoch/batch: 18/3624, ibatch: 84624, loss: \u001b[0;36m0.5288\u001b[0m, std: 0.6045\n",
      "2021-05-13 12:27:56,604 - INFO - Epoch/batch: 18/3775, ibatch: 84775, loss: \u001b[0;36m0.5180\u001b[0m, std: 0.5981\n",
      "2021-05-13 12:28:02,411 - INFO - Epoch/batch: 18/3926, ibatch: 84926, loss: \u001b[0;36m0.5175\u001b[0m, std: 0.6011\n",
      "2021-05-13 12:28:17,625 - INFO - loss: \u001b[0;32m0.5266\u001b[0m, std: 0.6098\n",
      "2021-05-13 12:28:18,845 - INFO - Epoch/batch: 18/4077, ibatch: 85077, loss: \u001b[0;36m0.5274\u001b[0m, std: 0.6062\n",
      "2021-05-13 12:28:25,226 - INFO - Epoch/batch: 18/4228, ibatch: 85228, loss: \u001b[0;36m0.5396\u001b[0m, std: 0.6283\n",
      "2021-05-13 12:28:31,749 - INFO - Epoch/batch: 18/4379, ibatch: 85379, loss: \u001b[0;36m0.5342\u001b[0m, std: 0.6067\n",
      "2021-05-13 12:28:46,351 - INFO - loss: \u001b[0;32m0.5269\u001b[0m, std: 0.6144\n",
      "2021-05-13 12:28:47,455 - INFO - Epoch 18 average training loss: \u001b[0;46m0.5273\u001b[0m std: 0.6067\n",
      "2021-05-13 12:28:47,460 - INFO - Epoch 18 average validate loss: \u001b[0;46m0.5268\u001b[0m std: 0.6132\n",
      "2021-05-13 12:28:49,334 - INFO - Epoch/batch: 19/   0, ibatch: 85500, loss: \u001b[0;36m0.5165\u001b[0m, std: 0.6008\n",
      "2021-05-13 12:28:59,660 - INFO - loss: \u001b[0;32m0.5269\u001b[0m, std: 0.6144\n",
      "2021-05-13 12:29:05,645 - INFO - Epoch/batch: 19/ 151, ibatch: 85651, loss: \u001b[0;36m0.5203\u001b[0m, std: 0.5953\n",
      "2021-05-13 12:29:11,697 - INFO - Epoch/batch: 19/ 302, ibatch: 85802, loss: \u001b[0;36m0.5419\u001b[0m, std: 0.6295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 573: ReduceOnPlateau set learning rate to 2.6183890704263157e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 12:29:28,018 - INFO - loss: \u001b[0;32m0.5268\u001b[0m, std: 0.6124\n",
      "2021-05-13 12:29:28,133 - INFO - Epoch/batch: 19/ 453, ibatch: 85953, loss: \u001b[0;36m0.5153\u001b[0m, std: 0.5848\n",
      "2021-05-13 12:29:34,508 - INFO - Epoch/batch: 19/ 604, ibatch: 86104, loss: \u001b[0;36m0.5241\u001b[0m, std: 0.5981\n",
      "2021-05-13 12:29:40,621 - INFO - Epoch/batch: 19/ 755, ibatch: 86255, loss: \u001b[0;36m0.5357\u001b[0m, std: 0.6073\n",
      "2021-05-13 12:29:56,704 - INFO - loss: \u001b[0;32m0.5272\u001b[0m, std: 0.6182\n",
      "2021-05-13 12:29:57,018 - INFO - Epoch/batch: 19/ 906, ibatch: 86406, loss: \u001b[0;36m0.5058\u001b[0m, std: 0.5889\n",
      "2021-05-13 12:30:03,148 - INFO - Epoch/batch: 19/1057, ibatch: 86557, loss: \u001b[0;36m0.5175\u001b[0m, std: 0.6020\n",
      "2021-05-13 12:30:09,376 - INFO - Epoch/batch: 19/1208, ibatch: 86708, loss: \u001b[0;36m0.5371\u001b[0m, std: 0.6182\n",
      "2021-05-13 12:30:25,117 - INFO - loss: \u001b[0;32m0.5270\u001b[0m, std: 0.6147\n",
      "2021-05-13 12:30:25,451 - INFO - Epoch/batch: 19/1359, ibatch: 86859, loss: \u001b[0;36m0.5246\u001b[0m, std: 0.6021\n",
      "2021-05-13 12:30:31,921 - INFO - Epoch/batch: 19/1510, ibatch: 87010, loss: \u001b[0;36m0.5358\u001b[0m, std: 0.6228\n",
      "2021-05-13 12:30:38,249 - INFO - Epoch/batch: 19/1661, ibatch: 87161, loss: \u001b[0;36m0.5149\u001b[0m, std: 0.5947\n",
      "2021-05-13 12:30:53,357 - INFO - loss: \u001b[0;32m0.5266\u001b[0m, std: 0.6089\n",
      "2021-05-13 12:30:53,834 - INFO - Epoch/batch: 19/1812, ibatch: 87312, loss: \u001b[0;36m0.5445\u001b[0m, std: 0.6301\n",
      "2021-05-13 12:31:00,046 - INFO - Epoch/batch: 19/1963, ibatch: 87463, loss: \u001b[0;36m0.5335\u001b[0m, std: 0.6092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 584: ReduceOnPlateau set learning rate to 2.356550163383684e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 12:31:06,092 - INFO - Epoch/batch: 19/2114, ibatch: 87614, loss: \u001b[0;36m0.5177\u001b[0m, std: 0.5887\n",
      "2021-05-13 12:31:21,599 - INFO - loss: \u001b[0;32m0.5267\u001b[0m, std: 0.6118\n",
      "2021-05-13 12:31:22,237 - INFO - Epoch/batch: 19/2265, ibatch: 87765, loss: \u001b[0;36m0.5352\u001b[0m, std: 0.6181\n",
      "2021-05-13 12:31:29,590 - INFO - Epoch/batch: 19/2416, ibatch: 87916, loss: \u001b[0;36m0.5398\u001b[0m, std: 0.6194\n",
      "2021-05-13 12:31:35,812 - INFO - Epoch/batch: 19/2567, ibatch: 88067, loss: \u001b[0;36m0.5237\u001b[0m, std: 0.5931\n",
      "2021-05-13 12:31:50,617 - INFO - loss: \u001b[0;32m0.5268\u001b[0m, std: 0.6140\n",
      "2021-05-13 12:31:51,407 - INFO - Epoch/batch: 19/2718, ibatch: 88218, loss: \u001b[0;36m0.5113\u001b[0m, std: 0.5903\n",
      "2021-05-13 12:31:57,650 - INFO - Epoch/batch: 19/2869, ibatch: 88369, loss: \u001b[0;36m0.5301\u001b[0m, std: 0.6069\n",
      "2021-05-13 12:32:03,942 - INFO - Epoch/batch: 19/3020, ibatch: 88520, loss: \u001b[0;36m0.5157\u001b[0m, std: 0.6030\n",
      "2021-05-13 12:32:19,726 - INFO - loss: \u001b[0;32m0.5267\u001b[0m, std: 0.6118\n",
      "2021-05-13 12:32:20,634 - INFO - Epoch/batch: 19/3171, ibatch: 88671, loss: \u001b[0;36m0.5432\u001b[0m, std: 0.6269\n",
      "2021-05-13 12:32:26,915 - INFO - Epoch/batch: 19/3322, ibatch: 88822, loss: \u001b[0;36m0.5412\u001b[0m, std: 0.6144\n",
      "2021-05-13 12:32:33,181 - INFO - Epoch/batch: 19/3473, ibatch: 88973, loss: \u001b[0;36m0.5243\u001b[0m, std: 0.6138\n",
      "2021-05-13 12:32:48,225 - INFO - loss: \u001b[0;32m0.5265\u001b[0m, std: 0.6089\n",
      "2021-05-13 12:32:49,271 - INFO - Epoch/batch: 19/3624, ibatch: 89124, loss: \u001b[0;36m0.5247\u001b[0m, std: 0.5974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 595: ReduceOnPlateau set learning rate to 2.1208951470453157e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 12:32:55,713 - INFO - Epoch/batch: 19/3775, ibatch: 89275, loss: \u001b[0;36m0.5182\u001b[0m, std: 0.5927\n",
      "2021-05-13 12:33:02,381 - INFO - Epoch/batch: 19/3926, ibatch: 89426, loss: \u001b[0;36m0.5198\u001b[0m, std: 0.5986\n",
      "2021-05-13 12:33:17,754 - INFO - loss: \u001b[0;32m0.5269\u001b[0m, std: 0.6148\n",
      "2021-05-13 12:33:18,889 - INFO - Epoch/batch: 19/4077, ibatch: 89577, loss: \u001b[0;36m0.5255\u001b[0m, std: 0.6075\n",
      "2021-05-13 12:33:25,096 - INFO - Epoch/batch: 19/4228, ibatch: 89728, loss: \u001b[0;36m0.5392\u001b[0m, std: 0.6194\n",
      "2021-05-13 12:33:31,138 - INFO - Epoch/batch: 19/4379, ibatch: 89879, loss: \u001b[0;36m0.5352\u001b[0m, std: 0.6117\n",
      "2021-05-13 12:33:45,614 - INFO - loss: \u001b[0;32m0.5267\u001b[0m, std: 0.6128\n",
      "2021-05-13 12:33:46,752 - INFO - Epoch 19 average training loss: \u001b[0;46m0.5273\u001b[0m std: 0.6064\n",
      "2021-05-13 12:33:46,757 - INFO - Epoch 19 average validate loss: \u001b[0;46m0.5268\u001b[0m std: 0.6130\n",
      "2021-05-13 12:33:48,498 - INFO - Epoch/batch: 20/   0, ibatch: 90000, loss: \u001b[0;36m0.5184\u001b[0m, std: 0.6033\n",
      "2021-05-13 12:33:58,679 - INFO - loss: \u001b[0;32m0.5267\u001b[0m, std: 0.6129\n",
      "2021-05-13 12:34:04,938 - INFO - Epoch/batch: 20/ 151, ibatch: 90151, loss: \u001b[0;36m0.5207\u001b[0m, std: 0.6074\n",
      "2021-05-13 12:34:10,994 - INFO - Epoch/batch: 20/ 302, ibatch: 90302, loss: \u001b[0;36m0.5385\u001b[0m, std: 0.6141\n",
      "2021-05-13 12:34:27,562 - INFO - loss: \u001b[0;32m0.5268\u001b[0m, std: 0.6133\n",
      "2021-05-13 12:34:27,683 - INFO - Epoch/batch: 20/ 453, ibatch: 90453, loss: \u001b[0;36m0.5289\u001b[0m, std: 0.6078\n",
      "2021-05-13 12:34:34,073 - INFO - Epoch/batch: 20/ 604, ibatch: 90604, loss: \u001b[0;36m0.5198\u001b[0m, std: 0.5951\n",
      "2021-05-13 12:34:39,949 - INFO - Epoch/batch: 20/ 755, ibatch: 90755, loss: \u001b[0;36m0.5195\u001b[0m, std: 0.5914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 606: ReduceOnPlateau set learning rate to 1.9088056323407842e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 12:34:55,416 - INFO - loss: \u001b[0;32m0.5272\u001b[0m, std: 0.6176\n",
      "2021-05-13 12:34:55,671 - INFO - Epoch/batch: 20/ 906, ibatch: 90906, loss: \u001b[0;36m0.5170\u001b[0m, std: 0.5945\n",
      "2021-05-13 12:35:02,121 - INFO - Epoch/batch: 20/1057, ibatch: 91057, loss: \u001b[0;36m0.5300\u001b[0m, std: 0.6093\n",
      "2021-05-13 12:35:08,647 - INFO - Epoch/batch: 20/1208, ibatch: 91208, loss: \u001b[0;36m0.5092\u001b[0m, std: 0.5901\n",
      "2021-05-13 12:35:24,327 - INFO - loss: \u001b[0;32m0.5272\u001b[0m, std: 0.6179\n",
      "2021-05-13 12:35:24,727 - INFO - Epoch/batch: 20/1359, ibatch: 91359, loss: \u001b[0;36m0.5227\u001b[0m, std: 0.6010\n",
      "2021-05-13 12:35:30,948 - INFO - Epoch/batch: 20/1510, ibatch: 91510, loss: \u001b[0;36m0.5485\u001b[0m, std: 0.6408\n",
      "2021-05-13 12:35:36,877 - INFO - Epoch/batch: 20/1661, ibatch: 91661, loss: \u001b[0;36m0.5230\u001b[0m, std: 0.6096\n",
      "2021-05-13 12:35:52,153 - INFO - loss: \u001b[0;32m0.5267\u001b[0m, std: 0.6120\n",
      "2021-05-13 12:35:52,644 - INFO - Epoch/batch: 20/1812, ibatch: 91812, loss: \u001b[0;36m0.5288\u001b[0m, std: 0.6013\n",
      "2021-05-13 12:35:58,827 - INFO - Epoch/batch: 20/1963, ibatch: 91963, loss: \u001b[0;36m0.5228\u001b[0m, std: 0.6009\n",
      "2021-05-13 12:36:04,759 - INFO - Epoch/batch: 20/2114, ibatch: 92114, loss: \u001b[0;36m0.5135\u001b[0m, std: 0.6015\n",
      "2021-05-13 12:36:20,603 - INFO - loss: \u001b[0;32m0.5266\u001b[0m, std: 0.6123\n",
      "2021-05-13 12:36:21,348 - INFO - Epoch/batch: 20/2265, ibatch: 92265, loss: \u001b[0;36m0.5332\u001b[0m, std: 0.6172\n",
      "2021-05-13 12:36:27,886 - INFO - Epoch/batch: 20/2416, ibatch: 92416, loss: \u001b[0;36m0.5302\u001b[0m, std: 0.6156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 617: ReduceOnPlateau set learning rate to 1.717925069106706e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 12:36:33,773 - INFO - Epoch/batch: 20/2567, ibatch: 92567, loss: \u001b[0;36m0.5118\u001b[0m, std: 0.5949\n",
      "2021-05-13 12:36:48,851 - INFO - loss: \u001b[0;32m0.5266\u001b[0m, std: 0.6117\n",
      "2021-05-13 12:36:49,588 - INFO - Epoch/batch: 20/2718, ibatch: 92718, loss: \u001b[0;36m0.5361\u001b[0m, std: 0.6191\n",
      "2021-05-13 12:36:55,771 - INFO - Epoch/batch: 20/2869, ibatch: 92869, loss: \u001b[0;36m0.5348\u001b[0m, std: 0.6114\n",
      "2021-05-13 12:37:01,712 - INFO - Epoch/batch: 20/3020, ibatch: 93020, loss: \u001b[0;36m0.5353\u001b[0m, std: 0.6062\n",
      "2021-05-13 12:37:17,022 - INFO - loss: \u001b[0;32m0.5265\u001b[0m, std: 0.6087\n",
      "2021-05-13 12:37:17,916 - INFO - Epoch/batch: 20/3171, ibatch: 93171, loss: \u001b[0;36m0.5280\u001b[0m, std: 0.6126\n",
      "2021-05-13 12:37:23,763 - INFO - Epoch/batch: 20/3322, ibatch: 93322, loss: \u001b[0;36m0.5355\u001b[0m, std: 0.6133\n",
      "2021-05-13 12:37:29,869 - INFO - Epoch/batch: 20/3473, ibatch: 93473, loss: \u001b[0;36m0.5235\u001b[0m, std: 0.5930\n",
      "2021-05-13 12:37:45,210 - INFO - loss: \u001b[0;32m0.5266\u001b[0m, std: 0.6110\n",
      "2021-05-13 12:37:46,192 - INFO - Epoch/batch: 20/3624, ibatch: 93624, loss: \u001b[0;36m0.5324\u001b[0m, std: 0.6105\n",
      "2021-05-13 12:37:52,365 - INFO - Epoch/batch: 20/3775, ibatch: 93775, loss: \u001b[0;36m0.5318\u001b[0m, std: 0.6126\n",
      "2021-05-13 12:37:58,395 - INFO - Epoch/batch: 20/3926, ibatch: 93926, loss: \u001b[0;36m0.5216\u001b[0m, std: 0.6030\n",
      "2021-05-13 12:38:13,728 - INFO - loss: \u001b[0;32m0.5266\u001b[0m, std: 0.6095\n",
      "2021-05-13 12:38:14,862 - INFO - Epoch/batch: 20/4077, ibatch: 94077, loss: \u001b[0;36m0.5512\u001b[0m, std: 0.6282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 628: ReduceOnPlateau set learning rate to 1.5461325621960354e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 12:38:21,114 - INFO - Epoch/batch: 20/4228, ibatch: 94228, loss: \u001b[0;36m0.5238\u001b[0m, std: 0.6005\n",
      "2021-05-13 12:38:27,253 - INFO - Epoch/batch: 20/4379, ibatch: 94379, loss: \u001b[0;36m0.5276\u001b[0m, std: 0.6061\n",
      "2021-05-13 12:38:41,611 - INFO - loss: \u001b[0;32m0.5268\u001b[0m, std: 0.6126\n",
      "2021-05-13 12:38:42,732 - INFO - Epoch 20 average training loss: \u001b[0;46m0.5270\u001b[0m std: 0.6064\n",
      "2021-05-13 12:38:42,752 - INFO - Epoch 20 average validate loss: \u001b[0;46m0.5268\u001b[0m std: 0.6127\n"
     ]
    }
   ],
   "source": [
    "# 训练模型， 最后的loss应该在[0.52, 0.53]区间内\n",
    "# 每epoch需要五分钟左右(在CPU上)， 自然结束需要～20个epoch\n",
    "# train the model, the final loss should be within 0.52 and 0.53.\n",
    "# takes about 5 minutes to complete one epoch. \n",
    "# self-termination takes ~20 epochs\n",
    "train_loss, valid_loss = fp.train(model, train_data, num_epochs=21, validate_callback = fp.func_partial(fp.validate_in_train, midata=valid_data, save_dir='./'))\n",
    "# 注： 软标签的情况下不能得到0的交叉熵\n",
    "# Note: zero cross-entropy is not possible with soft labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 12:57:33,231 - INFO - Loading model states from: earlystop_0.5264\n",
      "2021-05-13 12:57:33,565 - INFO - Loaded net state: earlystop_0.5264/net.state\n",
      "2021-05-13 12:57:33,570 - WARNING - Error in optim state_dict loading!\n",
      "2021-05-13 12:57:33,571 - INFO - Getting loss function: ['softmax+mse']\n",
      "2021-05-13 12:57:33,573 - INFO - Validating, data size: 500\n",
      "2021-05-13 12:57:33,573 - INFO -            batch size: 64\n",
      "2021-05-13 12:57:33,574 - INFO -               shuffle: False\n",
      "2021-05-13 12:57:33,574 - INFO -          # of batches: 8\n",
      "2021-05-13 12:57:33,575 - INFO -        recap interval: 1\n",
      "2021-05-13 12:57:33,575 - INFO -          loss padding: False\n",
      "2021-05-13 12:57:36,627 - INFO - ibatch:    0, loss: 0.2021, std: 0.2891\n",
      "2021-05-13 12:57:37,426 - INFO - ibatch:    1, loss: 0.2071, std: 0.3006\n",
      "2021-05-13 12:57:38,265 - INFO - ibatch:    2, loss: 0.1919, std: 0.2813\n",
      "2021-05-13 12:57:39,052 - INFO - ibatch:    3, loss: 0.2279, std: 0.3225\n",
      "2021-05-13 12:57:39,839 - INFO - ibatch:    4, loss: 0.2210, std: 0.3141\n",
      "2021-05-13 12:57:40,624 - INFO - ibatch:    5, loss: 0.1947, std: 0.2815\n",
      "2021-05-13 12:57:41,411 - INFO - ibatch:    6, loss: 0.2102, std: 0.2975\n",
      "2021-05-13 12:57:42,054 - INFO - ibatch:    7, loss: 0.2184, std: 0.3121\n",
      "2021-05-13 12:57:43,100 - INFO - Validate mean: \u001b[0;46m0.2089\u001b[0m, std: 0.1084\n"
     ]
    }
   ],
   "source": [
    "# 读取最后一个checkpoint目录 (忽略优化器state_dict读取错误)\n",
    "# read the last saved earlystop directory （ignore the error in optimizer state_dict loading)\n",
    "fp.state_dict_load(model, model.validate_hist.saved_dirs[-1])\n",
    "# 可以改动损失函数，检测mse损失（单个模型最好的结果： ～0.20）\n",
    "# the loss_fn can be changed to softmax+mse to check the mse loss.\n",
    "# (the best/lowest loss obtained from a single model was ~0.20)\n",
    "args.loss_fn = ['softmax+mse']\n",
    "model.loss_fn = fp.get_loss_fn(args)\n",
    "valid_loss = fp.validate(model, valid_data, verbose=1, batch_size=64) # try a larger batch_size, should make no difference though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 12:57:54,373 - INFO - Loading data: work/data/predict.pkl\n",
      "2021-05-13 12:57:55,946 - INFO -    # of data: 112,  max seqlen: 861, user seq_length: -1\n",
      "2021-05-13 12:57:55,947 - INFO -  residue fmt: vector, nn: 0, dbn: True, attr: False, genre: upp\n",
      "2021-05-13 12:57:58,986 - INFO - Predicting, data size: 112\n",
      "2021-05-13 12:57:58,989 - INFO -            batch size: 1\n",
      "2021-05-13 12:57:58,990 - INFO -               shuffle: False\n",
      "2021-05-13 12:57:58,990 - INFO -          # of batches: 112\n",
      "2021-05-13 12:57:58,991 - INFO -        recap interval: 4\n",
      "2021-05-13 12:57:58,992 - INFO - Predicted files will be saved in: predict.files\n",
      "  0%|          | 0/112 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# 读取预测数据， 存储预测结果\n",
    "# 提交的结果是平均了三次运行最好checkpoint存储， 它们分别得到了0.24, 0.24, 0.242的sqrt(mse)损失， 平均后得到了0.238\n",
    "# 可惜前两次的checkpoint没有被保存， 只保存了预测的结果.\n",
    "# 虽然是同一个网络架构， 因为每次训练的train_test_split是随机的， 模型平均的效果和cross_validate相近\n",
    "# Read in prediction data, and save the predicted results\n",
    "# The submitted results are the average of the best checkpoints/earlystops from three independent trainings.\n",
    "# Unfortunately the checkpoints for the first two were not kept, except for the predicted results。\n",
    "# As each training randomly splits the train and validation data, this model averaging approximates\n",
    "# the effect/benefit of cross-validation.\n",
    "predict_data = fp.get_midata(args, data_name='predict', seq_length=-1)\n",
    "y_model, std_model = fp.predict(model, predict_data, save_dir='predict.files', batch_size=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
