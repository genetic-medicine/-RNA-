{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 螺旋桨RNA结构预测竞赛第一名方案 #\n",
    "*本文由来自广州的基因疗法团队投稿*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 生物基础 ## \n",
    "*注: 如有生物背景,请跳过本节*\n",
    "\n",
    "所有可以称为生命体的系统，无论简单或复杂，无一例外地以核酸为遗传物质。核酸主要分为脱氧核糖核酸(DNA)和核糖核酸(RNA)两大类，每类核酸都是由四种不同的单元（称为碱基）以线性的形式连接成一个长链，它们的排列顺序就是我们所说的基因序列。核酸在自然界中的重要地位毋庸置疑，甚至于无自我繁殖能力的病毒也是以核酸作为基因，比如正在全世界肆虐的新冠病毒的基因是大约3万个碱基长的单链RNA. 有些基因能够编码特异的蛋白，我们称这些为编码基因. 分子生物学的中心法则描述了从基因到蛋白的过程: DNA先被转录为信使RNA (mRNA), 然后mRNA被翻译为蛋白。但编码基因只占了基因组的一小部分(~2%)，大部分被转录的RNA是非编码RNA。\n",
    "\n",
    "RNA是本次螺旋浆结构预测竞赛的主角。竞赛要求用深度学习的方法预测每个碱基的不配对几率。 作为一个线性的链状大分子，RNA具有非常高的柔性， 易于弯曲， 好比一条细长的线。 如果碱基之间没有任何相互作用，RNA在三维空间里就会杂乱无章毫无结构而言。可作为生物大分子的RNA需要具有特定结构才能行使其生物功能, 因而碱基间是否配对和如何配对至关重要.\n",
    "\n",
    "RNA主要由四种碱基组成，虽然我们最熟悉的是两种碱基对类型, 真实的情况是碱基有很多配对的方式. 每个碱基有三个边可以配对，再加上糖苷键的空间取向，两个碱基间就有12种不同的配对方式, 全部一起有超过30种不同的碱基两两配对组合(这里不考虑概率极小的两个以上碱基的配对)。所以对于一个RNA序列, 其可能的配对方式是一个巨大的排列组合空间。 碱基之间配对的源驱动力来自于碱基之间的吸引力(直接或间接)，最主要的方式是氢键。每一个碱基和其它碱基都存在或强或弱的吸引力，但是只能和其中一个配对，所以它们相互竞争, 最优的配对组合能最大化碱基之间的吸引作用, 从热力学角度上说自由能最小化. 同时往往有很多配对组合有相近的能量, 并不存在一个单一的稳定结构, 而是有很多亚稳定结构. 这时候一个非常有意义的碱基特性就是它在这些结构集里的不配对几率(或者配对几率)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 赛题说明 ##\n",
    "### 技术基础 ###\n",
    "百度螺旋桨团队近年来在RNA结构预测上做出了重要的贡献, 研发出速度最快同时准确度极高的一系列线性算法, 做到了对RNA二级结构的直接预测和比对(LinearFold[1]), 对碱基配分和配对几率的预测(LinearPartition[2]), 和对RNA序列的设计优化(LinearDesign[3]).  这次比赛主要是基于螺旋桨团队的LinearFold和LinearPartition算法，用深度学习的方法来预测RNA的不配对几率. \n",
    "\n",
    "在LinearDesign文章里对碱基不配对几率的意义有很好的详述, 拿mRNA疫苗为例, 如果mRNA序列里大部分碱基有很小的不配对几率, 那说明这是一个稳定的二维甚至三维RNA结构. 稳定的结构会大大延长其在细胞内的寿命,从而提高蛋白的产量. 如果大部分碱基有非常大的不配对几率, 那说明这是一个非常无结构或动态的RNA. 这会让mRNA在溶液中易于被水解, 降低其稳定性和寿命, 不利于运输,储藏,翻译等等. 所以碱基不配对几率对RNA序列设计有极其重要的定量指导意义.\n",
    "\n",
    "### 训练数据分析 ###\n",
    "\n",
    "训练数据集一共4750个RNA序列， 验证集250个序列, 测试集112个序列，它们的长度分布如图一所示. 训练和验证集长度在100 和500碱基之间, 分布相对均匀，可是测试集里有较长的序列，尤其超过30%的序列的长度500以上，这给预测增加了难度。如果按照碱基计算，训练和验证集一共有1,562,736个碱基(序列平均长度313碱基)，测试集一共有49,279碱基（序列平均长度440碱基）。 \n",
    "\n",
    "![图一](https://ai-studio-static-online.cdn.bcebos.com/1785b0ba9f464b85a36211a9d8cc44e2012a563dbff641aabb6f4c683f74314a)\n",
    "\n",
    "图表 1: 训练(蓝),验证(红)和测试(绿)集RNA序列的长度分布.\n",
    "\n",
    "基于提供的序列, 我们用LinearFold计算二级结构和用LinearPartition计算不配对几率。这些计算的结果可以转换成碱基不配对几率, 比如LinearFold的二级结构可转换为二进制的不配对几率(0或1). LinearFold对训练，验证和测试集预测的碱基不配对几率分布大体一致，约62%的碱基是配对的，其余38%不配对. LinearPartition所预测的不配对几率是一个连续空间，其分布如图二所示。我们可以看到三个数据集的几率分布相差不多，最突出的特征是几率分布非常不均匀, 尤其有两个分布峰分别在0和1附近, 说明即使在连续空间里大多数碱基的配对几率集中在0或1附近. 为了能直接比较LinearFold和LinearPartition, 我们可以把LinearPartition的连续空间二进制化 (0.0-0.5为0, 0.5-1.0为1），这样得到～55%的碱基不配对几率为0， 和LinearFold预测的62%有一些差距. \n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/c8d6c171d7a44f2e9bca418c22f336d323df11c6c4644cd7944ee98f793a67dd)\n",
    "\n",
    "图表 2: LinearPartition预测的碱基不配对几率分布: 训练(蓝), 验证(红)和测试(绿)集.\n",
    "\n",
    "训练和验证集里的标注数据是每个碱基的不配对几率。图三展示了聚集了所有序列碱基不配对几率的分布, 我们可以看到不配对几率成两极分化走势， 绝大部分碱基的配对或不配对的几率在95%以上，这比LinearPartition预测的不配对概率分布要更加两极分化。值得一提的是标注数据在二进制离散化后的分布和LinearFold预测高度一致，给出～60%碱基配对.\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/b49c78dfc040408da4f09cad92a37b0cfd102db22a854cf1b2f81d9149528206)\n",
    "\n",
    "\n",
    "图表 3: 训练(蓝)和验证(红)集碱基不配对几率标注分布\n",
    "\n",
    "### 评比指标 ###\n",
    "\n",
    "碱基不配对几率预测和标注的差异采用的是经典而直观的均平方差的平方根（Root Mean Square Error, RMSE)。如前文所述， 我们可以把LinearFold和LinearPartition的预测结果转换为碱基的不配对几率， 这样可以先和标注作对比。 虽然LinearFold给出结果的整体分布和标注几乎形同，其RMSE却有～0.30， 而LinearPartition给出的RMSE稍小，～0.26. 可见在预测连续空间的不配对几率时， LinearPartition有更好的表现， 虽然其分布目测差距更大。 当然我们还可以继续分析RMSE和RNA序列长度的相关性等等， 这里不做详述。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型构建和训练 ##\n",
    "\n",
    "### 框架思路 ###\n",
    "这个深度学习任务是一个典型的序列到序列(seq2seq or seq2vec)问题： 输入是RNA的碱基序列， 输出是每个碱基的不配对概率。那么，一个序列到序列的深度学习模型需要“学到”碱基之间什么样的关系才能准确预测RNA的不配对几率呢？我们从以下几个角度考虑碱基配对问题, 并设计了对应的网络框架。\n",
    "\n",
    "第一，一个碱基的配对与否是由全部序列整体决定的，仅仅利用序列的任何片段的信息是不能准确预测的。这个特性可以用循环神经网络（RNN）或者变压器（Transformer）来描述。我们选择了一个Transformer编码器层来赋予每个碱基一个全局的信息，对位置的编码用的是正旋和余旋函数。\n",
    "第二，碱基的排列是一个有极强线性的序列，改变碱基间的任何顺序都会或大或小地影响其不配对概率，即使有些碱基变化能保持碱基的二级甚至三极的最稳定结构，碱基的不配对几率也不会丝毫不变。所以我们决定加入一个RNN来强化碱基间的序列依赖， 具体形式采用了一个双向LSTM层。 另外一个考虑是Transformer编码器在训练数据有限情况下可能会表现不佳。\n",
    "第三，碱基配对有较强的局部相关， 要形成稳定的碱基配对， 至少相邻的三个以上的碱基都配对， 自然越多的连续碱基配对越稳定，同时只有1-2个连续的碱基有很小的不配对几率是不可能的。 鉴于此， 我们采用了一维的卷积层(Conv1D)来描述这种局部相关性。 \n",
    "\n",
    "基于以上设计理念，如图四所示, 除了输入嵌入层外, 模型框架有以下组成。1） 输入全连接模块，2）Transformer编码模块， 3）双向LSTM模块， 4） Conv1D模块， 5）输出全连接模块. 对模型的深度和宽度的考量有两个主要方面。 一是训练和验证集的数量有限，标注的碱基个数一共有～1.6M，过大的模型容易过拟合， 二是LinearFold/LinearPartition的预测给了很好的起始点， 对模型优化有较好的引导。我们最后采用的参数为, 模块1-4的层数为1, 维度为32 ，输出全连接模块层数为3, 维度分别为32, 32, 2, 归一化为LayerNorm，激活函数为ReLU, Dropout为0.2, 最终可训练的参数个数为36,418, 大约是标注个数的2.2%。\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/7227509d29654c22ad88326a918926acf836ab482a3045f58bb6fdc64d1245c3\" width=480/>\n",
    "\n",
    "图表 4: 模型框架示意图\n",
    "\n",
    "### 模型实现 ###\n",
    "\n",
    "我们用飞浆2.0深度学习平台(Paddle)实现本模型， 过程中大量参考了飞浆的详细文档和开源的源代码， 尤其利用了Paddle.nn中直接可用的TransformerEncoder, LSTM, Conv1D等等。损失函数也主要采用Paddle.nn提供的函数库。为了更便捷的搭建不同的模型或改变模型的参数，我们把主要的参数存储在一个args结构里，然后包装每个模块（比如LSTM, Conv1D)能接受args为输入， 图5展示了竞赛所用模型的代码，图6展示了其中一个模块的代码, 搭建过程相对简单, paddle_nets.py包含了所有模型建构的相关代码.\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/851f3bb5c1774cedbddfa4a6e5dd9aec3c091f79f06b451d93af31ed7c66d634)\n",
    "\n",
    "图表 5: 网络框架构建代码范例\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/a83239417f1b4bfa983598977f2385952e20f162246d4e87b1a406501ed4d46b\" width=800/>\n",
    "\n",
    "图表 6: 模块构建代码范例\n",
    "\n",
    "### 输入输出 ###\n",
    "对于一个长度为L的RNA序列 每个碱基字符用一个四维的onehot向量表达， LinearFold的二级结构字符用一个四维的onehot向量表达(三维足够,这里凑了个偶数)，LinearPartition预测可以直接使用。最终得到的输入矩阵的维度为 [N, L, 10], 其中N是batch size, L是序列长度。 输出全连接模块的最后一层维度分别为2. 所以模型的输出是一个[N, L, 2]维的矩阵， 沿最后一维进行softmax操作，得到了每个碱基的配对和不配对几率。损失函数有两种可取的方法，最直接的是对不配对几率计算竞赛采用的均平方差的平方根（RMSE）， 同时考虑结果为softmax后的几率，标注为二分类的软标注，我们也可以采用交叉熵做为损失。 两种不同主要在于交叉熵在对几率p的梯度上多了一个1/p(1-p)的因子，对接近0或1的几率梯度增强。 在实验中我们发现两种损失函数的结果是大体相同， 猜想是因为几率主要分布在0和1的附近， 这个附加因子的作用不显著. \n",
    "\n",
    "### 模型训练 ###\n",
    "模型训练在单个CPU上进行, 每个epoch大约5分钟，一般一两个小时能训练结束. 有关代码在fly_paddle.py脚本里. 前文提到的args包含了大部分模型和训练的参数, fly_paddle.py设置了缺省参数,  同时可以在命令行设置参数取值, 图7演示了命令行启动模型训练的界面, 训练时汇报损失界面如图8所示.\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/8ff0490f54d740148e120f2577e9c39afb3ea482d8da4c82b809ef55f7b313fc)\n",
    "\n",
    "图表 7: 命令行启动模型训练范例\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/0c176a62798c4b80ba4ff86926f297f6ef045abb0a684e48bc0926dca4967f51)\n",
    "\n",
    "图表 8: 模型训练中的进度汇报范例\n",
    "\n",
    "模型的训练主要分以下三步骤:\n",
    "第一步是对深度学习任务培养一些直觉。我们先用最简单的全连接模型看一下在不考虑碱基间相互作用情况下的RMSE。如果输入的碱基信息只有其字符编码的四维的onehot向量， RMSE为～0.42，不出所料和随机猜测几率相差无几，加入LinearFold和LinearPartition预测信息能降低RMSE到0.25左右。可见LinearFold和LinearPartition给出了很好的初始点，如果要进一步降低RMSE, 我们需要考虑碱基之间的相互作用.\n",
    "\n",
    "第二步是分步加入TransformerEncoder, LSTM和Conv1D模. 我们发现单一的TransformerEncoder或者LSTM足够降低RMSE到0.21-0.22之间，Conv1D模块对RMSE影响较小, 这与我们的期望也大致相符，因为一层的Conv1D也只是考虑到了紧邻碱基之间的作用。最后三个模块一起能降低RMSE到0.20-0.21区间。\n",
    "\n",
    "第三步主要是摸索超参数和改进模型来进一步降低RMSE。我们首先对learning_rate， dropout, 和weight decay做了一系列的摸索， 发现在常用的超参数区间里模型表现相当， 原因可能主要有两个，一个是参数空间相对较小，其二是起始的数据和标注相距不远. 因为训练集和验证集的RMSE在所有的训练中RMSE相当, 我们没有采用参数正则化.  \n",
    "\n",
    "我们对改进模型的努力收获较小。图9展示了对训练集序列中模型预测的碱基不配对几率分布， 最突出的问题是有39%的碱基的几率在[0.1, 0.9]区间内，而标注集中只有22%碱基的的不配对几率在此区间。一个比较简单直接的方法是增加[0.1, 0.9]区间对损失的贡献，比如从损失中减去(几率-0.5)的平方, 可是这方面的尝试并没有减小RMSE。我们采用的交叉熵能增加在0/1附近的梯度，也没有能够减小RMSE。另外一个明显的问题是RMSE过大，常见的解决方法是增加模型的深度或宽度，我们发现更大的模型能够更好地拟合训练数据（比如模块维度为128层数为3时可达到RMSE < 0.1），可验证数据的RMSE停留在0.2左右。我们认为这是一个典型的过度拟合现象，所以没有采用更大的模型。\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/15636ffe2dc347328d46774059f356233c842e16bb5447bc86939158b46882ed\" width=600/>\n",
    "\n",
    "图表 9: 模型预测碱基不配对几率分布\n",
    "\n",
    "在最后递交的预测结果里, 鉴于我们的模型较小, 我们采用了模型平均。在每次训练中我们从训练集中随机选取10%的序列做为验证集并存取验证集最小RMSE的模型参数。多次的训练就得到不同的参数集的同一模型，最后的结果我们平均了三个最好的参数集的预测。可能因为是出于同一模型，模型平均也仅降低RMSE不到0.5%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 总结与讨论 ##\n",
    "本文主要叙述了作者参加螺旋桨RNA结构预测竞赛的过程, 尤其关于探索的很多走的通和走不通的方向, 希望对感兴趣的读者有些借鉴. 基因疗法团队是深度学习的初学者, 一个很大的感触是飞浆深度学习平台的强大功能和极低的学习门槛。我们深度学习框架的搭建绝大部分用的是飞浆平台提供的成熟的库函数，包括用到的TransformerEncoder /LSTM等等只需直接调用。飞浆是我们学习的第一也是唯一的深度学习平台, 这些佐证了飞浆的成熟和易用性。 \n",
    "\n",
    "如前文所述, RNA碱基不配对几率对科学和医学意义重大, 尤其我们可以说RNA医学已经到来而且必将蓬勃发展. 这次竞赛也正是为了提高对碱基不配对几率预测的精度, 我们在训练中能达到和验证集的RMSE~0.20, 比赛测试集得到的RMSE差一些, ~0.24. 我们猜想一个原因是测试集有31%的序列长度超过训练和验证集中的最长序列长度, 增加了预测难度. 无论是0.20还是0.24, 和实验结果都相距甚远, 还远远达不到实用或和实验媲美的精度, 可提高的空间还很多. 在现有的基础上我们还可以尝试更多的方法来提高预测精度, 下面讨论一些可能的方向.\n",
    "\n",
    "首先, 我们可以增强输入信息. 比如在LinearFold和LinearPartition的基础上我们可以加入更多计算方法的预测数据. 在RNA结构预测领域有很多可用的软件, 比如基于物理的MFOLD[4], VIENNA RNAFOLD[5], RNASTRUCTURE[6], 基于机器学习的CONTRAFOLD[7], CDPFOLD[8], 和近年来快速发展的深度学习方法SPOT-RNA[9], E2EFOLD[10], MXFOLD2[11], DIRECT[12], RNACONCAT[13], DMFOLD[14]. \n",
    "\n",
    "其次, 我们的模型相对较小, 难于捕捉RNA碱基配对所受支配的碱基间的多体相互吸引与竞争, 所以一个提高预测精度的方向是训练更大的模型. 尤其是当RNA序列长度更长, 比如新冠病毒的基因有~30,000个碱基, mRNA疫苗也有几千个碱基, 并且其碱基经过大量的化学修饰, 预测其结构更是难上加难. 要能够预测更长更复杂的RNA, 更大的模型几乎是必须的. 比如图10展示了模型和标注之间的对比, 上下两组的长度分别为110和480, 我们可以看到对较短的序列预测准确度很高, 可是对长序列的预测差距很大. 更大的模型就需要更多的更全面的训练数据, 尤其对于有化学修饰的碱基, 实验测量很有可能是一个瓶颈. 需要我们另辟蹊径, 比如用模拟的方法得到更多的数据. \n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/a35b6dd3835f4c8fa35a3f277a6a9b279f721edf220a4fb694723144f3d2d8e3)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/cdc9f78781f64feb8e838fe9a2b2f1ebbf61c233d67d46c58f7e8524459c5d56)\n",
    "\n",
    "图表 10: 模型预测碱基不配对几率和标注的对比, 上排两图中的RNA范例长度为110, 下排长度为480. 左列两图展示了标注(蓝),预测(红)和它们的差(绿)对序列位置(X), 右列两图展示了预测(Y)对标注(X).\n",
    "\n",
    "最后, 从纯科学角度上, 一个理想的模型只需要RNA序列信息的就能准确预测碱基不配对几率和其它的结构信息. 这并不是可望而不可及的, 在现有的方法里, 比如百度螺旋桨的LinearFold/LinearPartition和前面提到的第一个端到端的深度学习方法SPOT-RNA都只需要序列就可以非常好的预测RNA的二级结构. 当然从实用角度上看, 即使一个方法需要综合很多方面的信息(比如需要聚合很多现有软件的预测结果), 如果能较快的准确预测RNA结构, 那也已足够. 条条大路通罗马, 很多道路需要摸索, 每条路都需要很多人的不懈努力.\n",
    "\n",
    "### 作者介绍 ###\n",
    "本文作者是来自广州以个人名义参赛的邱祥运, 他是一名有20多年科研经验的实验工作者, 近来希望能把深度学习应用到自己的科研上. 他的主要研究方向是核酸力学和核酸医学, 致力于为全人类的健康做贡献.\n",
    "\n",
    "### 相关链接 ###\n",
    "螺旋桨RNA结构预测竞赛: https://aistudio.baidu.com/aistudio/competition/detail/61\n",
    "\n",
    "aistudio源代码公开: https://aistudio.baidu.com/aistudio/projectdetail/1479469\n",
    "\n",
    "github源代码公开: https://github.com/genetic-medicine/PaddleHelix_RNA_UPP\n",
    "\n",
    "### 引用文献 ###\n",
    "1.\tHuang L, Zhang H, Deng D, Zhao K, Liu K, Hendrix DA, Mathews DH. LinearFold: linear-time approximate RNA folding by 5'-to-3' dynamic programming and beam search. Bioinformatics. 2019;35(14):i295-i304. \n",
    "2.\tZhang H, Zhang L, Mathews DH, Huang L. LinearPartition: linear-time approximation of RNA folding partition function and base-pairing probabilities. Bioinformatics. 2020;36(Supplement_1):i258-i67. \n",
    "3.\tZhang H, Zhang L, Li Z, Liu K, Liu B, Mathews DH, Huang L. LinearDesign: Efficient Algorithms for Optimized mRNA Sequence Design2020 April 01, 2020:[arXiv:2004.10177 p.]. Available from: https://ui.adsabs.harvard.edu/abs/2020arXiv200410177Z.\n",
    "4.\tZuker M. Mfold web server for nucleic acid folding and hybridization prediction. Nucleic Acids Research. 2003;31(13):3406-15. \n",
    "5.\tLorenz R, Bernhart SH, Höner zu Siederdissen C, Tafer H, Flamm C, Stadler PF, Hofacker IL. ViennaRNA Package 2.0. Algorithms for Molecular Biology. 2011;6(1):26. \n",
    "6.\tReuter JS, Mathews DH. RNAstructure: software for RNA secondary structure prediction and analysis. BMC Bioinformatics. 2010;11(1):129. \n",
    "7.\tDo CB, Woods DA, Batzoglou S. CONTRAfold: RNA secondary structure prediction without physics-based models. Bioinformatics. 2006;22(14):e90-e8. \n",
    "8.\tZhang H, Zhang C, Li Z, Li C, Wei X, Zhang B, Liu Y. A New Method of RNA Secondary Structure Prediction Based on Convolutional Neural Network and Dynamic Programming. Frontiers in Genetics. 2019;10(467). \n",
    "9.\tSingh J, Hanson J, Paliwal K, Zhou Y. RNA secondary structure prediction using an ensemble of two-dimensional deep neural networks and transfer learning. Nature Communications. 2019;10(1):5407. \n",
    "10.\tChen X, Li Y, Umarov R, Gao X, Song L. RNA Secondary Structure Prediction By Learning Unrolled Algorithms2020 February 01, 2020:[arXiv:2002.05810 p.]. Available from: https://ui.adsabs.harvard.edu/abs/2020arXiv200205810C.\n",
    "11.\tSato K, Akiyama M, Sakakibara Y. RNA secondary structure prediction using deep learning with thermodynamic integration. Nature Communications. 2021;12(1):941. \n",
    "12.\tJian Y, Wang X, Qiu J, Wang H, Liu Z, Zhao Y, Zeng C. DIRECT: RNA contact predictions by integrating structural patterns. BMC Bioinformatics. 2019;20(1):497. \n",
    "13.\tSun S, Wang W, Peng Z, Yang J. RNA inter-nucleotide 3D closeness prediction by deep residual neural networks. Bioinformatics. 2020;37(8):1093-8. \n",
    "14.\tWang L, Liu Y, Zhong X, Liu H, Lu C, Li C, Zhang H. DMfold: A Novel Method to Predict RNA Secondary Structure With Pseudoknots Based on Deep Learning and Improved Base Pair Maximization Principle. Frontiers in Genetics. 2019;10(143). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 附：  运行示范  ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "# !mkdir /home/aistudio/external-libraries\n",
    "# !pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"action\": \"train\",\n",
      "   \"argv\": \"-h\",\n",
      "   \"verbose\": 1,\n",
      "   \"resume\": false,\n",
      "   \"load_dir\": null,\n",
      "   \"save_dir\": null,\n",
      "   \"save_level\": 2,\n",
      "   \"save_grpby\": [\"epoch\", \"batch\"],\n",
      "   \"log\": \"fly_paddle-Jun19.log\",\n",
      "   \"data_args\": \"======= data args =======\",\n",
      "   \"data_dir\": \"data\",\n",
      "   \"data_name\": \"predict\",\n",
      "   \"data_suffix\": \".pkl\",\n",
      "   \"data_size\": 0,\n",
      "   \"test_size\": 0.1,\n",
      "   \"split_seed\": null,\n",
      "   \"input_genre\": \"Seq\",\n",
      "   \"input_fmt\": \"NLC\",\n",
      "   \"seq_length\": [0, 512, -1],\n",
      "   \"residue_fmt\": \"vector\",\n",
      "   \"residue_nn\": 0,\n",
      "   \"residue_dbn\": false,\n",
      "   \"residue_attr\": false,\n",
      "   \"residue_extra\": false,\n",
      "   \"label_genre\": \"upp\",\n",
      "   \"label_fmt\": \"NL\",\n",
      "   \"label_tone\": \"none\",\n",
      "   \"label_ntype\": 2,\n",
      "   \"label_smooth\": false,\n",
      "   \"net_args\": \"======= net args =======\",\n",
      "   \"net_src_file\": \"/home/aistudio/work/code/paddle_nets.py\",\n",
      "   \"net\": \"lazylinear\",\n",
      "   \"resnet\": false,\n",
      "   \"act_fn\": \"relu\",\n",
      "   \"norm_fn\": \"none\",\n",
      "   \"norm_axis\": -1,\n",
      "   \"dropout\": 0.2,\n",
      "   \"feature_dim\": 1,\n",
      "   \"embed_dim\": 32,\n",
      "   \"embed_num\": 1,\n",
      "   \"linear_num\": 2,\n",
      "   \"linear_dim\": [32],\n",
      "   \"linear_resnet\": false,\n",
      "   \"conv1d_num\": 1,\n",
      "   \"conv1d_dim\": [32],\n",
      "   \"conv1d_resnet\": false,\n",
      "   \"conv1d_stride\": 1,\n",
      "   \"conv2d_num\": 1,\n",
      "   \"conv2d_dim\": [32],\n",
      "   \"conv2d_resnet\": false,\n",
      "   \"attn_num\": 2,\n",
      "   \"attn_nhead\": 2,\n",
      "   \"attn_act\": \"relu\",\n",
      "   \"attn_dropout\": null,\n",
      "   \"attn_ffdim\": 32,\n",
      "   \"attn_ffdropout\": null,\n",
      "   \"lstm_num\": 2,\n",
      "   \"lstm_dim\": [32],\n",
      "   \"lstm_direct\": 2,\n",
      "   \"lstm_resnet\": false,\n",
      "   \"output_num\": 1,\n",
      "   \"output_dim\": [32, 32, 2],\n",
      "   \"output_resnet\": false,\n",
      "   \"optim_args\": \"======= optim args =======\",\n",
      "   \"optim\": \"adam\",\n",
      "   \"learning_rate\": 0.003,\n",
      "   \"beta1\": 0.9,\n",
      "   \"beta2\": 0.999,\n",
      "   \"epsilon\": 1e-08,\n",
      "   \"lr_scheduler\": \"reduced\",\n",
      "   \"lr_factor\": 0.9,\n",
      "   \"lr_patience\": 10,\n",
      "   \"weight_decay\": \"none\",\n",
      "   \"l1decay\": 0.0001,\n",
      "   \"l2decay\": 0.0001,\n",
      "   \"train_args\": \"======= train/loss args =======\",\n",
      "   \"batch_size\": 4,\n",
      "   \"num_epochs\": 777,\n",
      "   \"num_recaps_per_epoch\": 30,\n",
      "   \"num_callbacks_per_epoch\": 10,\n",
      "   \"loss_fn\": [\"mse\"],\n",
      "   \"loss_fn_scale\": [1],\n",
      "   \"loss_sqrt\": false,\n",
      "   \"loss_padding\": false,\n",
      "   \"validate_callback\": null,\n",
      "   \"trainloss_rdiff\": 0.001,\n",
      "   \"validloss_rdiff\": 0.001,\n",
      "   \"trainloss_patience\": 11,\n",
      "   \"validloss_patience\": 11,\n",
      "   \"mood_args\": \"======= mood args =======\",\n",
      "   \"debug\": false,\n",
      "   \"lucky\": false,\n",
      "   \"lazy\": false,\n",
      "   \"sharp\": false,\n",
      "   \"comfort\": false,\n",
      "   \"explore\": false,\n",
      "   \"exploit\": false,\n",
      "   \"diehard\": false,\n",
      "   \"tune\": false,\n",
      "   \"action_args\": \"======= action args =======\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 程序在/work/code目录下， 需先加入路径\n",
    "import sys \n",
    "sys.path.append('/home/aistudio/work/code')\n",
    "# fly_paddle是唯一需要直接调用的模块\n",
    "# fly_paddle is the only module required for interactive sessions\n",
    "import fly_paddle as fp\n",
    "\n",
    "# args包括几乎所有需要的参数， 贯穿于几乎所有的程序调用中\n",
    "# args由fp.parse_args2()根据任务初始化, 要用到的任务包括： ‘train', 'validate', 'predict'\n",
    "# args is a structure storing most (if not all) parameters and used for most function calls.\n",
    "# args is initialized by fp.parse_args2(), depending on the specific task, such as \"train\", \"validate\" \"predict\"\n",
    "args, _ = fp.parse_args2('train')\n",
    "print(fp.gwio.json_str(vars(args)))\n",
    "# 注： 根据不同的网络等等需要， args可能包含一些用不到的参数\n",
    "# Attention: some parameters in args may not be used depending on the network etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 两种更新args的方法： 1） args.update(**dict), 2) args.[key] = value\n",
    "# Two main ways to update values in args: 1) args.update(**dict), 2) args.[key] = value\n",
    "args.update(data_dir='work/data', data_name='train', residue_dbn=True, residue_extra=True)\n",
    "\n",
    "# 网络参数 （net parameters): \n",
    "# 网络的设计主要考虑了三个支配RNA碱基配对的因素： \n",
    "#    1) 来自于全部序列的排列组合（配分）竞争，用Attention机制来模拟\n",
    "#    2）来自于线性大分子的一维序列限制， 用LSTM结构来模拟\n",
    "#    3）来自于局部紧邻碱基的合作（比如，一个孤立的碱基对极不稳定）， 用1D Convolution来模拟\n",
    "# 所以框架由以上三个模块组成， 并在输入和输出层加了1-3个线性层。除非特意说明， 所有的隐藏层的维度为32.\n",
    "# 训练中发现高维度和深度的网络并不能给出更好的结果！\n",
    "# Three main mechanisms directing RNA base pairing are taken into consideration for the \n",
    "# design of the network architecture. \n",
    "#   1) The combinatorial configurational space of attainable RNA base pairs, approximated by Attention Mechanism\n",
    "#   2) The quasi-1D nature of unbranched, continuous RNA polymers, approximated by LSTM\n",
    "#   3) The cooperativity of neighboring bases for stable base pairing, approximated by 1D Convolution\n",
    "# Hence the neural net comprises of three main building blocks, in addition to linear layers for the input and output. \n",
    "# The dimensions of all hidden layers are 32 unless noted otherwise.\n",
    "# Wider and/or deeper nets gave similar, but no better, performances!\n",
    "args.net='seq2seq_attnlstmconv1d'  # the net name defined in paddle_nets.py\n",
    "# 输入模块由一个线性层组成\n",
    "# The input block is a single linear feedforward layer\n",
    "args.linear_num = 1 # the number of linear feedforward layers\n",
    "# 三大处理模块 (the three main data-crunching blocks)\n",
    "args.attn_num = 1 # the number of transformer encoder layers\n",
    "args.attn_nhead = 2 # the number of heads (2 chosen to capture paired/unpaired states, naively)\n",
    "args.lstm_num = 1 # the number of bidirectional lstm layers\n",
    "args.conv1d_num = 1 # the number of 1D convolution layers\n",
    "# 输出模块由三个线性层组成， 维度分别为32, 32, 2\n",
    "# Three linear layers for the final output, with dimensions of 32, 32, and 2, respectively\n",
    "args.output_dim = [32, 32, 2]\n",
    "# 如果序列被补长到同一长度， 对归一化的影响不清楚， 所以用batch_size=1\n",
    "# If sequences are padded to the same length, such padding may interfere with normalization, hence batch_size=1 \n",
    "args.norm_fn = 'layer' # layer normalization\n",
    "args.batch_size = 1 # 1 is used in consideration of the layer norm above\n",
    "# 最后递交用的损失函数选为softmax+bce, 也可以用 softmax+mse, 结果几乎一样\n",
    "# The submitted results were trained with softmax+bce as the loss function. \n",
    "# Essentially the same results were obtained with softmax+mse\n",
    "args.loss_fn = ['softmax+bce'] # softmax is needed here as the final output has a dimension of 2\n",
    "args.label_tone = 'soft' # soft label for upp\n",
    "args.loss_sqrt = True # sqrt(loss) is only necessary for softmax+mse\n",
    "args.loss_padding = False # exclude padded residues from loss calculation\n",
    "# 需要运行fp.autoconfig_args()来消除参数的不一致性\n",
    "# fp.autoconfig_args() needs to be run to resolve inconsistencies between parameters\n",
    "args = fp.autoconfig_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 14:59:56,265 - INFO - Used net definition: \u001b[0;39;46m/home/aistudio/work/code/paddle_nets.py\u001b[0m\n",
      "2021-06-19 14:59:56,342 - INFO - {'total_params': 36418, 'trainable_params': 36418}\n",
      "2021-06-19 14:59:56,343 - INFO - Optimizer method: adam\n",
      "2021-06-19 14:59:56,343 - INFO -    learning rate: 0.003\n",
      "2021-06-19 14:59:56,344 - INFO -     lr_scheduler: reduced\n",
      "2021-06-19 14:59:56,344 - INFO -     weight decay: none\n",
      "2021-06-19 14:59:56,344 - INFO -          l1decay: 0.0001\n",
      "2021-06-19 14:59:56,345 - INFO -          l2decay: 0.0001\n",
      "2021-06-19 14:59:56,345 - INFO - Getting loss function: ['softmax+bce']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "      Layer (type)                          Input Shape                                  Output Shape                   Param #    \n",
      "=====================================================================================================================================\n",
      "   MyEmbeddingLayer-1                      [[2, 512, 10]]                                [2, 512, 10]                      0       \n",
      "        Linear-1                           [[2, 512, 10]]                                [2, 512, 32]                     352      \n",
      "         ReLU-1                            [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-1                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Dropout-1                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "     MyLinearTower-1                       [[2, 512, 10]]                                [2, 512, 32]                      0       \n",
      "    PositionEncoder-1                      [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-2                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Linear-2                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Linear-3                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Linear-4                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Linear-5                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "  MultiHeadAttention-1    [[2, 512, 32], [2, 512, 32], [2, 512, 32], None]               [2, 512, 32]                      0       \n",
      "        Dropout-3                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-3                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Linear-6                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Dropout-2                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "        Linear-7                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Dropout-4                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "TransformerEncoderLayer-1                  [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "  TransformerEncoder-1                  [[2, 512, 32], None]                             [2, 512, 32]                      0       \n",
      "      MyAttnTower-1                        [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "         LSTM-1                            [[2, 512, 32]]                  [[2, 512, 64], [[2, 2, 32], [2, 2, 32]]]     16,896     \n",
      "      MyLSTMTower-1                        [[2, 512, 32]]                                [2, 512, 64]                      0       \n",
      "        Conv1D-1                           [[2, 512, 64]]                                [2, 512, 32]                   10,272     \n",
      "         ReLU-2                            [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-4                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Dropout-5                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "     MyConv1DTower-1                       [[2, 512, 64]]                                [2, 512, 32]                      0       \n",
      "        Linear-8                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "         ReLU-3                            [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-5                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Dropout-6                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "        Linear-9                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "         ReLU-4                            [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-6                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Dropout-7                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "        Linear-10                          [[2, 512, 32]]                                [2, 512, 2]                      66       \n",
      "     MyLinearTower-2                       [[2, 512, 32]]                                [2, 512, 2]                       0       \n",
      "=====================================================================================================================================\n",
      "Total params: 36,418\n",
      "Trainable params: 36,418\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 9.73\n",
      "Params size (MB): 0.14\n",
      "Estimated Total Size (MB): 9.91\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 建立和检测模型 （Get and inspect the model）\n",
    "model = fp.get_model(args)\n",
    "# 注： 最后的输出矩阵的维度为[N, L, 2]\n",
    "# Note: the shape of the output is [N, L, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 14:59:56,350 - INFO - Loading data: work/data/train.pkl\n",
      "2021-06-19 14:59:56,396 - INFO -    # of data: 5000,  max seqlen: 500, user seq_length: [0, 512, -1]\n",
      "2021-06-19 14:59:56,397 - INFO -  residue fmt: vector, nn: 0, dbn: True, attr: False, genre: upp\n",
      "2021-06-19 14:59:56,415 - INFO - Selected 5000 data sets with length range: [0, 512, -1]\n",
      "2021-06-19 15:00:00,812 - INFO - Processing upp data...\n"
     ]
    }
   ],
   "source": [
    "# 读取数据. 提供的数据被转换成了一个dict, 存储为pickle文件. \n",
    "# 输入矩阵中最后两列的数据为linear_partition_c和linear_partition_v的预测结果\n",
    "# Load data. The provided data are transfomed into a dict saved into a pickle file\n",
    "# the last two columns in the input matrix are the predictions of linear_partition_c and linear_partition_v\n",
    "midata = fp.get_midata(args)\n",
    "train_data, valid_data = fp.train_test_split(midata, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:00:01,050 - INFO - Training, data size: 4500\n",
      "2021-06-19 15:00:01,052 - INFO -          batch size: 1\n",
      "2021-06-19 15:00:01,053 - INFO -             shuffle: True\n",
      "2021-06-19 15:00:01,147 - INFO -        # of batches: 4500\n",
      "2021-06-19 15:00:01,247 - INFO -      recap interval: 151\n",
      "2021-06-19 15:00:01,247 - INFO -   validate interval: 450\n",
      "2021-06-19 15:00:01,248 - INFO -         # of epochs: 21\n",
      "2021-06-19 15:00:01,249 - INFO -        loss padding: False\n",
      "2021-06-19 15:00:02,194 - INFO - Epoch/batch: 0/   0, ibatch:    0, loss: \u001b[0;36m0.8586\u001b[0m, std: 0.6104\n",
      "2021-06-19 15:00:09,460 - INFO - loss: \u001b[0;32m0.8437\u001b[0m, std: 0.5652\n",
      "2021-06-19 15:00:14,780 - INFO - Epoch/batch: 0/ 151, ibatch:  151, loss: \u001b[0;36m0.7099\u001b[0m, std: 0.6014\n",
      "2021-06-19 15:00:20,406 - INFO - Epoch/batch: 0/ 302, ibatch:  302, loss: \u001b[0;36m0.6127\u001b[0m, std: 0.6784\n",
      "2021-06-19 15:00:35,549 - INFO - loss: \u001b[0;32m0.5637\u001b[0m, std: 0.6327\n",
      "2021-06-19 15:00:35,566 - INFO - Saved model states in: earlystop_0.5637\n",
      "2021-06-19 15:00:35,569 - INFO - Saved net python code: earlystop_0.5637/paddle_nets.py\n",
      "2021-06-19 15:00:35,578 - INFO - Saved best model: earlystop_0.5637\n",
      "2021-06-19 15:00:35,691 - INFO - Epoch/batch: 0/ 453, ibatch:  453, loss: \u001b[0;36m0.6016\u001b[0m, std: 0.6758\n",
      "2021-06-19 15:00:41,194 - INFO - Epoch/batch: 0/ 604, ibatch:  604, loss: \u001b[0;36m0.5824\u001b[0m, std: 0.6623\n",
      "2021-06-19 15:00:46,648 - INFO - Epoch/batch: 0/ 755, ibatch:  755, loss: \u001b[0;36m0.5603\u001b[0m, std: 0.6526\n",
      "2021-06-19 15:01:01,320 - INFO - loss: \u001b[0;32m0.5556\u001b[0m, std: 0.5770\n",
      "2021-06-19 15:01:01,336 - INFO - Saved model states in: earlystop_0.5556\n",
      "2021-06-19 15:01:01,338 - INFO - Saved net python code: earlystop_0.5556/paddle_nets.py\n",
      "2021-06-19 15:01:01,345 - INFO - Saved best model: earlystop_0.5556\n",
      "2021-06-19 15:01:01,345 - INFO - Removing earlystop model: earlystop_0.5637\n",
      "2021-06-19 15:01:01,542 - INFO - Epoch/batch: 0/ 906, ibatch:  906, loss: \u001b[0;36m0.5674\u001b[0m, std: 0.6445\n",
      "2021-06-19 15:01:07,233 - INFO - Epoch/batch: 0/1057, ibatch: 1057, loss: \u001b[0;36m0.5760\u001b[0m, std: 0.6596\n",
      "2021-06-19 15:01:12,951 - INFO - Epoch/batch: 0/1208, ibatch: 1208, loss: \u001b[0;36m0.5763\u001b[0m, std: 0.6520\n",
      "2021-06-19 15:01:28,228 - INFO - loss: \u001b[0;32m0.5459\u001b[0m, std: 0.5890\n",
      "2021-06-19 15:01:28,244 - INFO - Saved model states in: earlystop_0.5459\n",
      "2021-06-19 15:01:28,246 - INFO - Saved net python code: earlystop_0.5459/paddle_nets.py\n",
      "2021-06-19 15:01:28,252 - INFO - Saved best model: earlystop_0.5459\n",
      "2021-06-19 15:01:28,253 - INFO - Removing earlystop model: earlystop_0.5556\n",
      "2021-06-19 15:01:28,596 - INFO - Epoch/batch: 0/1359, ibatch: 1359, loss: \u001b[0;36m0.5706\u001b[0m, std: 0.6498\n",
      "2021-06-19 15:01:33,864 - INFO - Epoch/batch: 0/1510, ibatch: 1510, loss: \u001b[0;36m0.5629\u001b[0m, std: 0.6458\n",
      "2021-06-19 15:01:39,016 - INFO - Epoch/batch: 0/1661, ibatch: 1661, loss: \u001b[0;36m0.5755\u001b[0m, std: 0.6441\n",
      "2021-06-19 15:01:53,396 - INFO - loss: \u001b[0;32m0.5452\u001b[0m, std: 0.5948\n",
      "2021-06-19 15:01:53,412 - INFO - Saved model states in: earlystop_0.5452\n",
      "2021-06-19 15:01:53,414 - INFO - Saved net python code: earlystop_0.5452/paddle_nets.py\n",
      "2021-06-19 15:01:53,420 - INFO - Saved best model: earlystop_0.5452\n",
      "2021-06-19 15:01:53,421 - INFO - Removing earlystop model: earlystop_0.5459\n",
      "2021-06-19 15:01:53,922 - INFO - Epoch/batch: 0/1812, ibatch: 1812, loss: \u001b[0;36m0.5667\u001b[0m, std: 0.6461\n",
      "2021-06-19 15:01:59,786 - INFO - Epoch/batch: 0/1963, ibatch: 1963, loss: \u001b[0;36m0.5657\u001b[0m, std: 0.6385\n",
      "2021-06-19 15:02:05,725 - INFO - Epoch/batch: 0/2114, ibatch: 2114, loss: \u001b[0;36m0.5635\u001b[0m, std: 0.6432\n",
      "2021-06-19 15:02:20,537 - INFO - loss: \u001b[0;32m0.5405\u001b[0m, std: 0.6257\n",
      "2021-06-19 15:02:20,553 - INFO - Saved model states in: earlystop_0.5405\n",
      "2021-06-19 15:02:20,554 - INFO - Saved net python code: earlystop_0.5405/paddle_nets.py\n",
      "2021-06-19 15:02:20,560 - INFO - Saved best model: earlystop_0.5405\n",
      "2021-06-19 15:02:20,561 - INFO - Removing earlystop model: earlystop_0.5452\n",
      "2021-06-19 15:02:21,163 - INFO - Epoch/batch: 0/2265, ibatch: 2265, loss: \u001b[0;36m0.5772\u001b[0m, std: 0.6563\n",
      "2021-06-19 15:02:26,602 - INFO - Epoch/batch: 0/2416, ibatch: 2416, loss: \u001b[0;36m0.5613\u001b[0m, std: 0.6339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: ReduceOnPlateau set learning rate to 0.0027.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:02:32,076 - INFO - Epoch/batch: 0/2567, ibatch: 2567, loss: \u001b[0;36m0.5395\u001b[0m, std: 0.6171\n",
      "2021-06-19 15:02:46,778 - INFO - loss: \u001b[0;32m0.5396\u001b[0m, std: 0.5904\n",
      "2021-06-19 15:02:46,793 - INFO - Saved model states in: earlystop_0.5396\n",
      "2021-06-19 15:02:46,795 - INFO - Saved net python code: earlystop_0.5396/paddle_nets.py\n",
      "2021-06-19 15:02:46,801 - INFO - Saved best model: earlystop_0.5396\n",
      "2021-06-19 15:02:46,802 - INFO - Removing earlystop model: earlystop_0.5405\n",
      "2021-06-19 15:02:47,401 - INFO - Epoch/batch: 0/2718, ibatch: 2718, loss: \u001b[0;36m0.5509\u001b[0m, std: 0.6302\n",
      "2021-06-19 15:02:53,007 - INFO - Epoch/batch: 0/2869, ibatch: 2869, loss: \u001b[0;36m0.5601\u001b[0m, std: 0.6436\n",
      "2021-06-19 15:02:58,712 - INFO - Epoch/batch: 0/3020, ibatch: 3020, loss: \u001b[0;36m0.5330\u001b[0m, std: 0.6192\n",
      "2021-06-19 15:03:13,343 - INFO - loss: \u001b[0;32m0.5425\u001b[0m, std: 0.5666\n",
      "2021-06-19 15:03:14,210 - INFO - Epoch/batch: 0/3171, ibatch: 3171, loss: \u001b[0;36m0.5507\u001b[0m, std: 0.6284\n",
      "2021-06-19 15:03:20,234 - INFO - Epoch/batch: 0/3322, ibatch: 3322, loss: \u001b[0;36m0.5692\u001b[0m, std: 0.6464\n",
      "2021-06-19 15:03:25,626 - INFO - Epoch/batch: 0/3473, ibatch: 3473, loss: \u001b[0;36m0.5563\u001b[0m, std: 0.6233\n",
      "2021-06-19 15:03:39,976 - INFO - loss: \u001b[0;32m0.5345\u001b[0m, std: 0.6104\n",
      "2021-06-19 15:03:39,991 - INFO - Saved model states in: earlystop_0.5345\n",
      "2021-06-19 15:03:39,993 - INFO - Saved net python code: earlystop_0.5345/paddle_nets.py\n",
      "2021-06-19 15:03:39,999 - INFO - Saved best model: earlystop_0.5345\n",
      "2021-06-19 15:03:40,000 - INFO - Removing earlystop model: earlystop_0.5396\n",
      "2021-06-19 15:03:40,791 - INFO - Epoch/batch: 0/3624, ibatch: 3624, loss: \u001b[0;36m0.5604\u001b[0m, std: 0.6348\n",
      "2021-06-19 15:03:46,096 - INFO - Epoch/batch: 0/3775, ibatch: 3775, loss: \u001b[0;36m0.5329\u001b[0m, std: 0.6097\n",
      "2021-06-19 15:03:51,395 - INFO - Epoch/batch: 0/3926, ibatch: 3926, loss: \u001b[0;36m0.5472\u001b[0m, std: 0.6261\n",
      "2021-06-19 15:04:05,268 - INFO - loss: \u001b[0;32m0.5386\u001b[0m, std: 0.6025\n",
      "2021-06-19 15:04:06,274 - INFO - Epoch/batch: 0/4077, ibatch: 4077, loss: \u001b[0;36m0.5596\u001b[0m, std: 0.6268\n",
      "2021-06-19 15:04:11,828 - INFO - Epoch/batch: 0/4228, ibatch: 4228, loss: \u001b[0;36m0.5663\u001b[0m, std: 0.6345\n",
      "2021-06-19 15:04:17,383 - INFO - Epoch/batch: 0/4379, ibatch: 4379, loss: \u001b[0;36m0.5591\u001b[0m, std: 0.6332\n",
      "2021-06-19 15:04:31,054 - INFO - loss: \u001b[0;32m0.5335\u001b[0m, std: 0.6153\n",
      "2021-06-19 15:04:31,070 - INFO - Saved model states in: earlystop_0.5335\n",
      "2021-06-19 15:04:31,072 - INFO - Saved net python code: earlystop_0.5335/paddle_nets.py\n",
      "2021-06-19 15:04:31,078 - INFO - Saved best model: earlystop_0.5335\n",
      "2021-06-19 15:04:31,079 - INFO - Removing earlystop model: earlystop_0.5345\n",
      "2021-06-19 15:04:31,549 - INFO - Epoch 0 average training loss: \u001b[0;46m0.5689\u001b[0m std: 0.6391\n",
      "2021-06-19 15:04:31,553 - INFO - Epoch 0 average validate loss: \u001b[0;46m0.5712\u001b[0m std: 0.5972\n",
      "2021-06-19 15:04:33,398 - INFO - Epoch/batch: 1/   0, ibatch: 4500, loss: \u001b[0;36m0.5457\u001b[0m, std: 0.6112\n",
      "2021-06-19 15:04:42,860 - INFO - loss: \u001b[0;32m0.5339\u001b[0m, std: 0.6196\n",
      "2021-06-19 15:04:48,560 - INFO - Epoch/batch: 1/ 151, ibatch: 4651, loss: \u001b[0;36m0.5441\u001b[0m, std: 0.6223\n",
      "2021-06-19 15:04:54,384 - INFO - Epoch/batch: 1/ 302, ibatch: 4802, loss: \u001b[0;36m0.5635\u001b[0m, std: 0.6412\n",
      "2021-06-19 15:05:09,427 - INFO - loss: \u001b[0;32m0.5496\u001b[0m, std: 0.5488\n",
      "2021-06-19 15:05:09,527 - INFO - Epoch/batch: 1/ 453, ibatch: 4953, loss: \u001b[0;36m0.5449\u001b[0m, std: 0.6123\n",
      "2021-06-19 15:05:14,848 - INFO - Epoch/batch: 1/ 604, ibatch: 5104, loss: \u001b[0;36m0.5532\u001b[0m, std: 0.6260\n",
      "2021-06-19 15:05:21,670 - INFO - Epoch/batch: 1/ 755, ibatch: 5255, loss: \u001b[0;36m0.5609\u001b[0m, std: 0.6379\n",
      "2021-06-19 15:05:38,043 - INFO - loss: \u001b[0;32m0.5349\u001b[0m, std: 0.5806\n",
      "2021-06-19 15:05:38,304 - INFO - Epoch/batch: 1/ 906, ibatch: 5406, loss: \u001b[0;36m0.5571\u001b[0m, std: 0.6323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: ReduceOnPlateau set learning rate to 0.0024300000000000003.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:05:43,957 - INFO - Epoch/batch: 1/1057, ibatch: 5557, loss: \u001b[0;36m0.5420\u001b[0m, std: 0.6226\n",
      "2021-06-19 15:05:49,224 - INFO - Epoch/batch: 1/1208, ibatch: 5708, loss: \u001b[0;36m0.5465\u001b[0m, std: 0.6181\n",
      "2021-06-19 15:06:03,621 - INFO - loss: \u001b[0;32m0.5407\u001b[0m, std: 0.5627\n",
      "2021-06-19 15:06:03,973 - INFO - Epoch/batch: 1/1359, ibatch: 5859, loss: \u001b[0;36m0.5461\u001b[0m, std: 0.6205\n",
      "2021-06-19 15:06:09,730 - INFO - Epoch/batch: 1/1510, ibatch: 6010, loss: \u001b[0;36m0.5685\u001b[0m, std: 0.6423\n",
      "2021-06-19 15:06:15,288 - INFO - Epoch/batch: 1/1661, ibatch: 6161, loss: \u001b[0;36m0.5475\u001b[0m, std: 0.6219\n",
      "2021-06-19 15:06:30,376 - INFO - loss: \u001b[0;32m0.5319\u001b[0m, std: 0.5979\n",
      "2021-06-19 15:06:30,394 - INFO - Saved model states in: earlystop_0.5319\n",
      "2021-06-19 15:06:30,395 - INFO - Saved net python code: earlystop_0.5319/paddle_nets.py\n",
      "2021-06-19 15:06:30,403 - INFO - Saved best model: earlystop_0.5319\n",
      "2021-06-19 15:06:30,404 - INFO - Removing earlystop model: earlystop_0.5335\n",
      "2021-06-19 15:06:30,868 - INFO - Epoch/batch: 1/1812, ibatch: 6312, loss: \u001b[0;36m0.5520\u001b[0m, std: 0.6307\n",
      "2021-06-19 15:06:36,836 - INFO - Epoch/batch: 1/1963, ibatch: 6463, loss: \u001b[0;36m0.5573\u001b[0m, std: 0.6320\n",
      "2021-06-19 15:06:42,528 - INFO - Epoch/batch: 1/2114, ibatch: 6614, loss: \u001b[0;36m0.5583\u001b[0m, std: 0.6334\n",
      "2021-06-19 15:06:56,619 - INFO - loss: \u001b[0;32m0.5342\u001b[0m, std: 0.6472\n",
      "2021-06-19 15:06:57,237 - INFO - Epoch/batch: 1/2265, ibatch: 6765, loss: \u001b[0;36m0.5306\u001b[0m, std: 0.6220\n",
      "2021-06-19 15:07:03,066 - INFO - Epoch/batch: 1/2416, ibatch: 6916, loss: \u001b[0;36m0.5426\u001b[0m, std: 0.6198\n",
      "2021-06-19 15:07:08,852 - INFO - Epoch/batch: 1/2567, ibatch: 7067, loss: \u001b[0;36m0.5443\u001b[0m, std: 0.6219\n",
      "2021-06-19 15:07:23,523 - INFO - loss: \u001b[0;32m0.5346\u001b[0m, std: 0.5835\n",
      "2021-06-19 15:07:24,173 - INFO - Epoch/batch: 1/2718, ibatch: 7218, loss: \u001b[0;36m0.5464\u001b[0m, std: 0.6205\n",
      "2021-06-19 15:07:29,735 - INFO - Epoch/batch: 1/2869, ibatch: 7369, loss: \u001b[0;36m0.5312\u001b[0m, std: 0.6196\n",
      "2021-06-19 15:07:35,123 - INFO - Epoch/batch: 1/3020, ibatch: 7520, loss: \u001b[0;36m0.5406\u001b[0m, std: 0.6195\n",
      "2021-06-19 15:07:50,007 - INFO - loss: \u001b[0;32m0.5317\u001b[0m, std: 0.6136\n",
      "2021-06-19 15:07:50,028 - INFO - Saved model states in: earlystop_0.5317\n",
      "2021-06-19 15:07:50,030 - INFO - Saved net python code: earlystop_0.5317/paddle_nets.py\n",
      "2021-06-19 15:07:50,040 - INFO - Saved best model: earlystop_0.5317\n",
      "2021-06-19 15:07:50,040 - INFO - Removing earlystop model: earlystop_0.5319\n",
      "2021-06-19 15:07:50,886 - INFO - Epoch/batch: 1/3171, ibatch: 7671, loss: \u001b[0;36m0.5517\u001b[0m, std: 0.6320\n",
      "2021-06-19 15:07:56,560 - INFO - Epoch/batch: 1/3322, ibatch: 7822, loss: \u001b[0;36m0.5390\u001b[0m, std: 0.6149\n",
      "2021-06-19 15:08:02,407 - INFO - Epoch/batch: 1/3473, ibatch: 7973, loss: \u001b[0;36m0.5455\u001b[0m, std: 0.6222\n",
      "2021-06-19 15:08:16,218 - INFO - loss: \u001b[0;32m0.5285\u001b[0m, std: 0.6160\n",
      "2021-06-19 15:08:16,233 - INFO - Saved model states in: earlystop_0.5285\n",
      "2021-06-19 15:08:16,235 - INFO - Saved net python code: earlystop_0.5285/paddle_nets.py\n",
      "2021-06-19 15:08:16,241 - INFO - Saved best model: earlystop_0.5285\n",
      "2021-06-19 15:08:16,242 - INFO - Removing earlystop model: earlystop_0.5317\n",
      "2021-06-19 15:08:17,166 - INFO - Epoch/batch: 1/3624, ibatch: 8124, loss: \u001b[0;36m0.5310\u001b[0m, std: 0.6139\n",
      "2021-06-19 15:08:23,024 - INFO - Epoch/batch: 1/3775, ibatch: 8275, loss: \u001b[0;36m0.5348\u001b[0m, std: 0.6164\n",
      "2021-06-19 15:08:28,849 - INFO - Epoch/batch: 1/3926, ibatch: 8426, loss: \u001b[0;36m0.5453\u001b[0m, std: 0.6272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: ReduceOnPlateau set learning rate to 0.002187.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:08:42,808 - INFO - loss: \u001b[0;32m0.5293\u001b[0m, std: 0.6106\n",
      "2021-06-19 15:08:43,817 - INFO - Epoch/batch: 1/4077, ibatch: 8577, loss: \u001b[0;36m0.5568\u001b[0m, std: 0.6315\n",
      "2021-06-19 15:08:49,624 - INFO - Epoch/batch: 1/4228, ibatch: 8728, loss: \u001b[0;36m0.5476\u001b[0m, std: 0.6238\n",
      "2021-06-19 15:08:55,232 - INFO - Epoch/batch: 1/4379, ibatch: 8879, loss: \u001b[0;36m0.5436\u001b[0m, std: 0.6225\n",
      "2021-06-19 15:09:09,056 - INFO - loss: \u001b[0;32m0.5275\u001b[0m, std: 0.5987\n",
      "2021-06-19 15:09:09,072 - INFO - Saved model states in: earlystop_0.5275\n",
      "2021-06-19 15:09:09,073 - INFO - Saved net python code: earlystop_0.5275/paddle_nets.py\n",
      "2021-06-19 15:09:09,079 - INFO - Saved best model: earlystop_0.5275\n",
      "2021-06-19 15:09:09,080 - INFO - Removing earlystop model: earlystop_0.5285\n",
      "2021-06-19 15:09:10,249 - INFO - Epoch 1 average training loss: \u001b[0;46m0.5474\u001b[0m std: 0.6248\n",
      "2021-06-19 15:09:10,253 - INFO - Epoch 1 average validate loss: \u001b[0;46m0.5343\u001b[0m std: 0.5981\n",
      "2021-06-19 15:09:12,010 - INFO - Epoch/batch: 2/   0, ibatch: 9000, loss: \u001b[0;36m0.5496\u001b[0m, std: 0.6226\n",
      "2021-06-19 15:09:21,680 - INFO - loss: \u001b[0;32m0.5274\u001b[0m, std: 0.5976\n",
      "2021-06-19 15:09:21,701 - INFO - Saved model states in: earlystop_0.5274\n",
      "2021-06-19 15:09:21,702 - INFO - Saved net python code: earlystop_0.5274/paddle_nets.py\n",
      "2021-06-19 15:09:21,711 - INFO - Saved best model: earlystop_0.5274\n",
      "2021-06-19 15:09:21,711 - INFO - Removing earlystop model: earlystop_0.5275\n",
      "2021-06-19 15:09:27,666 - INFO - Epoch/batch: 2/ 151, ibatch: 9151, loss: \u001b[0;36m0.5575\u001b[0m, std: 0.6290\n",
      "2021-06-19 15:09:33,549 - INFO - Epoch/batch: 2/ 302, ibatch: 9302, loss: \u001b[0;36m0.5391\u001b[0m, std: 0.6219\n",
      "2021-06-19 15:09:49,177 - INFO - loss: \u001b[0;32m0.5312\u001b[0m, std: 0.6227\n",
      "2021-06-19 15:09:49,302 - INFO - Epoch/batch: 2/ 453, ibatch: 9453, loss: \u001b[0;36m0.5421\u001b[0m, std: 0.6246\n",
      "2021-06-19 15:09:54,921 - INFO - Epoch/batch: 2/ 604, ibatch: 9604, loss: \u001b[0;36m0.5497\u001b[0m, std: 0.6268\n",
      "2021-06-19 15:10:00,765 - INFO - Epoch/batch: 2/ 755, ibatch: 9755, loss: \u001b[0;36m0.5295\u001b[0m, std: 0.6214\n",
      "2021-06-19 15:10:15,101 - INFO - loss: \u001b[0;32m0.5279\u001b[0m, std: 0.6196\n",
      "2021-06-19 15:10:15,341 - INFO - Epoch/batch: 2/ 906, ibatch: 9906, loss: \u001b[0;36m0.5395\u001b[0m, std: 0.6179\n",
      "2021-06-19 15:10:20,798 - INFO - Epoch/batch: 2/1057, ibatch: 10057, loss: \u001b[0;36m0.5491\u001b[0m, std: 0.6253\n",
      "2021-06-19 15:10:26,389 - INFO - Epoch/batch: 2/1208, ibatch: 10208, loss: \u001b[0;36m0.5555\u001b[0m, std: 0.6315\n",
      "2021-06-19 15:10:40,651 - INFO - loss: \u001b[0;32m0.5277\u001b[0m, std: 0.5894\n",
      "2021-06-19 15:10:41,028 - INFO - Epoch/batch: 2/1359, ibatch: 10359, loss: \u001b[0;36m0.5407\u001b[0m, std: 0.6129\n",
      "2021-06-19 15:10:46,683 - INFO - Epoch/batch: 2/1510, ibatch: 10510, loss: \u001b[0;36m0.5387\u001b[0m, std: 0.6157\n",
      "2021-06-19 15:10:52,476 - INFO - Epoch/batch: 2/1661, ibatch: 10661, loss: \u001b[0;36m0.5365\u001b[0m, std: 0.6148\n",
      "2021-06-19 15:11:07,809 - INFO - loss: \u001b[0;32m0.5258\u001b[0m, std: 0.5989\n",
      "2021-06-19 15:11:07,825 - INFO - Saved model states in: earlystop_0.5258\n",
      "2021-06-19 15:11:07,826 - INFO - Saved net python code: earlystop_0.5258/paddle_nets.py\n",
      "2021-06-19 15:11:07,833 - INFO - Saved best model: earlystop_0.5258\n",
      "2021-06-19 15:11:07,834 - INFO - Removing earlystop model: earlystop_0.5274\n",
      "2021-06-19 15:11:08,299 - INFO - Epoch/batch: 2/1812, ibatch: 10812, loss: \u001b[0;36m0.5418\u001b[0m, std: 0.6207\n",
      "2021-06-19 15:11:14,289 - INFO - Epoch/batch: 2/1963, ibatch: 10963, loss: \u001b[0;36m0.5534\u001b[0m, std: 0.6332\n",
      "2021-06-19 15:11:19,737 - INFO - Epoch/batch: 2/2114, ibatch: 11114, loss: \u001b[0;36m0.5238\u001b[0m, std: 0.6099\n",
      "2021-06-19 15:11:34,421 - INFO - loss: \u001b[0;32m0.5298\u001b[0m, std: 0.5883\n",
      "2021-06-19 15:11:34,940 - INFO - Epoch/batch: 2/2265, ibatch: 11265, loss: \u001b[0;36m0.5540\u001b[0m, std: 0.6218\n",
      "2021-06-19 15:11:40,567 - INFO - Epoch/batch: 2/2416, ibatch: 11416, loss: \u001b[0;36m0.5480\u001b[0m, std: 0.6235\n",
      "2021-06-19 15:11:46,061 - INFO - Epoch/batch: 2/2567, ibatch: 11567, loss: \u001b[0;36m0.5479\u001b[0m, std: 0.6279\n",
      "2021-06-19 15:12:00,023 - INFO - loss: \u001b[0;32m0.5302\u001b[0m, std: 0.6419\n",
      "2021-06-19 15:12:00,728 - INFO - Epoch/batch: 2/2718, ibatch: 11718, loss: \u001b[0;36m0.5387\u001b[0m, std: 0.6178\n",
      "2021-06-19 15:12:06,448 - INFO - Epoch/batch: 2/2869, ibatch: 11869, loss: \u001b[0;36m0.5251\u001b[0m, std: 0.6044\n",
      "2021-06-19 15:12:12,016 - INFO - Epoch/batch: 2/3020, ibatch: 12020, loss: \u001b[0;36m0.5531\u001b[0m, std: 0.6241\n",
      "2021-06-19 15:12:26,420 - INFO - loss: \u001b[0;32m0.5273\u001b[0m, std: 0.6175\n",
      "2021-06-19 15:12:27,204 - INFO - Epoch/batch: 2/3171, ibatch: 12171, loss: \u001b[0;36m0.5550\u001b[0m, std: 0.6322\n",
      "2021-06-19 15:12:32,849 - INFO - Epoch/batch: 2/3322, ibatch: 12322, loss: \u001b[0;36m0.5300\u001b[0m, std: 0.6095\n",
      "2021-06-19 15:12:38,026 - INFO - Epoch/batch: 2/3473, ibatch: 12473, loss: \u001b[0;36m0.5356\u001b[0m, std: 0.6078\n",
      "2021-06-19 15:12:52,240 - INFO - loss: \u001b[0;32m0.5259\u001b[0m, std: 0.6082\n",
      "2021-06-19 15:12:53,200 - INFO - Epoch/batch: 2/3624, ibatch: 12624, loss: \u001b[0;36m0.5544\u001b[0m, std: 0.6304\n",
      "2021-06-19 15:12:58,335 - INFO - Epoch/batch: 2/3775, ibatch: 12775, loss: \u001b[0;36m0.5309\u001b[0m, std: 0.6127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86: ReduceOnPlateau set learning rate to 0.0019683.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:13:03,935 - INFO - Epoch/batch: 2/3926, ibatch: 12926, loss: \u001b[0;36m0.5538\u001b[0m, std: 0.6258\n",
      "2021-06-19 15:13:18,023 - INFO - loss: \u001b[0;32m0.5267\u001b[0m, std: 0.5894\n",
      "2021-06-19 15:13:19,181 - INFO - Epoch/batch: 2/4077, ibatch: 13077, loss: \u001b[0;36m0.5393\u001b[0m, std: 0.6188\n",
      "2021-06-19 15:13:25,268 - INFO - Epoch/batch: 2/4228, ibatch: 13228, loss: \u001b[0;36m0.5482\u001b[0m, std: 0.6231\n",
      "2021-06-19 15:13:30,898 - INFO - Epoch/batch: 2/4379, ibatch: 13379, loss: \u001b[0;36m0.5314\u001b[0m, std: 0.6148\n",
      "2021-06-19 15:13:45,171 - INFO - loss: \u001b[0;32m0.5253\u001b[0m, std: 0.6054\n",
      "2021-06-19 15:13:45,187 - INFO - Saved model states in: earlystop_0.5253\n",
      "2021-06-19 15:13:45,188 - INFO - Saved net python code: earlystop_0.5253/paddle_nets.py\n",
      "2021-06-19 15:13:45,194 - INFO - Saved best model: earlystop_0.5253\n",
      "2021-06-19 15:13:45,195 - INFO - Removing earlystop model: earlystop_0.5258\n",
      "2021-06-19 15:13:46,349 - INFO - Epoch 2 average training loss: \u001b[0;46m0.5422\u001b[0m std: 0.6204\n",
      "2021-06-19 15:13:46,354 - INFO - Epoch 2 average validate loss: \u001b[0;46m0.5277\u001b[0m std: 0.6072\n",
      "2021-06-19 15:13:48,108 - INFO - Epoch/batch: 3/   0, ibatch: 13500, loss: \u001b[0;36m0.5156\u001b[0m, std: 0.6062\n",
      "2021-06-19 15:13:57,473 - INFO - loss: \u001b[0;32m0.5253\u001b[0m, std: 0.6063\n",
      "2021-06-19 15:13:57,494 - INFO - Saved model states in: earlystop_0.5253.1\n",
      "2021-06-19 15:13:57,496 - INFO - Saved net python code: earlystop_0.5253.1/paddle_nets.py\n",
      "2021-06-19 15:13:57,504 - INFO - Saved best model: earlystop_0.5253.1\n",
      "2021-06-19 15:13:57,505 - INFO - Removing earlystop model: earlystop_0.5253\n",
      "2021-06-19 15:14:02,981 - INFO - Epoch/batch: 3/ 151, ibatch: 13651, loss: \u001b[0;36m0.5419\u001b[0m, std: 0.6164\n",
      "2021-06-19 15:14:08,642 - INFO - Epoch/batch: 3/ 302, ibatch: 13802, loss: \u001b[0;36m0.5563\u001b[0m, std: 0.6348\n",
      "2021-06-19 15:14:23,605 - INFO - loss: \u001b[0;32m0.5270\u001b[0m, std: 0.5985\n",
      "2021-06-19 15:14:23,710 - INFO - Epoch/batch: 3/ 453, ibatch: 13953, loss: \u001b[0;36m0.5457\u001b[0m, std: 0.6199\n",
      "2021-06-19 15:14:29,584 - INFO - Epoch/batch: 3/ 604, ibatch: 14104, loss: \u001b[0;36m0.5358\u001b[0m, std: 0.6201\n",
      "2021-06-19 15:14:35,305 - INFO - Epoch/batch: 3/ 755, ibatch: 14255, loss: \u001b[0;36m0.5232\u001b[0m, std: 0.6047\n",
      "2021-06-19 15:14:49,954 - INFO - loss: \u001b[0;32m0.5284\u001b[0m, std: 0.6292\n",
      "2021-06-19 15:14:50,151 - INFO - Epoch/batch: 3/ 906, ibatch: 14406, loss: \u001b[0;36m0.5298\u001b[0m, std: 0.6084\n",
      "2021-06-19 15:14:55,824 - INFO - Epoch/batch: 3/1057, ibatch: 14557, loss: \u001b[0;36m0.5497\u001b[0m, std: 0.6306\n",
      "2021-06-19 15:15:01,211 - INFO - Epoch/batch: 3/1208, ibatch: 14708, loss: \u001b[0;36m0.5463\u001b[0m, std: 0.6200\n",
      "2021-06-19 15:15:16,352 - INFO - loss: \u001b[0;32m0.5281\u001b[0m, std: 0.5870\n",
      "2021-06-19 15:15:16,690 - INFO - Epoch/batch: 3/1359, ibatch: 14859, loss: \u001b[0;36m0.5503\u001b[0m, std: 0.6226\n",
      "2021-06-19 15:15:22,007 - INFO - Epoch/batch: 3/1510, ibatch: 15010, loss: \u001b[0;36m0.5483\u001b[0m, std: 0.6209\n",
      "2021-06-19 15:15:27,573 - INFO - Epoch/batch: 3/1661, ibatch: 15161, loss: \u001b[0;36m0.5240\u001b[0m, std: 0.6043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102: ReduceOnPlateau set learning rate to 0.00177147.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:15:42,791 - INFO - loss: \u001b[0;32m0.5272\u001b[0m, std: 0.6243\n",
      "2021-06-19 15:15:43,362 - INFO - Epoch/batch: 3/1812, ibatch: 15312, loss: \u001b[0;36m0.5460\u001b[0m, std: 0.6232\n",
      "2021-06-19 15:15:49,215 - INFO - Epoch/batch: 3/1963, ibatch: 15463, loss: \u001b[0;36m0.5499\u001b[0m, std: 0.6293\n",
      "2021-06-19 15:15:54,573 - INFO - Epoch/batch: 3/2114, ibatch: 15614, loss: \u001b[0;36m0.5326\u001b[0m, std: 0.6064\n",
      "2021-06-19 15:16:09,042 - INFO - loss: \u001b[0;32m0.5248\u001b[0m, std: 0.6010\n",
      "2021-06-19 15:16:09,057 - INFO - Saved model states in: earlystop_0.5248\n",
      "2021-06-19 15:16:09,059 - INFO - Saved net python code: earlystop_0.5248/paddle_nets.py\n",
      "2021-06-19 15:16:09,066 - INFO - Saved best model: earlystop_0.5248\n",
      "2021-06-19 15:16:09,066 - INFO - Removing earlystop model: earlystop_0.5253.1\n",
      "2021-06-19 15:16:09,658 - INFO - Epoch/batch: 3/2265, ibatch: 15765, loss: \u001b[0;36m0.5552\u001b[0m, std: 0.6325\n",
      "2021-06-19 15:16:15,165 - INFO - Epoch/batch: 3/2416, ibatch: 15916, loss: \u001b[0;36m0.5467\u001b[0m, std: 0.6265\n",
      "2021-06-19 15:16:21,085 - INFO - Epoch/batch: 3/2567, ibatch: 16067, loss: \u001b[0;36m0.5227\u001b[0m, std: 0.6097\n",
      "2021-06-19 15:16:35,960 - INFO - loss: \u001b[0;32m0.5244\u001b[0m, std: 0.5928\n",
      "2021-06-19 15:16:35,975 - INFO - Saved model states in: earlystop_0.5244\n",
      "2021-06-19 15:16:35,976 - INFO - Saved net python code: earlystop_0.5244/paddle_nets.py\n",
      "2021-06-19 15:16:35,983 - INFO - Saved best model: earlystop_0.5244\n",
      "2021-06-19 15:16:35,983 - INFO - Removing earlystop model: earlystop_0.5248\n",
      "2021-06-19 15:16:36,705 - INFO - Epoch/batch: 3/2718, ibatch: 16218, loss: \u001b[0;36m0.5479\u001b[0m, std: 0.6259\n",
      "2021-06-19 15:16:42,597 - INFO - Epoch/batch: 3/2869, ibatch: 16369, loss: \u001b[0;36m0.5450\u001b[0m, std: 0.6244\n",
      "2021-06-19 15:16:48,221 - INFO - Epoch/batch: 3/3020, ibatch: 16520, loss: \u001b[0;36m0.5249\u001b[0m, std: 0.6069\n",
      "2021-06-19 15:17:03,124 - INFO - loss: \u001b[0;32m0.5258\u001b[0m, std: 0.6066\n",
      "2021-06-19 15:17:04,019 - INFO - Epoch/batch: 3/3171, ibatch: 16671, loss: \u001b[0;36m0.5353\u001b[0m, std: 0.6185\n",
      "2021-06-19 15:17:09,842 - INFO - Epoch/batch: 3/3322, ibatch: 16822, loss: \u001b[0;36m0.5439\u001b[0m, std: 0.6228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113: ReduceOnPlateau set learning rate to 0.0015943230000000001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:17:15,447 - INFO - Epoch/batch: 3/3473, ibatch: 16973, loss: \u001b[0;36m0.5464\u001b[0m, std: 0.6125\n",
      "2021-06-19 15:17:30,005 - INFO - loss: \u001b[0;32m0.5300\u001b[0m, std: 0.5735\n",
      "2021-06-19 15:17:31,094 - INFO - Epoch/batch: 3/3624, ibatch: 17124, loss: \u001b[0;36m0.5345\u001b[0m, std: 0.6083\n",
      "2021-06-19 15:17:36,830 - INFO - Epoch/batch: 3/3775, ibatch: 17275, loss: \u001b[0;36m0.5337\u001b[0m, std: 0.6154\n",
      "2021-06-19 15:17:42,728 - INFO - Epoch/batch: 3/3926, ibatch: 17426, loss: \u001b[0;36m0.5400\u001b[0m, std: 0.6252\n",
      "2021-06-19 15:17:57,692 - INFO - loss: \u001b[0;32m0.5280\u001b[0m, std: 0.6410\n",
      "2021-06-19 15:17:58,890 - INFO - Epoch/batch: 3/4077, ibatch: 17577, loss: \u001b[0;36m0.5243\u001b[0m, std: 0.6102\n",
      "2021-06-19 15:18:04,822 - INFO - Epoch/batch: 3/4228, ibatch: 17728, loss: \u001b[0;36m0.5378\u001b[0m, std: 0.6183\n",
      "2021-06-19 15:18:10,659 - INFO - Epoch/batch: 3/4379, ibatch: 17879, loss: \u001b[0;36m0.5223\u001b[0m, std: 0.6069\n",
      "2021-06-19 15:18:24,630 - INFO - loss: \u001b[0;32m0.5276\u001b[0m, std: 0.5737\n",
      "2021-06-19 15:18:25,820 - INFO - Epoch 3 average training loss: \u001b[0;46m0.5396\u001b[0m std: 0.6181\n",
      "2021-06-19 15:18:25,824 - INFO - Epoch 3 average validate loss: \u001b[0;46m0.5270\u001b[0m std: 0.6031\n",
      "2021-06-19 15:18:27,593 - INFO - Epoch/batch: 4/   0, ibatch: 18000, loss: \u001b[0;36m0.5504\u001b[0m, std: 0.6196\n",
      "2021-06-19 15:18:37,177 - INFO - loss: \u001b[0;32m0.5267\u001b[0m, std: 0.5766\n",
      "2021-06-19 15:18:43,018 - INFO - Epoch/batch: 4/ 151, ibatch: 18151, loss: \u001b[0;36m0.5442\u001b[0m, std: 0.6184\n",
      "2021-06-19 15:18:48,573 - INFO - Epoch/batch: 4/ 302, ibatch: 18302, loss: \u001b[0;36m0.5462\u001b[0m, std: 0.6202\n",
      "2021-06-19 15:19:03,757 - INFO - loss: \u001b[0;32m0.5307\u001b[0m, std: 0.5665\n",
      "2021-06-19 15:19:03,890 - INFO - Epoch/batch: 4/ 453, ibatch: 18453, loss: \u001b[0;36m0.5394\u001b[0m, std: 0.6181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124: ReduceOnPlateau set learning rate to 0.0014348907.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:19:09,816 - INFO - Epoch/batch: 4/ 604, ibatch: 18604, loss: \u001b[0;36m0.5367\u001b[0m, std: 0.6108\n",
      "2021-06-19 15:19:15,567 - INFO - Epoch/batch: 4/ 755, ibatch: 18755, loss: \u001b[0;36m0.5389\u001b[0m, std: 0.6166\n",
      "2021-06-19 15:19:31,220 - INFO - loss: \u001b[0;32m0.5325\u001b[0m, std: 0.5757\n",
      "2021-06-19 15:19:31,419 - INFO - Epoch/batch: 4/ 906, ibatch: 18906, loss: \u001b[0;36m0.5667\u001b[0m, std: 0.6413\n",
      "2021-06-19 15:19:37,137 - INFO - Epoch/batch: 4/1057, ibatch: 19057, loss: \u001b[0;36m0.5174\u001b[0m, std: 0.6039\n",
      "2021-06-19 15:19:42,530 - INFO - Epoch/batch: 4/1208, ibatch: 19208, loss: \u001b[0;36m0.5277\u001b[0m, std: 0.6104\n",
      "2021-06-19 15:19:57,807 - INFO - loss: \u001b[0;32m0.5226\u001b[0m, std: 0.5973\n",
      "2021-06-19 15:19:57,829 - INFO - Saved model states in: earlystop_0.5226\n",
      "2021-06-19 15:19:57,830 - INFO - Saved net python code: earlystop_0.5226/paddle_nets.py\n",
      "2021-06-19 15:19:57,842 - INFO - Saved best model: earlystop_0.5226\n",
      "2021-06-19 15:19:57,847 - INFO - Removing earlystop model: earlystop_0.5244\n",
      "2021-06-19 15:19:58,250 - INFO - Epoch/batch: 4/1359, ibatch: 19359, loss: \u001b[0;36m0.5376\u001b[0m, std: 0.6118\n",
      "2021-06-19 15:20:03,989 - INFO - Epoch/batch: 4/1510, ibatch: 19510, loss: \u001b[0;36m0.5229\u001b[0m, std: 0.6143\n",
      "2021-06-19 15:20:09,487 - INFO - Epoch/batch: 4/1661, ibatch: 19661, loss: \u001b[0;36m0.5414\u001b[0m, std: 0.6151\n",
      "2021-06-19 15:20:24,824 - INFO - loss: \u001b[0;32m0.5236\u001b[0m, std: 0.5991\n",
      "2021-06-19 15:20:25,362 - INFO - Epoch/batch: 4/1812, ibatch: 19812, loss: \u001b[0;36m0.5463\u001b[0m, std: 0.6187\n",
      "2021-06-19 15:20:31,246 - INFO - Epoch/batch: 4/1963, ibatch: 19963, loss: \u001b[0;36m0.5271\u001b[0m, std: 0.6046\n",
      "2021-06-19 15:20:37,268 - INFO - Epoch/batch: 4/2114, ibatch: 20114, loss: \u001b[0;36m0.5425\u001b[0m, std: 0.6237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135: ReduceOnPlateau set learning rate to 0.00129140163.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:20:52,325 - INFO - loss: \u001b[0;32m0.5252\u001b[0m, std: 0.6215\n",
      "2021-06-19 15:20:52,940 - INFO - Epoch/batch: 4/2265, ibatch: 20265, loss: \u001b[0;36m0.5480\u001b[0m, std: 0.6184\n",
      "2021-06-19 15:20:58,696 - INFO - Epoch/batch: 4/2416, ibatch: 20416, loss: \u001b[0;36m0.5298\u001b[0m, std: 0.6181\n",
      "2021-06-19 15:21:04,098 - INFO - Epoch/batch: 4/2567, ibatch: 20567, loss: \u001b[0;36m0.5353\u001b[0m, std: 0.6156\n",
      "2021-06-19 15:21:18,322 - INFO - loss: \u001b[0;32m0.5234\u001b[0m, std: 0.5925\n",
      "2021-06-19 15:21:18,993 - INFO - Epoch/batch: 4/2718, ibatch: 20718, loss: \u001b[0;36m0.5560\u001b[0m, std: 0.6347\n",
      "2021-06-19 15:21:24,910 - INFO - Epoch/batch: 4/2869, ibatch: 20869, loss: \u001b[0;36m0.5520\u001b[0m, std: 0.6230\n",
      "2021-06-19 15:21:30,597 - INFO - Epoch/batch: 4/3020, ibatch: 21020, loss: \u001b[0;36m0.5331\u001b[0m, std: 0.6080\n",
      "2021-06-19 15:21:45,548 - INFO - loss: \u001b[0;32m0.5222\u001b[0m, std: 0.6125\n",
      "2021-06-19 15:21:45,564 - INFO - Saved model states in: earlystop_0.5222\n",
      "2021-06-19 15:21:45,565 - INFO - Saved net python code: earlystop_0.5222/paddle_nets.py\n",
      "2021-06-19 15:21:45,573 - INFO - Saved best model: earlystop_0.5222\n",
      "2021-06-19 15:21:45,573 - INFO - Removing earlystop model: earlystop_0.5226\n",
      "2021-06-19 15:21:46,434 - INFO - Epoch/batch: 4/3171, ibatch: 21171, loss: \u001b[0;36m0.5167\u001b[0m, std: 0.6045\n",
      "2021-06-19 15:21:53,053 - INFO - Epoch/batch: 4/3322, ibatch: 21322, loss: \u001b[0;36m0.5382\u001b[0m, std: 0.6180\n",
      "2021-06-19 15:21:59,614 - INFO - Epoch/batch: 4/3473, ibatch: 21473, loss: \u001b[0;36m0.5214\u001b[0m, std: 0.6035\n",
      "2021-06-19 15:22:14,647 - INFO - loss: \u001b[0;32m0.5260\u001b[0m, std: 0.5769\n",
      "2021-06-19 15:22:15,776 - INFO - Epoch/batch: 4/3624, ibatch: 21624, loss: \u001b[0;36m0.5452\u001b[0m, std: 0.6165\n",
      "2021-06-19 15:22:21,898 - INFO - Epoch/batch: 4/3775, ibatch: 21775, loss: \u001b[0;36m0.5374\u001b[0m, std: 0.6224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146: ReduceOnPlateau set learning rate to 0.001162261467.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:22:27,891 - INFO - Epoch/batch: 4/3926, ibatch: 21926, loss: \u001b[0;36m0.5361\u001b[0m, std: 0.6269\n",
      "2021-06-19 15:22:42,033 - INFO - loss: \u001b[0;32m0.5246\u001b[0m, std: 0.5860\n",
      "2021-06-19 15:22:43,131 - INFO - Epoch/batch: 4/4077, ibatch: 22077, loss: \u001b[0;36m0.5415\u001b[0m, std: 0.6143\n",
      "2021-06-19 15:22:48,724 - INFO - Epoch/batch: 4/4228, ibatch: 22228, loss: \u001b[0;36m0.5158\u001b[0m, std: 0.6031\n",
      "2021-06-19 15:22:54,281 - INFO - Epoch/batch: 4/4379, ibatch: 22379, loss: \u001b[0;36m0.5268\u001b[0m, std: 0.6007\n",
      "2021-06-19 15:23:08,442 - INFO - loss: \u001b[0;32m0.5239\u001b[0m, std: 0.5808\n",
      "2021-06-19 15:23:09,634 - INFO - Epoch 4 average training loss: \u001b[0;46m0.5368\u001b[0m std: 0.6158\n",
      "2021-06-19 15:23:09,638 - INFO - Epoch 4 average validate loss: \u001b[0;46m0.5256\u001b[0m std: 0.5896\n",
      "2021-06-19 15:23:11,254 - INFO - Epoch/batch: 5/   0, ibatch: 22500, loss: \u001b[0;36m0.5330\u001b[0m, std: 0.6150\n",
      "2021-06-19 15:23:20,576 - INFO - loss: \u001b[0;32m0.5242\u001b[0m, std: 0.5794\n",
      "2021-06-19 15:23:26,403 - INFO - Epoch/batch: 5/ 151, ibatch: 22651, loss: \u001b[0;36m0.5533\u001b[0m, std: 0.6227\n",
      "2021-06-19 15:23:31,905 - INFO - Epoch/batch: 5/ 302, ibatch: 22802, loss: \u001b[0;36m0.5196\u001b[0m, std: 0.6006\n",
      "2021-06-19 15:23:46,917 - INFO - loss: \u001b[0;32m0.5222\u001b[0m, std: 0.5984\n",
      "2021-06-19 15:23:46,935 - INFO - Saved model states in: earlystop_0.5222.1\n",
      "2021-06-19 15:23:46,936 - INFO - Saved net python code: earlystop_0.5222.1/paddle_nets.py\n",
      "2021-06-19 15:23:46,944 - INFO - Saved best model: earlystop_0.5222.1\n",
      "2021-06-19 15:23:46,944 - INFO - Removing earlystop model: earlystop_0.5222\n",
      "2021-06-19 15:23:47,048 - INFO - Epoch/batch: 5/ 453, ibatch: 22953, loss: \u001b[0;36m0.5181\u001b[0m, std: 0.6031\n",
      "2021-06-19 15:23:52,503 - INFO - Epoch/batch: 5/ 604, ibatch: 23104, loss: \u001b[0;36m0.5522\u001b[0m, std: 0.6255\n",
      "2021-06-19 15:23:58,240 - INFO - Epoch/batch: 5/ 755, ibatch: 23255, loss: \u001b[0;36m0.5489\u001b[0m, std: 0.6175\n",
      "2021-06-19 15:24:13,529 - INFO - loss: \u001b[0;32m0.5228\u001b[0m, std: 0.6071\n",
      "2021-06-19 15:24:13,734 - INFO - Epoch/batch: 5/ 906, ibatch: 23406, loss: \u001b[0;36m0.5188\u001b[0m, std: 0.6029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157: ReduceOnPlateau set learning rate to 0.0010460353203000001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:24:19,120 - INFO - Epoch/batch: 5/1057, ibatch: 23557, loss: \u001b[0;36m0.5152\u001b[0m, std: 0.5944\n",
      "2021-06-19 15:24:24,653 - INFO - Epoch/batch: 5/1208, ibatch: 23708, loss: \u001b[0;36m0.5444\u001b[0m, std: 0.6224\n",
      "2021-06-19 15:24:39,529 - INFO - loss: \u001b[0;32m0.5213\u001b[0m, std: 0.6019\n",
      "2021-06-19 15:24:39,544 - INFO - Saved model states in: earlystop_0.5213\n",
      "2021-06-19 15:24:39,546 - INFO - Saved net python code: earlystop_0.5213/paddle_nets.py\n",
      "2021-06-19 15:24:39,552 - INFO - Saved best model: earlystop_0.5213\n",
      "2021-06-19 15:24:39,553 - INFO - Removing earlystop model: earlystop_0.5222.1\n",
      "2021-06-19 15:24:39,861 - INFO - Epoch/batch: 5/1359, ibatch: 23859, loss: \u001b[0;36m0.5240\u001b[0m, std: 0.6118\n",
      "2021-06-19 15:24:45,635 - INFO - Epoch/batch: 5/1510, ibatch: 24010, loss: \u001b[0;36m0.5397\u001b[0m, std: 0.6191\n",
      "2021-06-19 15:24:51,044 - INFO - Epoch/batch: 5/1661, ibatch: 24161, loss: \u001b[0;36m0.5500\u001b[0m, std: 0.6268\n",
      "2021-06-19 15:25:06,647 - INFO - loss: \u001b[0;32m0.5292\u001b[0m, std: 0.5687\n",
      "2021-06-19 15:25:07,077 - INFO - Epoch/batch: 5/1812, ibatch: 24312, loss: \u001b[0;36m0.5541\u001b[0m, std: 0.6278\n",
      "2021-06-19 15:25:13,344 - INFO - Epoch/batch: 5/1963, ibatch: 24463, loss: \u001b[0;36m0.5178\u001b[0m, std: 0.6012\n",
      "2021-06-19 15:25:19,164 - INFO - Epoch/batch: 5/2114, ibatch: 24614, loss: \u001b[0;36m0.5361\u001b[0m, std: 0.6141\n",
      "2021-06-19 15:25:34,043 - INFO - loss: \u001b[0;32m0.5239\u001b[0m, std: 0.5819\n",
      "2021-06-19 15:25:34,647 - INFO - Epoch/batch: 5/2265, ibatch: 24765, loss: \u001b[0;36m0.5308\u001b[0m, std: 0.6120\n",
      "2021-06-19 15:25:40,195 - INFO - Epoch/batch: 5/2416, ibatch: 24916, loss: \u001b[0;36m0.5344\u001b[0m, std: 0.6127\n",
      "2021-06-19 15:25:46,324 - INFO - Epoch/batch: 5/2567, ibatch: 25067, loss: \u001b[0;36m0.5652\u001b[0m, std: 0.6446\n",
      "2021-06-19 15:26:01,034 - INFO - loss: \u001b[0;32m0.5226\u001b[0m, std: 0.6084\n",
      "2021-06-19 15:26:01,677 - INFO - Epoch/batch: 5/2718, ibatch: 25218, loss: \u001b[0;36m0.5281\u001b[0m, std: 0.6054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169: ReduceOnPlateau set learning rate to 0.0009414317882700001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:26:07,757 - INFO - Epoch/batch: 5/2869, ibatch: 25369, loss: \u001b[0;36m0.5368\u001b[0m, std: 0.6162\n",
      "2021-06-19 15:26:13,606 - INFO - Epoch/batch: 5/3020, ibatch: 25520, loss: \u001b[0;36m0.5386\u001b[0m, std: 0.6134\n",
      "2021-06-19 15:26:28,229 - INFO - loss: \u001b[0;32m0.5207\u001b[0m, std: 0.6080\n",
      "2021-06-19 15:26:28,244 - INFO - Saved model states in: earlystop_0.5207\n",
      "2021-06-19 15:26:28,245 - INFO - Saved net python code: earlystop_0.5207/paddle_nets.py\n",
      "2021-06-19 15:26:28,252 - INFO - Saved best model: earlystop_0.5207\n",
      "2021-06-19 15:26:28,253 - INFO - Removing earlystop model: earlystop_0.5213\n",
      "2021-06-19 15:26:29,012 - INFO - Epoch/batch: 5/3171, ibatch: 25671, loss: \u001b[0;36m0.5310\u001b[0m, std: 0.6062\n",
      "2021-06-19 15:26:34,648 - INFO - Epoch/batch: 5/3322, ibatch: 25822, loss: \u001b[0;36m0.5298\u001b[0m, std: 0.6092\n",
      "2021-06-19 15:26:40,452 - INFO - Epoch/batch: 5/3473, ibatch: 25973, loss: \u001b[0;36m0.5247\u001b[0m, std: 0.6086\n",
      "2021-06-19 15:26:55,051 - INFO - loss: \u001b[0;32m0.5212\u001b[0m, std: 0.6065\n",
      "2021-06-19 15:26:55,985 - INFO - Epoch/batch: 5/3624, ibatch: 26124, loss: \u001b[0;36m0.5363\u001b[0m, std: 0.6175\n",
      "2021-06-19 15:27:01,811 - INFO - Epoch/batch: 5/3775, ibatch: 26275, loss: \u001b[0;36m0.5562\u001b[0m, std: 0.6283\n",
      "2021-06-19 15:27:07,855 - INFO - Epoch/batch: 5/3926, ibatch: 26426, loss: \u001b[0;36m0.5132\u001b[0m, std: 0.6025\n",
      "2021-06-19 15:27:22,951 - INFO - loss: \u001b[0;32m0.5237\u001b[0m, std: 0.5835\n",
      "2021-06-19 15:27:23,940 - INFO - Epoch/batch: 5/4077, ibatch: 26577, loss: \u001b[0;36m0.5143\u001b[0m, std: 0.6011\n",
      "2021-06-19 15:27:30,468 - INFO - Epoch/batch: 5/4228, ibatch: 26728, loss: \u001b[0;36m0.5444\u001b[0m, std: 0.6245\n",
      "2021-06-19 15:27:36,683 - INFO - Epoch/batch: 5/4379, ibatch: 26879, loss: \u001b[0;36m0.5351\u001b[0m, std: 0.6061\n",
      "2021-06-19 15:27:51,662 - INFO - loss: \u001b[0;32m0.5217\u001b[0m, std: 0.5871\n",
      "2021-06-19 15:27:52,960 - INFO - Epoch 5 average training loss: \u001b[0;46m0.5349\u001b[0m std: 0.6136\n",
      "2021-06-19 15:27:52,966 - INFO - Epoch 5 average validate loss: \u001b[0;46m0.5230\u001b[0m std: 0.5937\n",
      "2021-06-19 15:27:54,886 - INFO - Epoch/batch: 6/   0, ibatch: 27000, loss: \u001b[0;36m0.5349\u001b[0m, std: 0.6119\n",
      "2021-06-19 15:28:04,675 - INFO - loss: \u001b[0;32m0.5218\u001b[0m, std: 0.5866\n",
      "2021-06-19 15:28:10,776 - INFO - Epoch/batch: 6/ 151, ibatch: 27151, loss: \u001b[0;36m0.5540\u001b[0m, std: 0.6237\n",
      "2021-06-19 15:28:16,253 - INFO - Epoch/batch: 6/ 302, ibatch: 27302, loss: \u001b[0;36m0.5291\u001b[0m, std: 0.6059\n",
      "2021-06-19 15:28:31,333 - INFO - loss: \u001b[0;32m0.5221\u001b[0m, std: 0.5900\n",
      "2021-06-19 15:28:31,465 - INFO - Epoch/batch: 6/ 453, ibatch: 27453, loss: \u001b[0;36m0.5286\u001b[0m, std: 0.6094\n",
      "2021-06-19 15:28:37,565 - INFO - Epoch/batch: 6/ 604, ibatch: 27604, loss: \u001b[0;36m0.5468\u001b[0m, std: 0.6272\n",
      "2021-06-19 15:28:43,881 - INFO - Epoch/batch: 6/ 755, ibatch: 27755, loss: \u001b[0;36m0.5304\u001b[0m, std: 0.6137\n",
      "2021-06-19 15:28:59,193 - INFO - loss: \u001b[0;32m0.5209\u001b[0m, std: 0.5882\n",
      "2021-06-19 15:28:59,421 - INFO - Epoch/batch: 6/ 906, ibatch: 27906, loss: \u001b[0;36m0.5471\u001b[0m, std: 0.6224\n",
      "2021-06-19 15:29:05,184 - INFO - Epoch/batch: 6/1057, ibatch: 28057, loss: \u001b[0;36m0.5377\u001b[0m, std: 0.6148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188: ReduceOnPlateau set learning rate to 0.0008472886094430002.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:29:10,570 - INFO - Epoch/batch: 6/1208, ibatch: 28208, loss: \u001b[0;36m0.5349\u001b[0m, std: 0.6019\n",
      "2021-06-19 15:29:26,041 - INFO - loss: \u001b[0;32m0.5217\u001b[0m, std: 0.6040\n",
      "2021-06-19 15:29:26,372 - INFO - Epoch/batch: 6/1359, ibatch: 28359, loss: \u001b[0;36m0.5144\u001b[0m, std: 0.6041\n",
      "2021-06-19 15:29:32,303 - INFO - Epoch/batch: 6/1510, ibatch: 28510, loss: \u001b[0;36m0.5381\u001b[0m, std: 0.6225\n",
      "2021-06-19 15:29:38,098 - INFO - Epoch/batch: 6/1661, ibatch: 28661, loss: \u001b[0;36m0.5210\u001b[0m, std: 0.6013\n",
      "2021-06-19 15:29:53,357 - INFO - loss: \u001b[0;32m0.5212\u001b[0m, std: 0.6052\n",
      "2021-06-19 15:29:53,853 - INFO - Epoch/batch: 6/1812, ibatch: 28812, loss: \u001b[0;36m0.5403\u001b[0m, std: 0.6248\n",
      "2021-06-19 15:29:59,530 - INFO - Epoch/batch: 6/1963, ibatch: 28963, loss: \u001b[0;36m0.5304\u001b[0m, std: 0.6132\n",
      "2021-06-19 15:30:05,414 - INFO - Epoch/batch: 6/2114, ibatch: 29114, loss: \u001b[0;36m0.5154\u001b[0m, std: 0.6011\n",
      "2021-06-19 15:30:20,319 - INFO - loss: \u001b[0;32m0.5240\u001b[0m, std: 0.5790\n",
      "2021-06-19 15:30:20,956 - INFO - Epoch/batch: 6/2265, ibatch: 29265, loss: \u001b[0;36m0.5303\u001b[0m, std: 0.6118\n",
      "2021-06-19 15:30:27,294 - INFO - Epoch/batch: 6/2416, ibatch: 29416, loss: \u001b[0;36m0.5504\u001b[0m, std: 0.6246\n",
      "2021-06-19 15:30:32,993 - INFO - Epoch/batch: 6/2567, ibatch: 29567, loss: \u001b[0;36m0.5437\u001b[0m, std: 0.6163\n",
      "2021-06-19 15:30:47,927 - INFO - loss: \u001b[0;32m0.5215\u001b[0m, std: 0.5821\n",
      "2021-06-19 15:30:48,569 - INFO - Epoch/batch: 6/2718, ibatch: 29718, loss: \u001b[0;36m0.5180\u001b[0m, std: 0.6038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: ReduceOnPlateau set learning rate to 0.0007625597484987002.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:30:54,063 - INFO - Epoch/batch: 6/2869, ibatch: 29869, loss: \u001b[0;36m0.5216\u001b[0m, std: 0.6080\n",
      "2021-06-19 15:30:59,641 - INFO - Epoch/batch: 6/3020, ibatch: 30020, loss: \u001b[0;36m0.5325\u001b[0m, std: 0.6089\n",
      "2021-06-19 15:31:13,847 - INFO - loss: \u001b[0;32m0.5253\u001b[0m, std: 0.5702\n",
      "2021-06-19 15:31:14,607 - INFO - Epoch/batch: 6/3171, ibatch: 30171, loss: \u001b[0;36m0.5314\u001b[0m, std: 0.6084\n",
      "2021-06-19 15:31:20,157 - INFO - Epoch/batch: 6/3322, ibatch: 30322, loss: \u001b[0;36m0.5430\u001b[0m, std: 0.6102\n",
      "2021-06-19 15:31:25,545 - INFO - Epoch/batch: 6/3473, ibatch: 30473, loss: \u001b[0;36m0.5239\u001b[0m, std: 0.6042\n",
      "2021-06-19 15:31:40,549 - INFO - loss: \u001b[0;32m0.5207\u001b[0m, std: 0.5932\n",
      "2021-06-19 15:31:41,553 - INFO - Epoch/batch: 6/3624, ibatch: 30624, loss: \u001b[0;36m0.5398\u001b[0m, std: 0.6195\n",
      "2021-06-19 15:31:47,403 - INFO - Epoch/batch: 6/3775, ibatch: 30775, loss: \u001b[0;36m0.5440\u001b[0m, std: 0.6161\n",
      "2021-06-19 15:31:53,159 - INFO - Epoch/batch: 6/3926, ibatch: 30926, loss: \u001b[0;36m0.5375\u001b[0m, std: 0.6203\n",
      "2021-06-19 15:32:07,512 - INFO - loss: \u001b[0;32m0.5207\u001b[0m, std: 0.6009\n",
      "2021-06-19 15:32:08,584 - INFO - Epoch/batch: 6/4077, ibatch: 31077, loss: \u001b[0;36m0.5226\u001b[0m, std: 0.6037\n",
      "2021-06-19 15:32:14,292 - INFO - Epoch/batch: 6/4228, ibatch: 31228, loss: \u001b[0;36m0.5348\u001b[0m, std: 0.6136\n",
      "2021-06-19 15:32:19,952 - INFO - Epoch/batch: 6/4379, ibatch: 31379, loss: \u001b[0;36m0.5279\u001b[0m, std: 0.6053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 210: ReduceOnPlateau set learning rate to 0.0006863037736488302.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:32:34,221 - INFO - loss: \u001b[0;32m0.5207\u001b[0m, std: 0.6125\n",
      "2021-06-19 15:32:35,410 - INFO - Epoch 6 average training loss: \u001b[0;46m0.5335\u001b[0m std: 0.6124\n",
      "2021-06-19 15:32:35,414 - INFO - Epoch 6 average validate loss: \u001b[0;46m0.5219\u001b[0m std: 0.5920\n",
      "2021-06-19 15:32:37,382 - INFO - Epoch/batch: 7/   0, ibatch: 31500, loss: \u001b[0;36m0.5350\u001b[0m, std: 0.6098\n",
      "2021-06-19 15:32:46,775 - INFO - loss: \u001b[0;32m0.5208\u001b[0m, std: 0.6134\n",
      "2021-06-19 15:32:52,206 - INFO - Epoch/batch: 7/ 151, ibatch: 31651, loss: \u001b[0;36m0.5234\u001b[0m, std: 0.6079\n",
      "2021-06-19 15:32:57,620 - INFO - Epoch/batch: 7/ 302, ibatch: 31802, loss: \u001b[0;36m0.5149\u001b[0m, std: 0.6035\n",
      "2021-06-19 15:33:12,468 - INFO - loss: \u001b[0;32m0.5247\u001b[0m, std: 0.5698\n",
      "2021-06-19 15:33:12,571 - INFO - Epoch/batch: 7/ 453, ibatch: 31953, loss: \u001b[0;36m0.5537\u001b[0m, std: 0.6294\n",
      "2021-06-19 15:33:18,128 - INFO - Epoch/batch: 7/ 604, ibatch: 32104, loss: \u001b[0;36m0.5338\u001b[0m, std: 0.6054\n",
      "2021-06-19 15:33:23,758 - INFO - Epoch/batch: 7/ 755, ibatch: 32255, loss: \u001b[0;36m0.5358\u001b[0m, std: 0.6164\n",
      "2021-06-19 15:33:38,595 - INFO - loss: \u001b[0;32m0.5206\u001b[0m, std: 0.5961\n",
      "2021-06-19 15:33:38,612 - INFO - Saved model states in: earlystop_0.5206\n",
      "2021-06-19 15:33:38,613 - INFO - Saved net python code: earlystop_0.5206/paddle_nets.py\n",
      "2021-06-19 15:33:38,621 - INFO - Saved best model: earlystop_0.5206\n",
      "2021-06-19 15:33:38,622 - INFO - Removing earlystop model: earlystop_0.5207\n",
      "2021-06-19 15:33:38,789 - INFO - Epoch/batch: 7/ 906, ibatch: 32406, loss: \u001b[0;36m0.5359\u001b[0m, std: 0.6182\n",
      "2021-06-19 15:33:44,220 - INFO - Epoch/batch: 7/1057, ibatch: 32557, loss: \u001b[0;36m0.5349\u001b[0m, std: 0.6150\n",
      "2021-06-19 15:33:50,123 - INFO - Epoch/batch: 7/1208, ibatch: 32708, loss: \u001b[0;36m0.5461\u001b[0m, std: 0.6141\n",
      "2021-06-19 15:34:04,963 - INFO - loss: \u001b[0;32m0.5231\u001b[0m, std: 0.5749\n",
      "2021-06-19 15:34:05,361 - INFO - Epoch/batch: 7/1359, ibatch: 32859, loss: \u001b[0;36m0.5179\u001b[0m, std: 0.5988\n",
      "2021-06-19 15:34:11,226 - INFO - Epoch/batch: 7/1510, ibatch: 33010, loss: \u001b[0;36m0.5180\u001b[0m, std: 0.5971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221: ReduceOnPlateau set learning rate to 0.0006176733962839472.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:34:17,181 - INFO - Epoch/batch: 7/1661, ibatch: 33161, loss: \u001b[0;36m0.5440\u001b[0m, std: 0.6143\n",
      "2021-06-19 15:34:32,047 - INFO - loss: \u001b[0;32m0.5213\u001b[0m, std: 0.5854\n",
      "2021-06-19 15:34:32,479 - INFO - Epoch/batch: 7/1812, ibatch: 33312, loss: \u001b[0;36m0.5320\u001b[0m, std: 0.6071\n",
      "2021-06-19 15:34:38,292 - INFO - Epoch/batch: 7/1963, ibatch: 33463, loss: \u001b[0;36m0.5314\u001b[0m, std: 0.6068\n",
      "2021-06-19 15:34:44,378 - INFO - Epoch/batch: 7/2114, ibatch: 33614, loss: \u001b[0;36m0.5270\u001b[0m, std: 0.6076\n",
      "2021-06-19 15:34:59,904 - INFO - loss: \u001b[0;32m0.5211\u001b[0m, std: 0.5889\n",
      "2021-06-19 15:35:00,479 - INFO - Epoch/batch: 7/2265, ibatch: 33765, loss: \u001b[0;36m0.5451\u001b[0m, std: 0.6211\n",
      "2021-06-19 15:35:06,003 - INFO - Epoch/batch: 7/2416, ibatch: 33916, loss: \u001b[0;36m0.5314\u001b[0m, std: 0.6098\n",
      "2021-06-19 15:35:12,006 - INFO - Epoch/batch: 7/2567, ibatch: 34067, loss: \u001b[0;36m0.5369\u001b[0m, std: 0.6145\n",
      "2021-06-19 15:35:26,933 - INFO - loss: \u001b[0;32m0.5234\u001b[0m, std: 0.5829\n",
      "2021-06-19 15:35:27,556 - INFO - Epoch/batch: 7/2718, ibatch: 34218, loss: \u001b[0;36m0.5403\u001b[0m, std: 0.6164\n",
      "2021-06-19 15:35:33,070 - INFO - Epoch/batch: 7/2869, ibatch: 34369, loss: \u001b[0;36m0.5221\u001b[0m, std: 0.6085\n",
      "2021-06-19 15:35:38,450 - INFO - Epoch/batch: 7/3020, ibatch: 34520, loss: \u001b[0;36m0.5027\u001b[0m, std: 0.5927\n",
      "2021-06-19 15:35:52,408 - INFO - loss: \u001b[0;32m0.5230\u001b[0m, std: 0.5937\n",
      "2021-06-19 15:35:53,138 - INFO - Epoch/batch: 7/3171, ibatch: 34671, loss: \u001b[0;36m0.5353\u001b[0m, std: 0.6064\n",
      "2021-06-19 15:35:58,709 - INFO - Epoch/batch: 7/3322, ibatch: 34822, loss: \u001b[0;36m0.5415\u001b[0m, std: 0.6203\n",
      "2021-06-19 15:36:04,662 - INFO - Epoch/batch: 7/3473, ibatch: 34973, loss: \u001b[0;36m0.5412\u001b[0m, std: 0.6183\n",
      "2021-06-19 15:36:19,051 - INFO - loss: \u001b[0;32m0.5210\u001b[0m, std: 0.5858\n",
      "2021-06-19 15:36:19,993 - INFO - Epoch/batch: 7/3624, ibatch: 35124, loss: \u001b[0;36m0.5499\u001b[0m, std: 0.6262\n",
      "2021-06-19 15:36:25,783 - INFO - Epoch/batch: 7/3775, ibatch: 35275, loss: \u001b[0;36m0.5315\u001b[0m, std: 0.6138\n",
      "2021-06-19 15:36:31,603 - INFO - Epoch/batch: 7/3926, ibatch: 35426, loss: \u001b[0;36m0.5240\u001b[0m, std: 0.6097\n",
      "2021-06-19 15:36:45,729 - INFO - loss: \u001b[0;32m0.5202\u001b[0m, std: 0.5919\n",
      "2021-06-19 15:36:45,753 - INFO - Saved model states in: earlystop_0.5202\n",
      "2021-06-19 15:36:45,755 - INFO - Saved net python code: earlystop_0.5202/paddle_nets.py\n",
      "2021-06-19 15:36:45,761 - INFO - Saved best model: earlystop_0.5202\n",
      "2021-06-19 15:36:45,762 - INFO - Removing earlystop model: earlystop_0.5206\n",
      "2021-06-19 15:36:46,811 - INFO - Epoch/batch: 7/4077, ibatch: 35577, loss: \u001b[0;36m0.5380\u001b[0m, std: 0.6244\n",
      "2021-06-19 15:36:52,229 - INFO - Epoch/batch: 7/4228, ibatch: 35728, loss: \u001b[0;36m0.5319\u001b[0m, std: 0.6110\n",
      "2021-06-19 15:36:57,637 - INFO - Epoch/batch: 7/4379, ibatch: 35879, loss: \u001b[0;36m0.5294\u001b[0m, std: 0.6116\n",
      "2021-06-19 15:37:11,354 - INFO - loss: \u001b[0;32m0.5235\u001b[0m, std: 0.5797\n",
      "2021-06-19 15:37:12,747 - INFO - Epoch 7 average training loss: \u001b[0;46m0.5324\u001b[0m std: 0.6116\n",
      "2021-06-19 15:37:12,751 - INFO - Epoch 7 average validate loss: \u001b[0;46m0.5221\u001b[0m std: 0.5875\n",
      "2021-06-19 15:37:14,699 - INFO - Epoch/batch: 8/   0, ibatch: 36000, loss: \u001b[0;36m0.5194\u001b[0m, std: 0.6013\n",
      "2021-06-19 15:37:24,077 - INFO - loss: \u001b[0;32m0.5236\u001b[0m, std: 0.5795\n",
      "2021-06-19 15:37:30,239 - INFO - Epoch/batch: 8/ 151, ibatch: 36151, loss: \u001b[0;36m0.5442\u001b[0m, std: 0.6124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 242: ReduceOnPlateau set learning rate to 0.0005559060566555524.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:37:36,462 - INFO - Epoch/batch: 8/ 302, ibatch: 36302, loss: \u001b[0;36m0.5583\u001b[0m, std: 0.6305\n",
      "2021-06-19 15:37:52,153 - INFO - loss: \u001b[0;32m0.5217\u001b[0m, std: 0.5816\n",
      "2021-06-19 15:37:52,292 - INFO - Epoch/batch: 8/ 453, ibatch: 36453, loss: \u001b[0;36m0.5388\u001b[0m, std: 0.6190\n",
      "2021-06-19 15:37:57,882 - INFO - Epoch/batch: 8/ 604, ibatch: 36604, loss: \u001b[0;36m0.5304\u001b[0m, std: 0.6031\n",
      "2021-06-19 15:38:03,464 - INFO - Epoch/batch: 8/ 755, ibatch: 36755, loss: \u001b[0;36m0.5250\u001b[0m, std: 0.6013\n",
      "2021-06-19 15:38:18,650 - INFO - loss: \u001b[0;32m0.5196\u001b[0m, std: 0.5965\n",
      "2021-06-19 15:38:18,668 - INFO - Saved model states in: earlystop_0.5196\n",
      "2021-06-19 15:38:18,671 - INFO - Saved net python code: earlystop_0.5196/paddle_nets.py\n",
      "2021-06-19 15:38:18,679 - INFO - Saved best model: earlystop_0.5196\n",
      "2021-06-19 15:38:18,680 - INFO - Removing earlystop model: earlystop_0.5202\n",
      "2021-06-19 15:38:18,978 - INFO - Epoch/batch: 8/ 906, ibatch: 36906, loss: \u001b[0;36m0.5302\u001b[0m, std: 0.6120\n",
      "2021-06-19 15:38:24,825 - INFO - Epoch/batch: 8/1057, ibatch: 37057, loss: \u001b[0;36m0.5466\u001b[0m, std: 0.6135\n",
      "2021-06-19 15:38:30,548 - INFO - Epoch/batch: 8/1208, ibatch: 37208, loss: \u001b[0;36m0.5260\u001b[0m, std: 0.6093\n",
      "2021-06-19 15:38:45,341 - INFO - loss: \u001b[0;32m0.5213\u001b[0m, std: 0.5967\n",
      "2021-06-19 15:38:45,740 - INFO - Epoch/batch: 8/1359, ibatch: 37359, loss: \u001b[0;36m0.5381\u001b[0m, std: 0.6032\n",
      "2021-06-19 15:38:51,603 - INFO - Epoch/batch: 8/1510, ibatch: 37510, loss: \u001b[0;36m0.5339\u001b[0m, std: 0.6127\n",
      "2021-06-19 15:38:57,566 - INFO - Epoch/batch: 8/1661, ibatch: 37661, loss: \u001b[0;36m0.5349\u001b[0m, std: 0.6155\n",
      "2021-06-19 15:39:12,690 - INFO - loss: \u001b[0;32m0.5203\u001b[0m, std: 0.6062\n",
      "2021-06-19 15:39:13,139 - INFO - Epoch/batch: 8/1812, ibatch: 37812, loss: \u001b[0;36m0.5227\u001b[0m, std: 0.6045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 253: ReduceOnPlateau set learning rate to 0.0005003154509899972.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:39:19,246 - INFO - Epoch/batch: 8/1963, ibatch: 37963, loss: \u001b[0;36m0.5306\u001b[0m, std: 0.6076\n",
      "2021-06-19 15:39:25,091 - INFO - Epoch/batch: 8/2114, ibatch: 38114, loss: \u001b[0;36m0.5220\u001b[0m, std: 0.6042\n",
      "2021-06-19 15:39:40,045 - INFO - loss: \u001b[0;32m0.5222\u001b[0m, std: 0.5810\n",
      "2021-06-19 15:39:40,583 - INFO - Epoch/batch: 8/2265, ibatch: 38265, loss: \u001b[0;36m0.5290\u001b[0m, std: 0.6101\n",
      "2021-06-19 15:39:46,220 - INFO - Epoch/batch: 8/2416, ibatch: 38416, loss: \u001b[0;36m0.5121\u001b[0m, std: 0.5977\n",
      "2021-06-19 15:39:52,189 - INFO - Epoch/batch: 8/2567, ibatch: 38567, loss: \u001b[0;36m0.5599\u001b[0m, std: 0.6340\n",
      "2021-06-19 15:40:06,738 - INFO - loss: \u001b[0;32m0.5202\u001b[0m, std: 0.5950\n",
      "2021-06-19 15:40:07,454 - INFO - Epoch/batch: 8/2718, ibatch: 38718, loss: \u001b[0;36m0.5398\u001b[0m, std: 0.6195\n",
      "2021-06-19 15:40:13,002 - INFO - Epoch/batch: 8/2869, ibatch: 38869, loss: \u001b[0;36m0.5217\u001b[0m, std: 0.6031\n",
      "2021-06-19 15:40:18,354 - INFO - Epoch/batch: 8/3020, ibatch: 39020, loss: \u001b[0;36m0.5234\u001b[0m, std: 0.6118\n",
      "2021-06-19 15:40:32,723 - INFO - loss: \u001b[0;32m0.5240\u001b[0m, std: 0.5871\n",
      "2021-06-19 15:40:33,541 - INFO - Epoch/batch: 8/3171, ibatch: 39171, loss: \u001b[0;36m0.5428\u001b[0m, std: 0.6271\n",
      "2021-06-19 15:40:39,436 - INFO - Epoch/batch: 8/3322, ibatch: 39322, loss: \u001b[0;36m0.5244\u001b[0m, std: 0.6029\n",
      "2021-06-19 15:40:44,985 - INFO - Epoch/batch: 8/3473, ibatch: 39473, loss: \u001b[0;36m0.5239\u001b[0m, std: 0.6111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 264: ReduceOnPlateau set learning rate to 0.00045028390589099747.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:40:59,013 - INFO - loss: \u001b[0;32m0.5193\u001b[0m, std: 0.6026\n",
      "2021-06-19 15:40:59,029 - INFO - Saved model states in: earlystop_0.5193\n",
      "2021-06-19 15:40:59,030 - INFO - Saved net python code: earlystop_0.5193/paddle_nets.py\n",
      "2021-06-19 15:40:59,036 - INFO - Saved best model: earlystop_0.5193\n",
      "2021-06-19 15:40:59,037 - INFO - Removing earlystop model: earlystop_0.5196\n",
      "2021-06-19 15:40:59,963 - INFO - Epoch/batch: 8/3624, ibatch: 39624, loss: \u001b[0;36m0.5127\u001b[0m, std: 0.5925\n",
      "2021-06-19 15:41:05,438 - INFO - Epoch/batch: 8/3775, ibatch: 39775, loss: \u001b[0;36m0.5242\u001b[0m, std: 0.6098\n",
      "2021-06-19 15:41:10,721 - INFO - Epoch/batch: 8/3926, ibatch: 39926, loss: \u001b[0;36m0.5130\u001b[0m, std: 0.5945\n",
      "2021-06-19 15:41:24,532 - INFO - loss: \u001b[0;32m0.5247\u001b[0m, std: 0.5739\n",
      "2021-06-19 15:41:25,508 - INFO - Epoch/batch: 8/4077, ibatch: 40077, loss: \u001b[0;36m0.5446\u001b[0m, std: 0.6122\n",
      "2021-06-19 15:41:30,974 - INFO - Epoch/batch: 8/4228, ibatch: 40228, loss: \u001b[0;36m0.5289\u001b[0m, std: 0.6102\n",
      "2021-06-19 15:41:36,317 - INFO - Epoch/batch: 8/4379, ibatch: 40379, loss: \u001b[0;36m0.5295\u001b[0m, std: 0.6064\n",
      "2021-06-19 15:41:50,136 - INFO - loss: \u001b[0;32m0.5212\u001b[0m, std: 0.5794\n",
      "2021-06-19 15:41:51,266 - INFO - Epoch 8 average training loss: \u001b[0;46m0.5315\u001b[0m std: 0.6104\n",
      "2021-06-19 15:41:51,350 - INFO - Epoch 8 average validate loss: \u001b[0;46m0.5217\u001b[0m std: 0.5890\n",
      "2021-06-19 15:41:52,999 - INFO - Epoch/batch: 9/   0, ibatch: 40500, loss: \u001b[0;36m0.5351\u001b[0m, std: 0.6224\n",
      "2021-06-19 15:42:02,284 - INFO - loss: \u001b[0;32m0.5212\u001b[0m, std: 0.5794\n",
      "2021-06-19 15:42:07,619 - INFO - Epoch/batch: 9/ 151, ibatch: 40651, loss: \u001b[0;36m0.5247\u001b[0m, std: 0.5945\n",
      "2021-06-19 15:42:13,353 - INFO - Epoch/batch: 9/ 302, ibatch: 40802, loss: \u001b[0;36m0.5202\u001b[0m, std: 0.6030\n",
      "2021-06-19 15:42:28,973 - INFO - loss: \u001b[0;32m0.5199\u001b[0m, std: 0.5894\n",
      "2021-06-19 15:42:29,164 - INFO - Epoch/batch: 9/ 453, ibatch: 40953, loss: \u001b[0;36m0.5411\u001b[0m, std: 0.6173\n",
      "2021-06-19 15:42:35,130 - INFO - Epoch/batch: 9/ 604, ibatch: 41104, loss: \u001b[0;36m0.5338\u001b[0m, std: 0.6106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 275: ReduceOnPlateau set learning rate to 0.0004052555153018977.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:42:40,913 - INFO - Epoch/batch: 9/ 755, ibatch: 41255, loss: \u001b[0;36m0.5183\u001b[0m, std: 0.6050\n",
      "2021-06-19 15:42:56,279 - INFO - loss: \u001b[0;32m0.5224\u001b[0m, std: 0.5850\n",
      "2021-06-19 15:42:56,512 - INFO - Epoch/batch: 9/ 906, ibatch: 41406, loss: \u001b[0;36m0.5244\u001b[0m, std: 0.6093\n",
      "2021-06-19 15:43:02,349 - INFO - Epoch/batch: 9/1057, ibatch: 41557, loss: \u001b[0;36m0.5263\u001b[0m, std: 0.6013\n",
      "2021-06-19 15:43:08,343 - INFO - Epoch/batch: 9/1208, ibatch: 41708, loss: \u001b[0;36m0.5235\u001b[0m, std: 0.6140\n",
      "2021-06-19 15:43:23,685 - INFO - loss: \u001b[0;32m0.5220\u001b[0m, std: 0.5865\n",
      "2021-06-19 15:43:24,034 - INFO - Epoch/batch: 9/1359, ibatch: 41859, loss: \u001b[0;36m0.5425\u001b[0m, std: 0.6284\n",
      "2021-06-19 15:43:29,746 - INFO - Epoch/batch: 9/1510, ibatch: 42010, loss: \u001b[0;36m0.5462\u001b[0m, std: 0.6174\n",
      "2021-06-19 15:43:35,331 - INFO - Epoch/batch: 9/1661, ibatch: 42161, loss: \u001b[0;36m0.5381\u001b[0m, std: 0.6186\n",
      "2021-06-19 15:43:50,084 - INFO - loss: \u001b[0;32m0.5191\u001b[0m, std: 0.5964\n",
      "2021-06-19 15:43:50,101 - INFO - Saved model states in: earlystop_0.5191\n",
      "2021-06-19 15:43:50,102 - INFO - Saved net python code: earlystop_0.5191/paddle_nets.py\n",
      "2021-06-19 15:43:50,110 - INFO - Saved best model: earlystop_0.5191\n",
      "2021-06-19 15:43:50,111 - INFO - Removing earlystop model: earlystop_0.5193\n",
      "2021-06-19 15:43:50,643 - INFO - Epoch/batch: 9/1812, ibatch: 42312, loss: \u001b[0;36m0.5139\u001b[0m, std: 0.5933\n",
      "2021-06-19 15:43:56,353 - INFO - Epoch/batch: 9/1963, ibatch: 42463, loss: \u001b[0;36m0.5403\u001b[0m, std: 0.6164\n",
      "2021-06-19 15:44:02,473 - INFO - Epoch/batch: 9/2114, ibatch: 42614, loss: \u001b[0;36m0.5144\u001b[0m, std: 0.6010\n",
      "2021-06-19 15:44:17,459 - INFO - loss: \u001b[0;32m0.5197\u001b[0m, std: 0.6087\n",
      "2021-06-19 15:44:17,997 - INFO - Epoch/batch: 9/2265, ibatch: 42765, loss: \u001b[0;36m0.5305\u001b[0m, std: 0.6090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286: ReduceOnPlateau set learning rate to 0.00036472996377170795.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:44:23,377 - INFO - Epoch/batch: 9/2416, ibatch: 42916, loss: \u001b[0;36m0.5146\u001b[0m, std: 0.5952\n",
      "2021-06-19 15:44:29,463 - INFO - Epoch/batch: 9/2567, ibatch: 43067, loss: \u001b[0;36m0.5294\u001b[0m, std: 0.6151\n",
      "2021-06-19 15:44:43,996 - INFO - loss: \u001b[0;32m0.5246\u001b[0m, std: 0.5718\n",
      "2021-06-19 15:44:44,629 - INFO - Epoch/batch: 9/2718, ibatch: 43218, loss: \u001b[0;36m0.5531\u001b[0m, std: 0.6219\n",
      "2021-06-19 15:44:50,204 - INFO - Epoch/batch: 9/2869, ibatch: 43369, loss: \u001b[0;36m0.5333\u001b[0m, std: 0.6186\n",
      "2021-06-19 15:44:55,561 - INFO - Epoch/batch: 9/3020, ibatch: 43520, loss: \u001b[0;36m0.5274\u001b[0m, std: 0.6004\n",
      "2021-06-19 15:45:09,862 - INFO - loss: \u001b[0;32m0.5196\u001b[0m, std: 0.5913\n",
      "2021-06-19 15:45:10,918 - INFO - Epoch/batch: 9/3171, ibatch: 43671, loss: \u001b[0;36m0.5429\u001b[0m, std: 0.6198\n",
      "2021-06-19 15:45:16,506 - INFO - Epoch/batch: 9/3322, ibatch: 43822, loss: \u001b[0;36m0.5359\u001b[0m, std: 0.6161\n",
      "2021-06-19 15:45:21,799 - INFO - Epoch/batch: 9/3473, ibatch: 43973, loss: \u001b[0;36m0.5299\u001b[0m, std: 0.6013\n",
      "2021-06-19 15:45:35,694 - INFO - loss: \u001b[0;32m0.5202\u001b[0m, std: 0.5837\n",
      "2021-06-19 15:45:36,672 - INFO - Epoch/batch: 9/3624, ibatch: 44124, loss: \u001b[0;36m0.5325\u001b[0m, std: 0.6144\n",
      "2021-06-19 15:45:42,168 - INFO - Epoch/batch: 9/3775, ibatch: 44275, loss: \u001b[0;36m0.5252\u001b[0m, std: 0.6005\n",
      "2021-06-19 15:45:48,024 - INFO - Epoch/batch: 9/3926, ibatch: 44426, loss: \u001b[0;36m0.5189\u001b[0m, std: 0.6068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297: ReduceOnPlateau set learning rate to 0.00032825696739453717.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:46:02,228 - INFO - loss: \u001b[0;32m0.5202\u001b[0m, std: 0.5926\n",
      "2021-06-19 15:46:03,300 - INFO - Epoch/batch: 9/4077, ibatch: 44577, loss: \u001b[0;36m0.5201\u001b[0m, std: 0.6025\n",
      "2021-06-19 15:46:09,249 - INFO - Epoch/batch: 9/4228, ibatch: 44728, loss: \u001b[0;36m0.5416\u001b[0m, std: 0.6109\n",
      "2021-06-19 15:46:14,947 - INFO - Epoch/batch: 9/4379, ibatch: 44879, loss: \u001b[0;36m0.5352\u001b[0m, std: 0.6126\n",
      "2021-06-19 15:46:29,430 - INFO - loss: \u001b[0;32m0.5220\u001b[0m, std: 0.5863\n",
      "2021-06-19 15:46:30,505 - INFO - Epoch 9 average training loss: \u001b[0;46m0.5305\u001b[0m std: 0.6097\n",
      "2021-06-19 15:46:30,549 - INFO - Epoch 9 average validate loss: \u001b[0;46m0.5210\u001b[0m std: 0.5883\n",
      "2021-06-19 15:46:32,283 - INFO - Epoch/batch: 10/   0, ibatch: 45000, loss: \u001b[0;36m0.5361\u001b[0m, std: 0.6159\n",
      "2021-06-19 15:46:41,858 - INFO - loss: \u001b[0;32m0.5221\u001b[0m, std: 0.5859\n",
      "2021-06-19 15:46:47,510 - INFO - Epoch/batch: 10/ 151, ibatch: 45151, loss: \u001b[0;36m0.5548\u001b[0m, std: 0.6216\n",
      "2021-06-19 15:46:52,913 - INFO - Epoch/batch: 10/ 302, ibatch: 45302, loss: \u001b[0;36m0.5280\u001b[0m, std: 0.6178\n",
      "2021-06-19 15:47:07,989 - INFO - loss: \u001b[0;32m0.5216\u001b[0m, std: 0.5892\n",
      "2021-06-19 15:47:08,146 - INFO - Epoch/batch: 10/ 453, ibatch: 45453, loss: \u001b[0;36m0.5416\u001b[0m, std: 0.6154\n",
      "2021-06-19 15:47:13,633 - INFO - Epoch/batch: 10/ 604, ibatch: 45604, loss: \u001b[0;36m0.5447\u001b[0m, std: 0.6182\n",
      "2021-06-19 15:47:19,152 - INFO - Epoch/batch: 10/ 755, ibatch: 45755, loss: \u001b[0;36m0.5328\u001b[0m, std: 0.6177\n",
      "2021-06-19 15:47:34,249 - INFO - loss: \u001b[0;32m0.5218\u001b[0m, std: 0.5838\n",
      "2021-06-19 15:47:34,466 - INFO - Epoch/batch: 10/ 906, ibatch: 45906, loss: \u001b[0;36m0.5271\u001b[0m, std: 0.6026\n",
      "2021-06-19 15:47:40,219 - INFO - Epoch/batch: 10/1057, ibatch: 46057, loss: \u001b[0;36m0.5302\u001b[0m, std: 0.6120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 308: ReduceOnPlateau set learning rate to 0.00029543127065508344.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:47:46,007 - INFO - Epoch/batch: 10/1208, ibatch: 46208, loss: \u001b[0;36m0.5147\u001b[0m, std: 0.5936\n",
      "2021-06-19 15:48:00,989 - INFO - loss: \u001b[0;32m0.5198\u001b[0m, std: 0.5923\n",
      "2021-06-19 15:48:01,279 - INFO - Epoch/batch: 10/1359, ibatch: 46359, loss: \u001b[0;36m0.5118\u001b[0m, std: 0.5869\n",
      "2021-06-19 15:48:07,183 - INFO - Epoch/batch: 10/1510, ibatch: 46510, loss: \u001b[0;36m0.5338\u001b[0m, std: 0.6124\n",
      "2021-06-19 15:48:12,537 - INFO - Epoch/batch: 10/1661, ibatch: 46661, loss: \u001b[0;36m0.5372\u001b[0m, std: 0.6147\n",
      "2021-06-19 15:48:27,388 - INFO - loss: \u001b[0;32m0.5207\u001b[0m, std: 0.5896\n",
      "2021-06-19 15:48:27,890 - INFO - Epoch/batch: 10/1812, ibatch: 46812, loss: \u001b[0;36m0.5486\u001b[0m, std: 0.6175\n",
      "2021-06-19 15:48:33,412 - INFO - Epoch/batch: 10/1963, ibatch: 46963, loss: \u001b[0;36m0.5263\u001b[0m, std: 0.6113\n",
      "2021-06-19 15:48:38,847 - INFO - Epoch/batch: 10/2114, ibatch: 47114, loss: \u001b[0;36m0.5297\u001b[0m, std: 0.6005\n",
      "2021-06-19 15:48:53,152 - INFO - loss: \u001b[0;32m0.5196\u001b[0m, std: 0.5915\n",
      "2021-06-19 15:48:53,798 - INFO - Epoch/batch: 10/2265, ibatch: 47265, loss: \u001b[0;36m0.5197\u001b[0m, std: 0.5986\n",
      "2021-06-19 15:48:59,458 - INFO - Epoch/batch: 10/2416, ibatch: 47416, loss: \u001b[0;36m0.5382\u001b[0m, std: 0.6210\n",
      "2021-06-19 15:49:05,057 - INFO - Epoch/batch: 10/2567, ibatch: 47567, loss: \u001b[0;36m0.5375\u001b[0m, std: 0.6160\n",
      "2021-06-19 15:49:19,271 - INFO - loss: \u001b[0;32m0.5203\u001b[0m, std: 0.5921\n",
      "2021-06-19 15:49:19,941 - INFO - Epoch/batch: 10/2718, ibatch: 47718, loss: \u001b[0;36m0.5145\u001b[0m, std: 0.6001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 319: ReduceOnPlateau set learning rate to 0.0002658881435895751.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:49:25,358 - INFO - Epoch/batch: 10/2869, ibatch: 47869, loss: \u001b[0;36m0.5318\u001b[0m, std: 0.6067\n",
      "2021-06-19 15:49:31,037 - INFO - Epoch/batch: 10/3020, ibatch: 48020, loss: \u001b[0;36m0.5414\u001b[0m, std: 0.6148\n",
      "2021-06-19 15:49:45,276 - INFO - loss: \u001b[0;32m0.5199\u001b[0m, std: 0.5907\n",
      "2021-06-19 15:49:46,081 - INFO - Epoch/batch: 10/3171, ibatch: 48171, loss: \u001b[0;36m0.5303\u001b[0m, std: 0.6111\n",
      "2021-06-19 15:49:51,741 - INFO - Epoch/batch: 10/3322, ibatch: 48322, loss: \u001b[0;36m0.5456\u001b[0m, std: 0.6088\n",
      "2021-06-19 15:49:57,220 - INFO - Epoch/batch: 10/3473, ibatch: 48473, loss: \u001b[0;36m0.5084\u001b[0m, std: 0.5896\n",
      "2021-06-19 15:50:11,665 - INFO - loss: \u001b[0;32m0.5196\u001b[0m, std: 0.6131\n",
      "2021-06-19 15:50:12,478 - INFO - Epoch/batch: 10/3624, ibatch: 48624, loss: \u001b[0;36m0.5165\u001b[0m, std: 0.6048\n",
      "2021-06-19 15:50:18,101 - INFO - Epoch/batch: 10/3775, ibatch: 48775, loss: \u001b[0;36m0.5264\u001b[0m, std: 0.6065\n",
      "2021-06-19 15:50:23,795 - INFO - Epoch/batch: 10/3926, ibatch: 48926, loss: \u001b[0;36m0.5098\u001b[0m, std: 0.5992\n",
      "2021-06-19 15:50:37,954 - INFO - loss: \u001b[0;32m0.5196\u001b[0m, std: 0.5912\n",
      "2021-06-19 15:50:38,956 - INFO - Epoch/batch: 10/4077, ibatch: 49077, loss: \u001b[0;36m0.5187\u001b[0m, std: 0.6104\n",
      "2021-06-19 15:50:44,610 - INFO - Epoch/batch: 10/4228, ibatch: 49228, loss: \u001b[0;36m0.5317\u001b[0m, std: 0.6111\n",
      "2021-06-19 15:50:50,116 - INFO - Epoch/batch: 10/4379, ibatch: 49379, loss: \u001b[0;36m0.5259\u001b[0m, std: 0.6054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 330: ReduceOnPlateau set learning rate to 0.0002392993292306176.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:51:04,247 - INFO - loss: \u001b[0;32m0.5193\u001b[0m, std: 0.5960\n",
      "2021-06-19 15:51:05,364 - INFO - Epoch 10 average training loss: \u001b[0;46m0.5297\u001b[0m std: 0.6085\n",
      "2021-06-19 15:51:05,368 - INFO - Epoch 10 average validate loss: \u001b[0;46m0.5204\u001b[0m std: 0.5923\n",
      "2021-06-19 15:51:07,015 - INFO - Epoch/batch: 11/   0, ibatch: 49500, loss: \u001b[0;36m0.5388\u001b[0m, std: 0.6143\n",
      "2021-06-19 15:51:16,264 - INFO - loss: \u001b[0;32m0.5193\u001b[0m, std: 0.5957\n",
      "2021-06-19 15:51:21,841 - INFO - Epoch/batch: 11/ 151, ibatch: 49651, loss: \u001b[0;36m0.5383\u001b[0m, std: 0.6110\n",
      "2021-06-19 15:51:27,130 - INFO - Epoch/batch: 11/ 302, ibatch: 49802, loss: \u001b[0;36m0.5181\u001b[0m, std: 0.6012\n",
      "2021-06-19 15:51:41,953 - INFO - loss: \u001b[0;32m0.5200\u001b[0m, std: 0.5919\n",
      "2021-06-19 15:51:42,051 - INFO - Epoch/batch: 11/ 453, ibatch: 49953, loss: \u001b[0;36m0.5376\u001b[0m, std: 0.6125\n",
      "2021-06-19 15:51:47,400 - INFO - Epoch/batch: 11/ 604, ibatch: 50104, loss: \u001b[0;36m0.5229\u001b[0m, std: 0.6061\n",
      "2021-06-19 15:51:52,810 - INFO - Epoch/batch: 11/ 755, ibatch: 50255, loss: \u001b[0;36m0.5117\u001b[0m, std: 0.5981\n",
      "2021-06-19 15:52:07,854 - INFO - loss: \u001b[0;32m0.5205\u001b[0m, std: 0.5851\n",
      "2021-06-19 15:52:08,074 - INFO - Epoch/batch: 11/ 906, ibatch: 50406, loss: \u001b[0;36m0.5202\u001b[0m, std: 0.5982\n",
      "2021-06-19 15:52:13,906 - INFO - Epoch/batch: 11/1057, ibatch: 50557, loss: \u001b[0;36m0.5124\u001b[0m, std: 0.5930\n",
      "2021-06-19 15:52:19,566 - INFO - Epoch/batch: 11/1208, ibatch: 50708, loss: \u001b[0;36m0.5147\u001b[0m, std: 0.5968\n",
      "2021-06-19 15:52:34,488 - INFO - loss: \u001b[0;32m0.5195\u001b[0m, std: 0.5896\n",
      "2021-06-19 15:52:34,913 - INFO - Epoch/batch: 11/1359, ibatch: 50859, loss: \u001b[0;36m0.5548\u001b[0m, std: 0.6281\n",
      "2021-06-19 15:52:40,540 - INFO - Epoch/batch: 11/1510, ibatch: 51010, loss: \u001b[0;36m0.5406\u001b[0m, std: 0.6221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 341: ReduceOnPlateau set learning rate to 0.00021536939630755584.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:52:46,385 - INFO - Epoch/batch: 11/1661, ibatch: 51161, loss: \u001b[0;36m0.5196\u001b[0m, std: 0.6013\n",
      "2021-06-19 15:53:01,237 - INFO - loss: \u001b[0;32m0.5221\u001b[0m, std: 0.5872\n",
      "2021-06-19 15:53:01,709 - INFO - Epoch/batch: 11/1812, ibatch: 51312, loss: \u001b[0;36m0.5342\u001b[0m, std: 0.6200\n",
      "2021-06-19 15:53:07,434 - INFO - Epoch/batch: 11/1963, ibatch: 51463, loss: \u001b[0;36m0.5317\u001b[0m, std: 0.6138\n",
      "2021-06-19 15:53:13,162 - INFO - Epoch/batch: 11/2114, ibatch: 51614, loss: \u001b[0;36m0.5142\u001b[0m, std: 0.5976\n",
      "2021-06-19 15:53:27,982 - INFO - loss: \u001b[0;32m0.5188\u001b[0m, std: 0.5985\n",
      "2021-06-19 15:53:27,999 - INFO - Saved model states in: earlystop_0.5188\n",
      "2021-06-19 15:53:28,000 - INFO - Saved net python code: earlystop_0.5188/paddle_nets.py\n",
      "2021-06-19 15:53:28,008 - INFO - Saved best model: earlystop_0.5188\n",
      "2021-06-19 15:53:28,009 - INFO - Removing earlystop model: earlystop_0.5191\n",
      "2021-06-19 15:53:28,601 - INFO - Epoch/batch: 11/2265, ibatch: 51765, loss: \u001b[0;36m0.5156\u001b[0m, std: 0.5925\n",
      "2021-06-19 15:53:34,318 - INFO - Epoch/batch: 11/2416, ibatch: 51916, loss: \u001b[0;36m0.5337\u001b[0m, std: 0.6158\n",
      "2021-06-19 15:53:39,907 - INFO - Epoch/batch: 11/2567, ibatch: 52067, loss: \u001b[0;36m0.5370\u001b[0m, std: 0.6074\n",
      "2021-06-19 15:53:54,755 - INFO - loss: \u001b[0;32m0.5213\u001b[0m, std: 0.5822\n",
      "2021-06-19 15:53:55,450 - INFO - Epoch/batch: 11/2718, ibatch: 52218, loss: \u001b[0;36m0.5540\u001b[0m, std: 0.6250\n",
      "2021-06-19 15:54:01,057 - INFO - Epoch/batch: 11/2869, ibatch: 52369, loss: \u001b[0;36m0.5239\u001b[0m, std: 0.6024\n",
      "2021-06-19 15:54:06,567 - INFO - Epoch/batch: 11/3020, ibatch: 52520, loss: \u001b[0;36m0.5284\u001b[0m, std: 0.6023\n",
      "2021-06-19 15:54:20,889 - INFO - loss: \u001b[0;32m0.5204\u001b[0m, std: 0.5895\n",
      "2021-06-19 15:54:21,738 - INFO - Epoch/batch: 11/3171, ibatch: 52671, loss: \u001b[0;36m0.5220\u001b[0m, std: 0.6070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 352: ReduceOnPlateau set learning rate to 0.00019383245667680025.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:54:27,517 - INFO - Epoch/batch: 11/3322, ibatch: 52822, loss: \u001b[0;36m0.5210\u001b[0m, std: 0.6002\n",
      "2021-06-19 15:54:33,003 - INFO - Epoch/batch: 11/3473, ibatch: 52973, loss: \u001b[0;36m0.5353\u001b[0m, std: 0.6115\n",
      "2021-06-19 15:54:47,404 - INFO - loss: \u001b[0;32m0.5205\u001b[0m, std: 0.5867\n",
      "2021-06-19 15:54:48,295 - INFO - Epoch/batch: 11/3624, ibatch: 53124, loss: \u001b[0;36m0.5373\u001b[0m, std: 0.6215\n",
      "2021-06-19 15:54:53,864 - INFO - Epoch/batch: 11/3775, ibatch: 53275, loss: \u001b[0;36m0.5292\u001b[0m, std: 0.6042\n",
      "2021-06-19 15:54:59,361 - INFO - Epoch/batch: 11/3926, ibatch: 53426, loss: \u001b[0;36m0.5291\u001b[0m, std: 0.6091\n",
      "2021-06-19 15:55:13,767 - INFO - loss: \u001b[0;32m0.5218\u001b[0m, std: 0.5790\n",
      "2021-06-19 15:55:14,768 - INFO - Epoch/batch: 11/4077, ibatch: 53577, loss: \u001b[0;36m0.5381\u001b[0m, std: 0.6116\n",
      "2021-06-19 15:55:20,587 - INFO - Epoch/batch: 11/4228, ibatch: 53728, loss: \u001b[0;36m0.5276\u001b[0m, std: 0.6053\n",
      "2021-06-19 15:55:26,544 - INFO - Epoch/batch: 11/4379, ibatch: 53879, loss: \u001b[0;36m0.5335\u001b[0m, std: 0.6185\n",
      "2021-06-19 15:55:40,739 - INFO - loss: \u001b[0;32m0.5214\u001b[0m, std: 0.5788\n",
      "2021-06-19 15:55:41,948 - INFO - Epoch 11 average training loss: \u001b[0;46m0.5293\u001b[0m std: 0.6084\n",
      "2021-06-19 15:55:41,952 - INFO - Epoch 11 average validate loss: \u001b[0;46m0.5205\u001b[0m std: 0.5877\n",
      "2021-06-19 15:55:43,773 - INFO - Epoch/batch: 12/   0, ibatch: 54000, loss: \u001b[0;36m0.5431\u001b[0m, std: 0.6152\n",
      "2021-06-19 15:55:53,376 - INFO - loss: \u001b[0;32m0.5214\u001b[0m, std: 0.5788\n",
      "2021-06-19 15:55:59,222 - INFO - Epoch/batch: 12/ 151, ibatch: 54151, loss: \u001b[0;36m0.5205\u001b[0m, std: 0.6010\n",
      "2021-06-19 15:56:04,955 - INFO - Epoch/batch: 12/ 302, ibatch: 54302, loss: \u001b[0;36m0.5118\u001b[0m, std: 0.5985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 363: ReduceOnPlateau set learning rate to 0.00017444921100912022.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:56:20,215 - INFO - loss: \u001b[0;32m0.5202\u001b[0m, std: 0.5849\n",
      "2021-06-19 15:56:20,339 - INFO - Epoch/batch: 12/ 453, ibatch: 54453, loss: \u001b[0;36m0.5307\u001b[0m, std: 0.6140\n",
      "2021-06-19 15:56:26,004 - INFO - Epoch/batch: 12/ 604, ibatch: 54604, loss: \u001b[0;36m0.5212\u001b[0m, std: 0.6028\n",
      "2021-06-19 15:56:31,462 - INFO - Epoch/batch: 12/ 755, ibatch: 54755, loss: \u001b[0;36m0.5181\u001b[0m, std: 0.5965\n",
      "2021-06-19 15:56:46,928 - INFO - loss: \u001b[0;32m0.5207\u001b[0m, std: 0.5893\n",
      "2021-06-19 15:56:47,144 - INFO - Epoch/batch: 12/ 906, ibatch: 54906, loss: \u001b[0;36m0.5269\u001b[0m, std: 0.6093\n",
      "2021-06-19 15:56:52,913 - INFO - Epoch/batch: 12/1057, ibatch: 55057, loss: \u001b[0;36m0.5375\u001b[0m, std: 0.6103\n",
      "2021-06-19 15:56:58,875 - INFO - Epoch/batch: 12/1208, ibatch: 55208, loss: \u001b[0;36m0.5283\u001b[0m, std: 0.6108\n",
      "2021-06-19 15:57:13,650 - INFO - loss: \u001b[0;32m0.5202\u001b[0m, std: 0.5867\n",
      "2021-06-19 15:57:13,994 - INFO - Epoch/batch: 12/1359, ibatch: 55359, loss: \u001b[0;36m0.5413\u001b[0m, std: 0.6221\n",
      "2021-06-19 15:57:19,532 - INFO - Epoch/batch: 12/1510, ibatch: 55510, loss: \u001b[0;36m0.5291\u001b[0m, std: 0.6080\n",
      "2021-06-19 15:57:24,632 - INFO - Epoch/batch: 12/1661, ibatch: 55661, loss: \u001b[0;36m0.5268\u001b[0m, std: 0.6092\n",
      "2021-06-19 15:57:39,446 - INFO - loss: \u001b[0;32m0.5222\u001b[0m, std: 0.5792\n",
      "2021-06-19 15:57:39,921 - INFO - Epoch/batch: 12/1812, ibatch: 55812, loss: \u001b[0;36m0.5450\u001b[0m, std: 0.6149\n",
      "2021-06-19 15:57:45,820 - INFO - Epoch/batch: 12/1963, ibatch: 55963, loss: \u001b[0;36m0.5277\u001b[0m, std: 0.6032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 374: ReduceOnPlateau set learning rate to 0.0001570042899082082.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:57:51,665 - INFO - Epoch/batch: 12/2114, ibatch: 56114, loss: \u001b[0;36m0.5347\u001b[0m, std: 0.6073\n",
      "2021-06-19 15:58:06,739 - INFO - loss: \u001b[0;32m0.5194\u001b[0m, std: 0.5917\n",
      "2021-06-19 15:58:07,284 - INFO - Epoch/batch: 12/2265, ibatch: 56265, loss: \u001b[0;36m0.5245\u001b[0m, std: 0.6108\n",
      "2021-06-19 15:58:12,840 - INFO - Epoch/batch: 12/2416, ibatch: 56416, loss: \u001b[0;36m0.5388\u001b[0m, std: 0.6117\n",
      "2021-06-19 15:58:18,352 - INFO - Epoch/batch: 12/2567, ibatch: 56567, loss: \u001b[0;36m0.5456\u001b[0m, std: 0.6210\n",
      "2021-06-19 15:58:32,763 - INFO - loss: \u001b[0;32m0.5193\u001b[0m, std: 0.5958\n",
      "2021-06-19 15:58:33,350 - INFO - Epoch/batch: 12/2718, ibatch: 56718, loss: \u001b[0;36m0.5301\u001b[0m, std: 0.6093\n",
      "2021-06-19 15:58:38,571 - INFO - Epoch/batch: 12/2869, ibatch: 56869, loss: \u001b[0;36m0.5199\u001b[0m, std: 0.6003\n",
      "2021-06-19 15:58:44,144 - INFO - Epoch/batch: 12/3020, ibatch: 57020, loss: \u001b[0;36m0.5309\u001b[0m, std: 0.6111\n",
      "2021-06-19 15:58:58,758 - INFO - loss: \u001b[0;32m0.5211\u001b[0m, std: 0.5861\n",
      "2021-06-19 15:58:59,584 - INFO - Epoch/batch: 12/3171, ibatch: 57171, loss: \u001b[0;36m0.5389\u001b[0m, std: 0.6140\n",
      "2021-06-19 15:59:05,489 - INFO - Epoch/batch: 12/3322, ibatch: 57322, loss: \u001b[0;36m0.5400\u001b[0m, std: 0.6168\n",
      "2021-06-19 15:59:11,166 - INFO - Epoch/batch: 12/3473, ibatch: 57473, loss: \u001b[0;36m0.5301\u001b[0m, std: 0.6088\n",
      "2021-06-19 15:59:25,963 - INFO - loss: \u001b[0;32m0.5218\u001b[0m, std: 0.5804\n",
      "2021-06-19 15:59:26,994 - INFO - Epoch/batch: 12/3624, ibatch: 57624, loss: \u001b[0;36m0.5344\u001b[0m, std: 0.6127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385: ReduceOnPlateau set learning rate to 0.0001413038609173874.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 15:59:32,715 - INFO - Epoch/batch: 12/3775, ibatch: 57775, loss: \u001b[0;36m0.5082\u001b[0m, std: 0.5929\n",
      "2021-06-19 15:59:38,167 - INFO - Epoch/batch: 12/3926, ibatch: 57926, loss: \u001b[0;36m0.5084\u001b[0m, std: 0.5895\n",
      "2021-06-19 15:59:52,850 - INFO - loss: \u001b[0;32m0.5202\u001b[0m, std: 0.5829\n",
      "2021-06-19 15:59:53,915 - INFO - Epoch/batch: 12/4077, ibatch: 58077, loss: \u001b[0;36m0.5452\u001b[0m, std: 0.6248\n",
      "2021-06-19 15:59:59,454 - INFO - Epoch/batch: 12/4228, ibatch: 58228, loss: \u001b[0;36m0.5166\u001b[0m, std: 0.5925\n",
      "2021-06-19 16:00:05,279 - INFO - Epoch/batch: 12/4379, ibatch: 58379, loss: \u001b[0;36m0.5355\u001b[0m, std: 0.6117\n",
      "2021-06-19 16:00:19,305 - INFO - loss: \u001b[0;32m0.5189\u001b[0m, std: 0.5952\n",
      "2021-06-19 16:00:20,420 - INFO - Epoch 12 average training loss: \u001b[0;46m0.5290\u001b[0m std: 0.6079\n",
      "2021-06-19 16:00:20,453 - INFO - Epoch 12 average validate loss: \u001b[0;46m0.5205\u001b[0m std: 0.5865\n",
      "2021-06-19 16:00:22,294 - INFO - Epoch/batch: 13/   0, ibatch: 58500, loss: \u001b[0;36m0.5210\u001b[0m, std: 0.5993\n",
      "2021-06-19 16:00:32,174 - INFO - loss: \u001b[0;32m0.5189\u001b[0m, std: 0.5949\n",
      "2021-06-19 16:00:38,231 - INFO - Epoch/batch: 13/ 151, ibatch: 58651, loss: \u001b[0;36m0.5146\u001b[0m, std: 0.5985\n",
      "2021-06-19 16:00:44,624 - INFO - Epoch/batch: 13/ 302, ibatch: 58802, loss: \u001b[0;36m0.5424\u001b[0m, std: 0.6219\n",
      "2021-06-19 16:01:00,607 - INFO - loss: \u001b[0;32m0.5195\u001b[0m, std: 0.5939\n",
      "2021-06-19 16:01:00,765 - INFO - Epoch/batch: 13/ 453, ibatch: 58953, loss: \u001b[0;36m0.5044\u001b[0m, std: 0.5881\n",
      "2021-06-19 16:01:06,652 - INFO - Epoch/batch: 13/ 604, ibatch: 59104, loss: \u001b[0;36m0.5348\u001b[0m, std: 0.6125\n",
      "2021-06-19 16:01:12,120 - INFO - Epoch/batch: 13/ 755, ibatch: 59255, loss: \u001b[0;36m0.5328\u001b[0m, std: 0.5996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 396: ReduceOnPlateau set learning rate to 0.00012717347482564865.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:01:27,408 - INFO - loss: \u001b[0;32m0.5202\u001b[0m, std: 0.5878\n",
      "2021-06-19 16:01:27,692 - INFO - Epoch/batch: 13/ 906, ibatch: 59406, loss: \u001b[0;36m0.5304\u001b[0m, std: 0.6196\n",
      "2021-06-19 16:01:33,520 - INFO - Epoch/batch: 13/1057, ibatch: 59557, loss: \u001b[0;36m0.5319\u001b[0m, std: 0.6090\n",
      "2021-06-19 16:01:39,357 - INFO - Epoch/batch: 13/1208, ibatch: 59708, loss: \u001b[0;36m0.5358\u001b[0m, std: 0.6123\n",
      "2021-06-19 16:01:54,273 - INFO - loss: \u001b[0;32m0.5205\u001b[0m, std: 0.5841\n",
      "2021-06-19 16:01:54,554 - INFO - Epoch/batch: 13/1359, ibatch: 59859, loss: \u001b[0;36m0.5185\u001b[0m, std: 0.5975\n",
      "2021-06-19 16:02:00,388 - INFO - Epoch/batch: 13/1510, ibatch: 60010, loss: \u001b[0;36m0.5188\u001b[0m, std: 0.5989\n",
      "2021-06-19 16:02:05,890 - INFO - Epoch/batch: 13/1661, ibatch: 60161, loss: \u001b[0;36m0.5219\u001b[0m, std: 0.6029\n",
      "2021-06-19 16:02:20,636 - INFO - loss: \u001b[0;32m0.5211\u001b[0m, std: 0.5821\n",
      "2021-06-19 16:02:21,114 - INFO - Epoch/batch: 13/1812, ibatch: 60312, loss: \u001b[0;36m0.5431\u001b[0m, std: 0.6254\n",
      "2021-06-19 16:02:26,435 - INFO - Epoch/batch: 13/1963, ibatch: 60463, loss: \u001b[0;36m0.5268\u001b[0m, std: 0.6037\n",
      "2021-06-19 16:02:32,099 - INFO - Epoch/batch: 13/2114, ibatch: 60614, loss: \u001b[0;36m0.5161\u001b[0m, std: 0.5997\n",
      "2021-06-19 16:02:46,486 - INFO - loss: \u001b[0;32m0.5199\u001b[0m, std: 0.5855\n",
      "2021-06-19 16:02:47,117 - INFO - Epoch/batch: 13/2265, ibatch: 60765, loss: \u001b[0;36m0.5298\u001b[0m, std: 0.6124\n",
      "2021-06-19 16:02:53,028 - INFO - Epoch/batch: 13/2416, ibatch: 60916, loss: \u001b[0;36m0.5406\u001b[0m, std: 0.6208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 407: ReduceOnPlateau set learning rate to 0.00011445612734308378.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:02:58,925 - INFO - Epoch/batch: 13/2567, ibatch: 61067, loss: \u001b[0;36m0.5332\u001b[0m, std: 0.6079\n",
      "2021-06-19 16:03:13,655 - INFO - loss: \u001b[0;32m0.5196\u001b[0m, std: 0.5886\n",
      "2021-06-19 16:03:14,334 - INFO - Epoch/batch: 13/2718, ibatch: 61218, loss: \u001b[0;36m0.5318\u001b[0m, std: 0.6120\n",
      "2021-06-19 16:03:19,980 - INFO - Epoch/batch: 13/2869, ibatch: 61369, loss: \u001b[0;36m0.5188\u001b[0m, std: 0.5969\n",
      "2021-06-19 16:03:25,441 - INFO - Epoch/batch: 13/3020, ibatch: 61520, loss: \u001b[0;36m0.5463\u001b[0m, std: 0.6166\n",
      "2021-06-19 16:03:39,613 - INFO - loss: \u001b[0;32m0.5191\u001b[0m, std: 0.5938\n",
      "2021-06-19 16:03:40,352 - INFO - Epoch/batch: 13/3171, ibatch: 61671, loss: \u001b[0;36m0.5270\u001b[0m, std: 0.6010\n",
      "2021-06-19 16:03:46,100 - INFO - Epoch/batch: 13/3322, ibatch: 61822, loss: \u001b[0;36m0.5429\u001b[0m, std: 0.6203\n",
      "2021-06-19 16:03:51,880 - INFO - Epoch/batch: 13/3473, ibatch: 61973, loss: \u001b[0;36m0.5192\u001b[0m, std: 0.6018\n",
      "2021-06-19 16:04:06,144 - INFO - loss: \u001b[0;32m0.5194\u001b[0m, std: 0.5905\n",
      "2021-06-19 16:04:06,912 - INFO - Epoch/batch: 13/3624, ibatch: 62124, loss: \u001b[0;36m0.5226\u001b[0m, std: 0.5973\n",
      "2021-06-19 16:04:12,078 - INFO - Epoch/batch: 13/3775, ibatch: 62275, loss: \u001b[0;36m0.5253\u001b[0m, std: 0.6103\n",
      "2021-06-19 16:04:17,441 - INFO - Epoch/batch: 13/3926, ibatch: 62426, loss: \u001b[0;36m0.5363\u001b[0m, std: 0.6132\n",
      "2021-06-19 16:04:31,657 - INFO - loss: \u001b[0;32m0.5197\u001b[0m, std: 0.5884\n",
      "2021-06-19 16:04:32,698 - INFO - Epoch/batch: 13/4077, ibatch: 62577, loss: \u001b[0;36m0.5461\u001b[0m, std: 0.6268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 418: ReduceOnPlateau set learning rate to 0.00010301051460877541.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:04:38,797 - INFO - Epoch/batch: 13/4228, ibatch: 62728, loss: \u001b[0;36m0.5258\u001b[0m, std: 0.6000\n",
      "2021-06-19 16:04:45,164 - INFO - Epoch/batch: 13/4379, ibatch: 62879, loss: \u001b[0;36m0.5184\u001b[0m, std: 0.5946\n",
      "2021-06-19 16:05:00,228 - INFO - loss: \u001b[0;32m0.5191\u001b[0m, std: 0.5934\n",
      "2021-06-19 16:05:01,648 - INFO - Epoch 13 average training loss: \u001b[0;46m0.5286\u001b[0m std: 0.6075\n",
      "2021-06-19 16:05:01,652 - INFO - Epoch 13 average validate loss: \u001b[0;46m0.5197\u001b[0m std: 0.5894\n",
      "2021-06-19 16:05:03,485 - INFO - Epoch/batch: 14/   0, ibatch: 63000, loss: \u001b[0;36m0.5179\u001b[0m, std: 0.6014\n",
      "2021-06-19 16:05:13,258 - INFO - loss: \u001b[0;32m0.5191\u001b[0m, std: 0.5935\n",
      "2021-06-19 16:05:18,767 - INFO - Epoch/batch: 14/ 151, ibatch: 63151, loss: \u001b[0;36m0.5260\u001b[0m, std: 0.6137\n",
      "2021-06-19 16:05:24,504 - INFO - Epoch/batch: 14/ 302, ibatch: 63302, loss: \u001b[0;36m0.5210\u001b[0m, std: 0.5987\n",
      "2021-06-19 16:05:40,008 - INFO - loss: \u001b[0;32m0.5193\u001b[0m, std: 0.5911\n",
      "2021-06-19 16:05:40,144 - INFO - Epoch/batch: 14/ 453, ibatch: 63453, loss: \u001b[0;36m0.5214\u001b[0m, std: 0.6081\n",
      "2021-06-19 16:05:46,185 - INFO - Epoch/batch: 14/ 604, ibatch: 63604, loss: \u001b[0;36m0.5276\u001b[0m, std: 0.6161\n",
      "2021-06-19 16:05:52,103 - INFO - Epoch/batch: 14/ 755, ibatch: 63755, loss: \u001b[0;36m0.5285\u001b[0m, std: 0.6042\n",
      "2021-06-19 16:06:07,173 - INFO - loss: \u001b[0;32m0.5197\u001b[0m, std: 0.5869\n",
      "2021-06-19 16:06:07,350 - INFO - Epoch/batch: 14/ 906, ibatch: 63906, loss: \u001b[0;36m0.5161\u001b[0m, std: 0.5975\n",
      "2021-06-19 16:06:13,090 - INFO - Epoch/batch: 14/1057, ibatch: 64057, loss: \u001b[0;36m0.5347\u001b[0m, std: 0.6064\n",
      "2021-06-19 16:06:18,971 - INFO - Epoch/batch: 14/1208, ibatch: 64208, loss: \u001b[0;36m0.5459\u001b[0m, std: 0.6195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 429: ReduceOnPlateau set learning rate to 9.270946314789788e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:06:34,135 - INFO - loss: \u001b[0;32m0.5192\u001b[0m, std: 0.5920\n",
      "2021-06-19 16:06:34,486 - INFO - Epoch/batch: 14/1359, ibatch: 64359, loss: \u001b[0;36m0.5182\u001b[0m, std: 0.5950\n",
      "2021-06-19 16:06:39,755 - INFO - Epoch/batch: 14/1510, ibatch: 64510, loss: \u001b[0;36m0.5191\u001b[0m, std: 0.5974\n",
      "2021-06-19 16:06:45,505 - INFO - Epoch/batch: 14/1661, ibatch: 64661, loss: \u001b[0;36m0.5368\u001b[0m, std: 0.6172\n",
      "2021-06-19 16:07:00,516 - INFO - loss: \u001b[0;32m0.5192\u001b[0m, std: 0.5938\n",
      "2021-06-19 16:07:01,010 - INFO - Epoch/batch: 14/1812, ibatch: 64812, loss: \u001b[0;36m0.5221\u001b[0m, std: 0.6004\n",
      "2021-06-19 16:07:06,873 - INFO - Epoch/batch: 14/1963, ibatch: 64963, loss: \u001b[0;36m0.5376\u001b[0m, std: 0.6182\n",
      "2021-06-19 16:07:12,341 - INFO - Epoch/batch: 14/2114, ibatch: 65114, loss: \u001b[0;36m0.5260\u001b[0m, std: 0.6040\n",
      "2021-06-19 16:07:26,948 - INFO - loss: \u001b[0;32m0.5194\u001b[0m, std: 0.5897\n",
      "2021-06-19 16:07:27,487 - INFO - Epoch/batch: 14/2265, ibatch: 65265, loss: \u001b[0;36m0.5347\u001b[0m, std: 0.6201\n",
      "2021-06-19 16:07:32,872 - INFO - Epoch/batch: 14/2416, ibatch: 65416, loss: \u001b[0;36m0.5224\u001b[0m, std: 0.5936\n",
      "2021-06-19 16:07:38,427 - INFO - Epoch/batch: 14/2567, ibatch: 65567, loss: \u001b[0;36m0.5278\u001b[0m, std: 0.6068\n",
      "2021-06-19 16:07:52,748 - INFO - loss: \u001b[0;32m0.5194\u001b[0m, std: 0.5931\n",
      "2021-06-19 16:07:53,472 - INFO - Epoch/batch: 14/2718, ibatch: 65718, loss: \u001b[0;36m0.5226\u001b[0m, std: 0.6018\n",
      "2021-06-19 16:07:59,232 - INFO - Epoch/batch: 14/2869, ibatch: 65869, loss: \u001b[0;36m0.5363\u001b[0m, std: 0.6171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 440: ReduceOnPlateau set learning rate to 8.343851683310809e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:08:04,647 - INFO - Epoch/batch: 14/3020, ibatch: 66020, loss: \u001b[0;36m0.5229\u001b[0m, std: 0.6054\n",
      "2021-06-19 16:08:19,056 - INFO - loss: \u001b[0;32m0.5193\u001b[0m, std: 0.5917\n",
      "2021-06-19 16:08:19,909 - INFO - Epoch/batch: 14/3171, ibatch: 66171, loss: \u001b[0;36m0.5233\u001b[0m, std: 0.6057\n",
      "2021-06-19 16:08:25,676 - INFO - Epoch/batch: 14/3322, ibatch: 66322, loss: \u001b[0;36m0.5202\u001b[0m, std: 0.6002\n",
      "2021-06-19 16:08:31,252 - INFO - Epoch/batch: 14/3473, ibatch: 66473, loss: \u001b[0;36m0.5493\u001b[0m, std: 0.6201\n",
      "2021-06-19 16:08:45,519 - INFO - loss: \u001b[0;32m0.5204\u001b[0m, std: 0.5869\n",
      "2021-06-19 16:08:46,471 - INFO - Epoch/batch: 14/3624, ibatch: 66624, loss: \u001b[0;36m0.5441\u001b[0m, std: 0.6191\n",
      "2021-06-19 16:08:51,858 - INFO - Epoch/batch: 14/3775, ibatch: 66775, loss: \u001b[0;36m0.5098\u001b[0m, std: 0.5965\n",
      "2021-06-19 16:08:57,304 - INFO - Epoch/batch: 14/3926, ibatch: 66926, loss: \u001b[0;36m0.5422\u001b[0m, std: 0.6133\n",
      "2021-06-19 16:09:12,035 - INFO - loss: \u001b[0;32m0.5195\u001b[0m, std: 0.5925\n",
      "2021-06-19 16:09:13,034 - INFO - Epoch/batch: 14/4077, ibatch: 67077, loss: \u001b[0;36m0.5217\u001b[0m, std: 0.5987\n",
      "2021-06-19 16:09:18,744 - INFO - Epoch/batch: 14/4228, ibatch: 67228, loss: \u001b[0;36m0.5293\u001b[0m, std: 0.6097\n",
      "2021-06-19 16:09:24,453 - INFO - Epoch/batch: 14/4379, ibatch: 67379, loss: \u001b[0;36m0.5302\u001b[0m, std: 0.6017\n",
      "2021-06-19 16:09:38,746 - INFO - loss: \u001b[0;32m0.5196\u001b[0m, std: 0.5868\n",
      "2021-06-19 16:09:39,950 - INFO - Epoch 14 average training loss: \u001b[0;46m0.5285\u001b[0m std: 0.6076\n",
      "2021-06-19 16:09:39,954 - INFO - Epoch 14 average validate loss: \u001b[0;46m0.5195\u001b[0m std: 0.5907\n",
      "2021-06-19 16:09:41,918 - INFO - Epoch/batch: 15/   0, ibatch: 67500, loss: \u001b[0;36m0.5395\u001b[0m, std: 0.6278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 451: ReduceOnPlateau set learning rate to 7.509466514979728e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:09:51,482 - INFO - loss: \u001b[0;32m0.5196\u001b[0m, std: 0.5866\n",
      "2021-06-19 16:09:57,096 - INFO - Epoch/batch: 15/ 151, ibatch: 67651, loss: \u001b[0;36m0.5404\u001b[0m, std: 0.6140\n",
      "2021-06-19 16:10:02,615 - INFO - Epoch/batch: 15/ 302, ibatch: 67802, loss: \u001b[0;36m0.5165\u001b[0m, std: 0.5865\n",
      "2021-06-19 16:10:18,157 - INFO - loss: \u001b[0;32m0.5196\u001b[0m, std: 0.5889\n",
      "2021-06-19 16:10:18,289 - INFO - Epoch/batch: 15/ 453, ibatch: 67953, loss: \u001b[0;36m0.5303\u001b[0m, std: 0.6005\n",
      "2021-06-19 16:10:24,070 - INFO - Epoch/batch: 15/ 604, ibatch: 68104, loss: \u001b[0;36m0.5534\u001b[0m, std: 0.6338\n",
      "2021-06-19 16:10:29,675 - INFO - Epoch/batch: 15/ 755, ibatch: 68255, loss: \u001b[0;36m0.5229\u001b[0m, std: 0.6043\n",
      "2021-06-19 16:10:44,636 - INFO - loss: \u001b[0;32m0.5194\u001b[0m, std: 0.5918\n",
      "2021-06-19 16:10:44,876 - INFO - Epoch/batch: 15/ 906, ibatch: 68406, loss: \u001b[0;36m0.5274\u001b[0m, std: 0.6076\n",
      "2021-06-19 16:10:50,088 - INFO - Epoch/batch: 15/1057, ibatch: 68557, loss: \u001b[0;36m0.5335\u001b[0m, std: 0.6128\n",
      "2021-06-19 16:10:55,669 - INFO - Epoch/batch: 15/1208, ibatch: 68708, loss: \u001b[0;36m0.5366\u001b[0m, std: 0.6165\n",
      "2021-06-19 16:11:10,159 - INFO - loss: \u001b[0;32m0.5197\u001b[0m, std: 0.5887\n",
      "2021-06-19 16:11:10,516 - INFO - Epoch/batch: 15/1359, ibatch: 68859, loss: \u001b[0;36m0.5285\u001b[0m, std: 0.6047\n",
      "2021-06-19 16:11:16,646 - INFO - Epoch/batch: 15/1510, ibatch: 69010, loss: \u001b[0;36m0.5364\u001b[0m, std: 0.6197\n",
      "2021-06-19 16:11:22,492 - INFO - Epoch/batch: 15/1661, ibatch: 69161, loss: \u001b[0;36m0.5446\u001b[0m, std: 0.6187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 462: ReduceOnPlateau set learning rate to 6.758519863481756e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:11:36,724 - INFO - loss: \u001b[0;32m0.5190\u001b[0m, std: 0.5960\n",
      "2021-06-19 16:11:37,220 - INFO - Epoch/batch: 15/1812, ibatch: 69312, loss: \u001b[0;36m0.5200\u001b[0m, std: 0.5900\n",
      "2021-06-19 16:11:42,921 - INFO - Epoch/batch: 15/1963, ibatch: 69463, loss: \u001b[0;36m0.5086\u001b[0m, std: 0.5969\n",
      "2021-06-19 16:11:48,850 - INFO - Epoch/batch: 15/2114, ibatch: 69614, loss: \u001b[0;36m0.5403\u001b[0m, std: 0.6168\n",
      "2021-06-19 16:12:03,533 - INFO - loss: \u001b[0;32m0.5196\u001b[0m, std: 0.5901\n",
      "2021-06-19 16:12:04,130 - INFO - Epoch/batch: 15/2265, ibatch: 69765, loss: \u001b[0;36m0.5230\u001b[0m, std: 0.6042\n",
      "2021-06-19 16:12:09,496 - INFO - Epoch/batch: 15/2416, ibatch: 69916, loss: \u001b[0;36m0.5327\u001b[0m, std: 0.6154\n",
      "2021-06-19 16:12:14,908 - INFO - Epoch/batch: 15/2567, ibatch: 70067, loss: \u001b[0;36m0.5161\u001b[0m, std: 0.5858\n",
      "2021-06-19 16:12:29,397 - INFO - loss: \u001b[0;32m0.5193\u001b[0m, std: 0.5938\n",
      "2021-06-19 16:12:29,983 - INFO - Epoch/batch: 15/2718, ibatch: 70218, loss: \u001b[0;36m0.5095\u001b[0m, std: 0.5919\n",
      "2021-06-19 16:12:35,497 - INFO - Epoch/batch: 15/2869, ibatch: 70369, loss: \u001b[0;36m0.5336\u001b[0m, std: 0.6175\n",
      "2021-06-19 16:12:41,039 - INFO - Epoch/batch: 15/3020, ibatch: 70520, loss: \u001b[0;36m0.5338\u001b[0m, std: 0.6114\n",
      "2021-06-19 16:12:55,241 - INFO - loss: \u001b[0;32m0.5193\u001b[0m, std: 0.5938\n",
      "2021-06-19 16:12:56,023 - INFO - Epoch/batch: 15/3171, ibatch: 70671, loss: \u001b[0;36m0.5282\u001b[0m, std: 0.6007\n",
      "2021-06-19 16:13:01,939 - INFO - Epoch/batch: 15/3322, ibatch: 70822, loss: \u001b[0;36m0.5301\u001b[0m, std: 0.6128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 473: ReduceOnPlateau set learning rate to 6.0826678771335806e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:13:07,461 - INFO - Epoch/batch: 15/3473, ibatch: 70973, loss: \u001b[0;36m0.4977\u001b[0m, std: 0.5926\n",
      "2021-06-19 16:13:21,670 - INFO - loss: \u001b[0;32m0.5192\u001b[0m, std: 0.5901\n",
      "2021-06-19 16:13:22,647 - INFO - Epoch/batch: 15/3624, ibatch: 71124, loss: \u001b[0;36m0.5213\u001b[0m, std: 0.6024\n",
      "2021-06-19 16:13:28,502 - INFO - Epoch/batch: 15/3775, ibatch: 71275, loss: \u001b[0;36m0.5261\u001b[0m, std: 0.6067\n",
      "2021-06-19 16:13:34,403 - INFO - Epoch/batch: 15/3926, ibatch: 71426, loss: \u001b[0;36m0.5398\u001b[0m, std: 0.6175\n",
      "2021-06-19 16:13:48,951 - INFO - loss: \u001b[0;32m0.5201\u001b[0m, std: 0.5829\n",
      "2021-06-19 16:13:50,070 - INFO - Epoch/batch: 15/4077, ibatch: 71577, loss: \u001b[0;36m0.5381\u001b[0m, std: 0.6109\n",
      "2021-06-19 16:13:55,709 - INFO - Epoch/batch: 15/4228, ibatch: 71728, loss: \u001b[0;36m0.5185\u001b[0m, std: 0.6024\n",
      "2021-06-19 16:14:01,639 - INFO - Epoch/batch: 15/4379, ibatch: 71879, loss: \u001b[0;36m0.5374\u001b[0m, std: 0.6145\n",
      "2021-06-19 16:14:15,973 - INFO - loss: \u001b[0;32m0.5194\u001b[0m, std: 0.5908\n",
      "2021-06-19 16:14:17,113 - INFO - Epoch 15 average training loss: \u001b[0;46m0.5283\u001b[0m std: 0.6069\n",
      "2021-06-19 16:14:17,150 - INFO - Epoch 15 average validate loss: \u001b[0;46m0.5195\u001b[0m std: 0.5903\n",
      "2021-06-19 16:14:18,931 - INFO - Epoch/batch: 16/   0, ibatch: 72000, loss: \u001b[0;36m0.5227\u001b[0m, std: 0.5970\n",
      "2021-06-19 16:14:28,367 - INFO - loss: \u001b[0;32m0.5194\u001b[0m, std: 0.5908\n",
      "2021-06-19 16:14:34,084 - INFO - Epoch/batch: 16/ 151, ibatch: 72151, loss: \u001b[0;36m0.5176\u001b[0m, std: 0.5947\n",
      "2021-06-19 16:14:39,673 - INFO - Epoch/batch: 16/ 302, ibatch: 72302, loss: \u001b[0;36m0.5056\u001b[0m, std: 0.5807\n",
      "2021-06-19 16:14:55,107 - INFO - loss: \u001b[0;32m0.5191\u001b[0m, std: 0.5923\n",
      "2021-06-19 16:14:55,237 - INFO - Epoch/batch: 16/ 453, ibatch: 72453, loss: \u001b[0;36m0.5378\u001b[0m, std: 0.6200\n",
      "2021-06-19 16:15:01,023 - INFO - Epoch/batch: 16/ 604, ibatch: 72604, loss: \u001b[0;36m0.5444\u001b[0m, std: 0.6206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 485: ReduceOnPlateau set learning rate to 5.4744010894202224e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:15:06,743 - INFO - Epoch/batch: 16/ 755, ibatch: 72755, loss: \u001b[0;36m0.5282\u001b[0m, std: 0.6114\n",
      "2021-06-19 16:15:21,760 - INFO - loss: \u001b[0;32m0.5191\u001b[0m, std: 0.5920\n",
      "2021-06-19 16:15:22,005 - INFO - Epoch/batch: 16/ 906, ibatch: 72906, loss: \u001b[0;36m0.5242\u001b[0m, std: 0.5978\n",
      "2021-06-19 16:15:27,425 - INFO - Epoch/batch: 16/1057, ibatch: 73057, loss: \u001b[0;36m0.5261\u001b[0m, std: 0.6068\n",
      "2021-06-19 16:15:32,949 - INFO - Epoch/batch: 16/1208, ibatch: 73208, loss: \u001b[0;36m0.5273\u001b[0m, std: 0.5990\n",
      "2021-06-19 16:15:47,788 - INFO - loss: \u001b[0;32m0.5192\u001b[0m, std: 0.5906\n",
      "2021-06-19 16:15:48,116 - INFO - Epoch/batch: 16/1359, ibatch: 73359, loss: \u001b[0;36m0.5305\u001b[0m, std: 0.6100\n",
      "2021-06-19 16:15:53,757 - INFO - Epoch/batch: 16/1510, ibatch: 73510, loss: \u001b[0;36m0.5300\u001b[0m, std: 0.6101\n",
      "2021-06-19 16:15:59,048 - INFO - Epoch/batch: 16/1661, ibatch: 73661, loss: \u001b[0;36m0.5215\u001b[0m, std: 0.6005\n",
      "2021-06-19 16:16:13,232 - INFO - loss: \u001b[0;32m0.5199\u001b[0m, std: 0.5880\n",
      "2021-06-19 16:16:13,646 - INFO - Epoch/batch: 16/1812, ibatch: 73812, loss: \u001b[0;36m0.5334\u001b[0m, std: 0.6179\n",
      "2021-06-19 16:16:19,017 - INFO - Epoch/batch: 16/1963, ibatch: 73963, loss: \u001b[0;36m0.5164\u001b[0m, std: 0.5917\n",
      "2021-06-19 16:16:24,629 - INFO - Epoch/batch: 16/2114, ibatch: 74114, loss: \u001b[0;36m0.5530\u001b[0m, std: 0.6319\n",
      "2021-06-19 16:16:39,033 - INFO - loss: \u001b[0;32m0.5216\u001b[0m, std: 0.5789\n",
      "2021-06-19 16:16:39,618 - INFO - Epoch/batch: 16/2265, ibatch: 74265, loss: \u001b[0;36m0.5391\u001b[0m, std: 0.6143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 496: ReduceOnPlateau set learning rate to 4.9269609804782e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:16:45,187 - INFO - Epoch/batch: 16/2416, ibatch: 74416, loss: \u001b[0;36m0.5165\u001b[0m, std: 0.5952\n",
      "2021-06-19 16:16:50,853 - INFO - Epoch/batch: 16/2567, ibatch: 74567, loss: \u001b[0;36m0.5302\u001b[0m, std: 0.6077\n",
      "2021-06-19 16:17:06,051 - INFO - loss: \u001b[0;32m0.5195\u001b[0m, std: 0.5885\n",
      "2021-06-19 16:17:06,741 - INFO - Epoch/batch: 16/2718, ibatch: 74718, loss: \u001b[0;36m0.5268\u001b[0m, std: 0.6073\n",
      "2021-06-19 16:17:12,768 - INFO - Epoch/batch: 16/2869, ibatch: 74869, loss: \u001b[0;36m0.5331\u001b[0m, std: 0.6135\n",
      "2021-06-19 16:17:18,425 - INFO - Epoch/batch: 16/3020, ibatch: 75020, loss: \u001b[0;36m0.5356\u001b[0m, std: 0.6084\n",
      "2021-06-19 16:17:32,728 - INFO - loss: \u001b[0;32m0.5196\u001b[0m, std: 0.5875\n",
      "2021-06-19 16:17:33,513 - INFO - Epoch/batch: 16/3171, ibatch: 75171, loss: \u001b[0;36m0.5380\u001b[0m, std: 0.6220\n",
      "2021-06-19 16:17:39,245 - INFO - Epoch/batch: 16/3322, ibatch: 75322, loss: \u001b[0;36m0.5343\u001b[0m, std: 0.6089\n",
      "2021-06-19 16:17:44,777 - INFO - Epoch/batch: 16/3473, ibatch: 75473, loss: \u001b[0;36m0.5175\u001b[0m, std: 0.6038\n",
      "2021-06-19 16:17:58,966 - INFO - loss: \u001b[0;32m0.5197\u001b[0m, std: 0.5878\n",
      "2021-06-19 16:18:00,074 - INFO - Epoch/batch: 16/3624, ibatch: 75624, loss: \u001b[0;36m0.5113\u001b[0m, std: 0.5852\n",
      "2021-06-19 16:18:05,972 - INFO - Epoch/batch: 16/3775, ibatch: 75775, loss: \u001b[0;36m0.5242\u001b[0m, std: 0.6112\n",
      "2021-06-19 16:18:12,147 - INFO - Epoch/batch: 16/3926, ibatch: 75926, loss: \u001b[0;36m0.5234\u001b[0m, std: 0.6033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 507: ReduceOnPlateau set learning rate to 4.43426488243038e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:18:27,008 - INFO - loss: \u001b[0;32m0.5199\u001b[0m, std: 0.5880\n",
      "2021-06-19 16:18:28,061 - INFO - Epoch/batch: 16/4077, ibatch: 76077, loss: \u001b[0;36m0.5437\u001b[0m, std: 0.6159\n",
      "2021-06-19 16:18:33,827 - INFO - Epoch/batch: 16/4228, ibatch: 76228, loss: \u001b[0;36m0.5220\u001b[0m, std: 0.5959\n",
      "2021-06-19 16:18:39,813 - INFO - Epoch/batch: 16/4379, ibatch: 76379, loss: \u001b[0;36m0.5214\u001b[0m, std: 0.6085\n",
      "2021-06-19 16:18:54,451 - INFO - loss: \u001b[0;32m0.5192\u001b[0m, std: 0.5925\n",
      "2021-06-19 16:18:55,644 - INFO - Epoch 16 average training loss: \u001b[0;46m0.5279\u001b[0m std: 0.6066\n",
      "2021-06-19 16:18:55,650 - INFO - Epoch 16 average validate loss: \u001b[0;46m0.5197\u001b[0m std: 0.5888\n",
      "2021-06-19 16:18:57,498 - INFO - Epoch/batch: 17/   0, ibatch: 76500, loss: \u001b[0;36m0.5198\u001b[0m, std: 0.5997\n",
      "2021-06-19 16:19:07,186 - INFO - loss: \u001b[0;32m0.5192\u001b[0m, std: 0.5926\n",
      "2021-06-19 16:19:13,531 - INFO - Epoch/batch: 17/ 151, ibatch: 76651, loss: \u001b[0;36m0.5441\u001b[0m, std: 0.6214\n",
      "2021-06-19 16:19:19,554 - INFO - Epoch/batch: 17/ 302, ibatch: 76802, loss: \u001b[0;36m0.5315\u001b[0m, std: 0.6112\n",
      "2021-06-19 16:19:34,704 - INFO - loss: \u001b[0;32m0.5197\u001b[0m, std: 0.5889\n",
      "2021-06-19 16:19:34,830 - INFO - Epoch/batch: 17/ 453, ibatch: 76953, loss: \u001b[0;36m0.5191\u001b[0m, std: 0.5952\n",
      "2021-06-19 16:19:40,576 - INFO - Epoch/batch: 17/ 604, ibatch: 77104, loss: \u001b[0;36m0.5143\u001b[0m, std: 0.5951\n",
      "2021-06-19 16:19:46,408 - INFO - Epoch/batch: 17/ 755, ibatch: 77255, loss: \u001b[0;36m0.5363\u001b[0m, std: 0.6215\n",
      "2021-06-19 16:20:01,145 - INFO - loss: \u001b[0;32m0.5203\u001b[0m, std: 0.5862\n",
      "2021-06-19 16:20:01,464 - INFO - Epoch/batch: 17/ 906, ibatch: 77406, loss: \u001b[0;36m0.5492\u001b[0m, std: 0.6286\n",
      "2021-06-19 16:20:07,214 - INFO - Epoch/batch: 17/1057, ibatch: 77557, loss: \u001b[0;36m0.5300\u001b[0m, std: 0.6063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 518: ReduceOnPlateau set learning rate to 3.990838394187342e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:20:12,487 - INFO - Epoch/batch: 17/1208, ibatch: 77708, loss: \u001b[0;36m0.5170\u001b[0m, std: 0.5953\n",
      "2021-06-19 16:20:27,420 - INFO - loss: \u001b[0;32m0.5196\u001b[0m, std: 0.5898\n",
      "2021-06-19 16:20:27,773 - INFO - Epoch/batch: 17/1359, ibatch: 77859, loss: \u001b[0;36m0.5201\u001b[0m, std: 0.6078\n",
      "2021-06-19 16:20:33,437 - INFO - Epoch/batch: 17/1510, ibatch: 78010, loss: \u001b[0;36m0.5283\u001b[0m, std: 0.6133\n",
      "2021-06-19 16:20:39,093 - INFO - Epoch/batch: 17/1661, ibatch: 78161, loss: \u001b[0;36m0.5220\u001b[0m, std: 0.5990\n",
      "2021-06-19 16:20:54,171 - INFO - loss: \u001b[0;32m0.5197\u001b[0m, std: 0.5884\n",
      "2021-06-19 16:20:54,726 - INFO - Epoch/batch: 17/1812, ibatch: 78312, loss: \u001b[0;36m0.5253\u001b[0m, std: 0.6085\n",
      "2021-06-19 16:21:00,734 - INFO - Epoch/batch: 17/1963, ibatch: 78463, loss: \u001b[0;36m0.5202\u001b[0m, std: 0.5980\n",
      "2021-06-19 16:21:06,334 - INFO - Epoch/batch: 17/2114, ibatch: 78614, loss: \u001b[0;36m0.5266\u001b[0m, std: 0.6061\n",
      "2021-06-19 16:21:21,245 - INFO - loss: \u001b[0;32m0.5197\u001b[0m, std: 0.5869\n",
      "2021-06-19 16:21:21,777 - INFO - Epoch/batch: 17/2265, ibatch: 78765, loss: \u001b[0;36m0.5320\u001b[0m, std: 0.6077\n",
      "2021-06-19 16:21:28,294 - INFO - Epoch/batch: 17/2416, ibatch: 78916, loss: \u001b[0;36m0.5366\u001b[0m, std: 0.6185\n",
      "2021-06-19 16:21:34,462 - INFO - Epoch/batch: 17/2567, ibatch: 79067, loss: \u001b[0;36m0.5052\u001b[0m, std: 0.5957\n",
      "2021-06-19 16:21:50,058 - INFO - loss: \u001b[0;32m0.5198\u001b[0m, std: 0.5880\n",
      "2021-06-19 16:21:50,897 - INFO - Epoch/batch: 17/2718, ibatch: 79218, loss: \u001b[0;36m0.5326\u001b[0m, std: 0.6104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 529: ReduceOnPlateau set learning rate to 3.591754554768608e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:21:57,142 - INFO - Epoch/batch: 17/2869, ibatch: 79369, loss: \u001b[0;36m0.5300\u001b[0m, std: 0.6088\n",
      "2021-06-19 16:22:02,987 - INFO - Epoch/batch: 17/3020, ibatch: 79520, loss: \u001b[0;36m0.5523\u001b[0m, std: 0.6174\n",
      "2021-06-19 16:22:17,248 - INFO - loss: \u001b[0;32m0.5201\u001b[0m, std: 0.5865\n",
      "2021-06-19 16:22:18,097 - INFO - Epoch/batch: 17/3171, ibatch: 79671, loss: \u001b[0;36m0.5033\u001b[0m, std: 0.5789\n",
      "2021-06-19 16:22:23,807 - INFO - Epoch/batch: 17/3322, ibatch: 79822, loss: \u001b[0;36m0.5336\u001b[0m, std: 0.6088\n",
      "2021-06-19 16:22:29,364 - INFO - Epoch/batch: 17/3473, ibatch: 79973, loss: \u001b[0;36m0.5211\u001b[0m, std: 0.5974\n",
      "2021-06-19 16:22:44,160 - INFO - loss: \u001b[0;32m0.5194\u001b[0m, std: 0.5914\n",
      "2021-06-19 16:22:45,143 - INFO - Epoch/batch: 17/3624, ibatch: 80124, loss: \u001b[0;36m0.5440\u001b[0m, std: 0.6175\n",
      "2021-06-19 16:22:50,813 - INFO - Epoch/batch: 17/3775, ibatch: 80275, loss: \u001b[0;36m0.5292\u001b[0m, std: 0.6160\n",
      "2021-06-19 16:22:56,221 - INFO - Epoch/batch: 17/3926, ibatch: 80426, loss: \u001b[0;36m0.5103\u001b[0m, std: 0.5976\n",
      "2021-06-19 16:23:10,061 - INFO - loss: \u001b[0;32m0.5206\u001b[0m, std: 0.5829\n",
      "2021-06-19 16:23:11,097 - INFO - Epoch/batch: 17/4077, ibatch: 80577, loss: \u001b[0;36m0.5556\u001b[0m, std: 0.6323\n",
      "2021-06-19 16:23:16,781 - INFO - Epoch/batch: 17/4228, ibatch: 80728, loss: \u001b[0;36m0.5318\u001b[0m, std: 0.6023\n",
      "2021-06-19 16:23:22,430 - INFO - Epoch/batch: 17/4379, ibatch: 80879, loss: \u001b[0;36m0.5188\u001b[0m, std: 0.5893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 540: ReduceOnPlateau set learning rate to 3.232579099291747e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:23:36,341 - INFO - loss: \u001b[0;32m0.5194\u001b[0m, std: 0.5895\n",
      "2021-06-19 16:23:37,563 - INFO - Epoch 17 average training loss: \u001b[0;46m0.5280\u001b[0m std: 0.6071\n",
      "2021-06-19 16:23:37,567 - INFO - Epoch 17 average validate loss: \u001b[0;46m0.5198\u001b[0m std: 0.5883\n",
      "2021-06-19 16:23:39,502 - INFO - Epoch/batch: 18/   0, ibatch: 81000, loss: \u001b[0;36m0.5182\u001b[0m, std: 0.6060\n",
      "2021-06-19 16:23:49,272 - INFO - loss: \u001b[0;32m0.5194\u001b[0m, std: 0.5895\n",
      "2021-06-19 16:23:55,314 - INFO - Epoch/batch: 18/ 151, ibatch: 81151, loss: \u001b[0;36m0.5421\u001b[0m, std: 0.6212\n",
      "2021-06-19 16:24:00,901 - INFO - Epoch/batch: 18/ 302, ibatch: 81302, loss: \u001b[0;36m0.5289\u001b[0m, std: 0.6075\n",
      "2021-06-19 16:24:15,952 - INFO - loss: \u001b[0;32m0.5196\u001b[0m, std: 0.5892\n",
      "2021-06-19 16:24:16,093 - INFO - Epoch/batch: 18/ 453, ibatch: 81453, loss: \u001b[0;36m0.5246\u001b[0m, std: 0.6049\n",
      "2021-06-19 16:24:22,104 - INFO - Epoch/batch: 18/ 604, ibatch: 81604, loss: \u001b[0;36m0.5084\u001b[0m, std: 0.5886\n",
      "2021-06-19 16:24:28,070 - INFO - Epoch/batch: 18/ 755, ibatch: 81755, loss: \u001b[0;36m0.5241\u001b[0m, std: 0.5955\n",
      "2021-06-19 16:24:42,634 - INFO - loss: \u001b[0;32m0.5199\u001b[0m, std: 0.5880\n",
      "2021-06-19 16:24:42,882 - INFO - Epoch/batch: 18/ 906, ibatch: 81906, loss: \u001b[0;36m0.5303\u001b[0m, std: 0.6136\n",
      "2021-06-19 16:24:48,551 - INFO - Epoch/batch: 18/1057, ibatch: 82057, loss: \u001b[0;36m0.5247\u001b[0m, std: 0.5979\n",
      "2021-06-19 16:24:54,217 - INFO - Epoch/batch: 18/1208, ibatch: 82208, loss: \u001b[0;36m0.5304\u001b[0m, std: 0.6150\n",
      "2021-06-19 16:25:09,227 - INFO - loss: \u001b[0;32m0.5194\u001b[0m, std: 0.5896\n",
      "2021-06-19 16:25:09,580 - INFO - Epoch/batch: 18/1359, ibatch: 82359, loss: \u001b[0;36m0.5166\u001b[0m, std: 0.5964\n",
      "2021-06-19 16:25:15,050 - INFO - Epoch/batch: 18/1510, ibatch: 82510, loss: \u001b[0;36m0.5310\u001b[0m, std: 0.6091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 551: ReduceOnPlateau set learning rate to 2.9093211893625727e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:25:20,742 - INFO - Epoch/batch: 18/1661, ibatch: 82661, loss: \u001b[0;36m0.5365\u001b[0m, std: 0.6175\n",
      "2021-06-19 16:25:35,846 - INFO - loss: \u001b[0;32m0.5200\u001b[0m, std: 0.5870\n",
      "2021-06-19 16:25:36,327 - INFO - Epoch/batch: 18/1812, ibatch: 82812, loss: \u001b[0;36m0.5425\u001b[0m, std: 0.6224\n",
      "2021-06-19 16:25:41,934 - INFO - Epoch/batch: 18/1963, ibatch: 82963, loss: \u001b[0;36m0.5189\u001b[0m, std: 0.5988\n",
      "2021-06-19 16:25:47,660 - INFO - Epoch/batch: 18/2114, ibatch: 83114, loss: \u001b[0;36m0.5252\u001b[0m, std: 0.5983\n",
      "2021-06-19 16:26:02,392 - INFO - loss: \u001b[0;32m0.5205\u001b[0m, std: 0.5855\n",
      "2021-06-19 16:26:02,926 - INFO - Epoch/batch: 18/2265, ibatch: 83265, loss: \u001b[0;36m0.5377\u001b[0m, std: 0.6228\n",
      "2021-06-19 16:26:08,672 - INFO - Epoch/batch: 18/2416, ibatch: 83416, loss: \u001b[0;36m0.5078\u001b[0m, std: 0.5872\n",
      "2021-06-19 16:26:14,636 - INFO - Epoch/batch: 18/2567, ibatch: 83567, loss: \u001b[0;36m0.5186\u001b[0m, std: 0.5991\n",
      "2021-06-19 16:26:28,728 - INFO - loss: \u001b[0;32m0.5199\u001b[0m, std: 0.5887\n",
      "2021-06-19 16:26:29,398 - INFO - Epoch/batch: 18/2718, ibatch: 83718, loss: \u001b[0;36m0.5410\u001b[0m, std: 0.6195\n",
      "2021-06-19 16:26:34,869 - INFO - Epoch/batch: 18/2869, ibatch: 83869, loss: \u001b[0;36m0.5346\u001b[0m, std: 0.6150\n",
      "2021-06-19 16:26:40,352 - INFO - Epoch/batch: 18/3020, ibatch: 84020, loss: \u001b[0;36m0.5181\u001b[0m, std: 0.5994\n",
      "2021-06-19 16:26:54,752 - INFO - loss: \u001b[0;32m0.5197\u001b[0m, std: 0.5891\n",
      "2021-06-19 16:26:55,626 - INFO - Epoch/batch: 18/3171, ibatch: 84171, loss: \u001b[0;36m0.5426\u001b[0m, std: 0.6200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 562: ReduceOnPlateau set learning rate to 2.6183890704263157e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:27:01,035 - INFO - Epoch/batch: 18/3322, ibatch: 84322, loss: \u001b[0;36m0.5196\u001b[0m, std: 0.5983\n",
      "2021-06-19 16:27:06,801 - INFO - Epoch/batch: 18/3473, ibatch: 84473, loss: \u001b[0;36m0.5323\u001b[0m, std: 0.6133\n",
      "2021-06-19 16:27:21,423 - INFO - loss: \u001b[0;32m0.5191\u001b[0m, std: 0.5920\n",
      "2021-06-19 16:27:22,407 - INFO - Epoch/batch: 18/3624, ibatch: 84624, loss: \u001b[0;36m0.5129\u001b[0m, std: 0.5965\n",
      "2021-06-19 16:27:28,297 - INFO - Epoch/batch: 18/3775, ibatch: 84775, loss: \u001b[0;36m0.5360\u001b[0m, std: 0.6094\n",
      "2021-06-19 16:27:33,715 - INFO - Epoch/batch: 18/3926, ibatch: 84926, loss: \u001b[0;36m0.5321\u001b[0m, std: 0.6238\n",
      "2021-06-19 16:27:47,540 - INFO - loss: \u001b[0;32m0.5204\u001b[0m, std: 0.5835\n",
      "2021-06-19 16:27:48,584 - INFO - Epoch/batch: 18/4077, ibatch: 85077, loss: \u001b[0;36m0.5433\u001b[0m, std: 0.6227\n",
      "2021-06-19 16:27:53,998 - INFO - Epoch/batch: 18/4228, ibatch: 85228, loss: \u001b[0;36m0.5186\u001b[0m, std: 0.5912\n",
      "2021-06-19 16:27:59,684 - INFO - Epoch/batch: 18/4379, ibatch: 85379, loss: \u001b[0;36m0.5371\u001b[0m, std: 0.6145\n",
      "2021-06-19 16:28:13,351 - INFO - loss: \u001b[0;32m0.5196\u001b[0m, std: 0.5874\n",
      "2021-06-19 16:28:14,551 - INFO - Epoch 18 average training loss: \u001b[0;46m0.5281\u001b[0m std: 0.6073\n",
      "2021-06-19 16:28:14,556 - INFO - Epoch 18 average validate loss: \u001b[0;46m0.5198\u001b[0m std: 0.5881\n",
      "2021-06-19 16:28:16,333 - INFO - Epoch/batch: 19/   0, ibatch: 85500, loss: \u001b[0;36m0.5272\u001b[0m, std: 0.5987\n",
      "2021-06-19 16:28:25,956 - INFO - loss: \u001b[0;32m0.5196\u001b[0m, std: 0.5875\n",
      "2021-06-19 16:28:31,906 - INFO - Epoch/batch: 19/ 151, ibatch: 85651, loss: \u001b[0;36m0.5136\u001b[0m, std: 0.5965\n",
      "2021-06-19 16:28:37,306 - INFO - Epoch/batch: 19/ 302, ibatch: 85802, loss: \u001b[0;36m0.5253\u001b[0m, std: 0.5929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 573: ReduceOnPlateau set learning rate to 2.356550163383684e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:28:52,120 - INFO - loss: \u001b[0;32m0.5191\u001b[0m, std: 0.5907\n",
      "2021-06-19 16:28:52,250 - INFO - Epoch/batch: 19/ 453, ibatch: 85953, loss: \u001b[0;36m0.5300\u001b[0m, std: 0.6129\n",
      "2021-06-19 16:28:58,084 - INFO - Epoch/batch: 19/ 604, ibatch: 86104, loss: \u001b[0;36m0.5292\u001b[0m, std: 0.6125\n",
      "2021-06-19 16:29:03,703 - INFO - Epoch/batch: 19/ 755, ibatch: 86255, loss: \u001b[0;36m0.5282\u001b[0m, std: 0.5985\n",
      "2021-06-19 16:29:18,466 - INFO - loss: \u001b[0;32m0.5196\u001b[0m, std: 0.5887\n",
      "2021-06-19 16:29:18,649 - INFO - Epoch/batch: 19/ 906, ibatch: 86406, loss: \u001b[0;36m0.5261\u001b[0m, std: 0.6088\n",
      "2021-06-19 16:29:23,985 - INFO - Epoch/batch: 19/1057, ibatch: 86557, loss: \u001b[0;36m0.5106\u001b[0m, std: 0.5919\n",
      "2021-06-19 16:29:29,620 - INFO - Epoch/batch: 19/1208, ibatch: 86708, loss: \u001b[0;36m0.5464\u001b[0m, std: 0.6312\n",
      "2021-06-19 16:29:44,263 - INFO - loss: \u001b[0;32m0.5196\u001b[0m, std: 0.5877\n",
      "2021-06-19 16:29:44,614 - INFO - Epoch/batch: 19/1359, ibatch: 86859, loss: \u001b[0;36m0.5345\u001b[0m, std: 0.6165\n",
      "2021-06-19 16:29:50,236 - INFO - Epoch/batch: 19/1510, ibatch: 87010, loss: \u001b[0;36m0.5170\u001b[0m, std: 0.5888\n",
      "2021-06-19 16:29:55,558 - INFO - Epoch/batch: 19/1661, ibatch: 87161, loss: \u001b[0;36m0.5230\u001b[0m, std: 0.6193\n",
      "2021-06-19 16:30:10,075 - INFO - loss: \u001b[0;32m0.5193\u001b[0m, std: 0.5890\n",
      "2021-06-19 16:30:10,467 - INFO - Epoch/batch: 19/1812, ibatch: 87312, loss: \u001b[0;36m0.5330\u001b[0m, std: 0.6149\n",
      "2021-06-19 16:30:16,156 - INFO - Epoch/batch: 19/1963, ibatch: 87463, loss: \u001b[0;36m0.5290\u001b[0m, std: 0.6099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 584: ReduceOnPlateau set learning rate to 2.1208951470453157e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:30:22,178 - INFO - Epoch/batch: 19/2114, ibatch: 87614, loss: \u001b[0;36m0.5484\u001b[0m, std: 0.6239\n",
      "2021-06-19 16:30:36,638 - INFO - loss: \u001b[0;32m0.5194\u001b[0m, std: 0.5884\n",
      "2021-06-19 16:30:37,205 - INFO - Epoch/batch: 19/2265, ibatch: 87765, loss: \u001b[0;36m0.5295\u001b[0m, std: 0.5997\n",
      "2021-06-19 16:30:42,841 - INFO - Epoch/batch: 19/2416, ibatch: 87916, loss: \u001b[0;36m0.5343\u001b[0m, std: 0.6155\n",
      "2021-06-19 16:30:48,501 - INFO - Epoch/batch: 19/2567, ibatch: 88067, loss: \u001b[0;36m0.5217\u001b[0m, std: 0.5919\n",
      "2021-06-19 16:31:02,820 - INFO - loss: \u001b[0;32m0.5194\u001b[0m, std: 0.5891\n",
      "2021-06-19 16:31:03,614 - INFO - Epoch/batch: 19/2718, ibatch: 88218, loss: \u001b[0;36m0.5272\u001b[0m, std: 0.6051\n",
      "2021-06-19 16:31:09,435 - INFO - Epoch/batch: 19/2869, ibatch: 88369, loss: \u001b[0;36m0.5252\u001b[0m, std: 0.6023\n",
      "2021-06-19 16:31:15,077 - INFO - Epoch/batch: 19/3020, ibatch: 88520, loss: \u001b[0;36m0.5200\u001b[0m, std: 0.6048\n",
      "2021-06-19 16:31:29,414 - INFO - loss: \u001b[0;32m0.5198\u001b[0m, std: 0.5875\n",
      "2021-06-19 16:31:30,235 - INFO - Epoch/batch: 19/3171, ibatch: 88671, loss: \u001b[0;36m0.5318\u001b[0m, std: 0.6078\n",
      "2021-06-19 16:31:36,122 - INFO - Epoch/batch: 19/3322, ibatch: 88822, loss: \u001b[0;36m0.5284\u001b[0m, std: 0.6069\n",
      "2021-06-19 16:31:41,948 - INFO - Epoch/batch: 19/3473, ibatch: 88973, loss: \u001b[0;36m0.5261\u001b[0m, std: 0.6016\n",
      "2021-06-19 16:31:56,550 - INFO - loss: \u001b[0;32m0.5195\u001b[0m, std: 0.5890\n",
      "2021-06-19 16:31:57,432 - INFO - Epoch/batch: 19/3624, ibatch: 89124, loss: \u001b[0;36m0.5277\u001b[0m, std: 0.6059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 595: ReduceOnPlateau set learning rate to 1.9088056323407842e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:32:03,112 - INFO - Epoch/batch: 19/3775, ibatch: 89275, loss: \u001b[0;36m0.5226\u001b[0m, std: 0.6081\n",
      "2021-06-19 16:32:08,731 - INFO - Epoch/batch: 19/3926, ibatch: 89426, loss: \u001b[0;36m0.5250\u001b[0m, std: 0.6113\n",
      "2021-06-19 16:32:22,759 - INFO - loss: \u001b[0;32m0.5199\u001b[0m, std: 0.5864\n",
      "2021-06-19 16:32:23,841 - INFO - Epoch/batch: 19/4077, ibatch: 89577, loss: \u001b[0;36m0.5286\u001b[0m, std: 0.6003\n",
      "2021-06-19 16:32:29,537 - INFO - Epoch/batch: 19/4228, ibatch: 89728, loss: \u001b[0;36m0.5186\u001b[0m, std: 0.5955\n",
      "2021-06-19 16:32:35,202 - INFO - Epoch/batch: 19/4379, ibatch: 89879, loss: \u001b[0;36m0.5312\u001b[0m, std: 0.6120\n",
      "2021-06-19 16:32:48,847 - INFO - loss: \u001b[0;32m0.5197\u001b[0m, std: 0.5867\n",
      "2021-06-19 16:32:50,049 - INFO - Epoch 19 average training loss: \u001b[0;46m0.5277\u001b[0m std: 0.6067\n",
      "2021-06-19 16:32:50,054 - INFO - Epoch 19 average validate loss: \u001b[0;46m0.5195\u001b[0m std: 0.5883\n",
      "2021-06-19 16:32:51,919 - INFO - Epoch/batch: 20/   0, ibatch: 90000, loss: \u001b[0;36m0.5438\u001b[0m, std: 0.6161\n",
      "2021-06-19 16:33:01,671 - INFO - loss: \u001b[0;32m0.5197\u001b[0m, std: 0.5868\n",
      "2021-06-19 16:33:07,325 - INFO - Epoch/batch: 20/ 151, ibatch: 90151, loss: \u001b[0;36m0.5370\u001b[0m, std: 0.6104\n",
      "2021-06-19 16:33:12,648 - INFO - Epoch/batch: 20/ 302, ibatch: 90302, loss: \u001b[0;36m0.5095\u001b[0m, std: 0.5832\n",
      "2021-06-19 16:33:27,535 - INFO - loss: \u001b[0;32m0.5192\u001b[0m, std: 0.5903\n",
      "2021-06-19 16:33:27,637 - INFO - Epoch/batch: 20/ 453, ibatch: 90453, loss: \u001b[0;36m0.5339\u001b[0m, std: 0.6123\n",
      "2021-06-19 16:33:33,274 - INFO - Epoch/batch: 20/ 604, ibatch: 90604, loss: \u001b[0;36m0.5349\u001b[0m, std: 0.6179\n",
      "2021-06-19 16:33:38,986 - INFO - Epoch/batch: 20/ 755, ibatch: 90755, loss: \u001b[0;36m0.5216\u001b[0m, std: 0.6005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 606: ReduceOnPlateau set learning rate to 1.717925069106706e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:33:53,737 - INFO - loss: \u001b[0;32m0.5193\u001b[0m, std: 0.5892\n",
      "2021-06-19 16:33:53,968 - INFO - Epoch/batch: 20/ 906, ibatch: 90906, loss: \u001b[0;36m0.5353\u001b[0m, std: 0.6170\n",
      "2021-06-19 16:33:59,604 - INFO - Epoch/batch: 20/1057, ibatch: 91057, loss: \u001b[0;36m0.5380\u001b[0m, std: 0.6213\n",
      "2021-06-19 16:34:05,221 - INFO - Epoch/batch: 20/1208, ibatch: 91208, loss: \u001b[0;36m0.5412\u001b[0m, std: 0.6170\n",
      "2021-06-19 16:34:20,037 - INFO - loss: \u001b[0;32m0.5192\u001b[0m, std: 0.5893\n",
      "2021-06-19 16:34:20,376 - INFO - Epoch/batch: 20/1359, ibatch: 91359, loss: \u001b[0;36m0.5280\u001b[0m, std: 0.6085\n",
      "2021-06-19 16:34:26,010 - INFO - Epoch/batch: 20/1510, ibatch: 91510, loss: \u001b[0;36m0.5110\u001b[0m, std: 0.5837\n",
      "2021-06-19 16:34:31,680 - INFO - Epoch/batch: 20/1661, ibatch: 91661, loss: \u001b[0;36m0.5374\u001b[0m, std: 0.6159\n",
      "2021-06-19 16:34:46,433 - INFO - loss: \u001b[0;32m0.5191\u001b[0m, std: 0.5906\n",
      "2021-06-19 16:34:46,936 - INFO - Epoch/batch: 20/1812, ibatch: 91812, loss: \u001b[0;36m0.5217\u001b[0m, std: 0.6028\n",
      "2021-06-19 16:34:52,623 - INFO - Epoch/batch: 20/1963, ibatch: 91963, loss: \u001b[0;36m0.5403\u001b[0m, std: 0.6208\n",
      "2021-06-19 16:34:58,131 - INFO - Epoch/batch: 20/2114, ibatch: 92114, loss: \u001b[0;36m0.5232\u001b[0m, std: 0.5982\n",
      "2021-06-19 16:35:13,136 - INFO - loss: \u001b[0;32m0.5193\u001b[0m, std: 0.5897\n",
      "2021-06-19 16:35:13,684 - INFO - Epoch/batch: 20/2265, ibatch: 92265, loss: \u001b[0;36m0.5155\u001b[0m, std: 0.5891\n",
      "2021-06-19 16:35:19,429 - INFO - Epoch/batch: 20/2416, ibatch: 92416, loss: \u001b[0;36m0.5356\u001b[0m, std: 0.6183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 617: ReduceOnPlateau set learning rate to 1.5461325621960354e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:35:25,265 - INFO - Epoch/batch: 20/2567, ibatch: 92567, loss: \u001b[0;36m0.5405\u001b[0m, std: 0.6215\n",
      "2021-06-19 16:35:40,165 - INFO - loss: \u001b[0;32m0.5197\u001b[0m, std: 0.5866\n",
      "2021-06-19 16:35:40,842 - INFO - Epoch/batch: 20/2718, ibatch: 92718, loss: \u001b[0;36m0.5424\u001b[0m, std: 0.6179\n",
      "2021-06-19 16:35:46,511 - INFO - Epoch/batch: 20/2869, ibatch: 92869, loss: \u001b[0;36m0.5270\u001b[0m, std: 0.5986\n",
      "2021-06-19 16:35:51,828 - INFO - Epoch/batch: 20/3020, ibatch: 93020, loss: \u001b[0;36m0.5161\u001b[0m, std: 0.6044\n",
      "2021-06-19 16:36:06,135 - INFO - loss: \u001b[0;32m0.5200\u001b[0m, std: 0.5852\n",
      "2021-06-19 16:36:06,963 - INFO - Epoch/batch: 20/3171, ibatch: 93171, loss: \u001b[0;36m0.5293\u001b[0m, std: 0.6181\n",
      "2021-06-19 16:36:12,626 - INFO - Epoch/batch: 20/3322, ibatch: 93322, loss: \u001b[0;36m0.5404\u001b[0m, std: 0.6143\n",
      "2021-06-19 16:36:18,298 - INFO - Epoch/batch: 20/3473, ibatch: 93473, loss: \u001b[0;36m0.5243\u001b[0m, std: 0.6001\n",
      "2021-06-19 16:36:32,988 - INFO - loss: \u001b[0;32m0.5198\u001b[0m, std: 0.5871\n",
      "2021-06-19 16:36:33,943 - INFO - Epoch/batch: 20/3624, ibatch: 93624, loss: \u001b[0;36m0.5151\u001b[0m, std: 0.5927\n",
      "2021-06-19 16:36:39,955 - INFO - Epoch/batch: 20/3775, ibatch: 93775, loss: \u001b[0;36m0.5305\u001b[0m, std: 0.6093\n",
      "2021-06-19 16:36:46,031 - INFO - Epoch/batch: 20/3926, ibatch: 93926, loss: \u001b[0;36m0.5256\u001b[0m, std: 0.6117\n",
      "2021-06-19 16:37:00,451 - INFO - loss: \u001b[0;32m0.5197\u001b[0m, std: 0.5874\n",
      "2021-06-19 16:37:01,567 - INFO - Epoch/batch: 20/4077, ibatch: 94077, loss: \u001b[0;36m0.5336\u001b[0m, std: 0.6067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 628: ReduceOnPlateau set learning rate to 1.391519305976432e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:37:07,170 - INFO - Epoch/batch: 20/4228, ibatch: 94228, loss: \u001b[0;36m0.5063\u001b[0m, std: 0.5811\n",
      "2021-06-19 16:37:12,857 - INFO - Epoch/batch: 20/4379, ibatch: 94379, loss: \u001b[0;36m0.5150\u001b[0m, std: 0.5981\n",
      "2021-06-19 16:37:27,027 - INFO - loss: \u001b[0;32m0.5192\u001b[0m, std: 0.5910\n",
      "2021-06-19 16:37:28,158 - INFO - Epoch 20 average training loss: \u001b[0;46m0.5278\u001b[0m std: 0.6063\n",
      "2021-06-19 16:37:28,251 - INFO - Epoch 20 average validate loss: \u001b[0;46m0.5195\u001b[0m std: 0.5885\n"
     ]
    }
   ],
   "source": [
    "# 训练模型，最后的loss应该在[0.52, 0.53]区间内. 每epoch里运行10次validation check, 存储最优的.\n",
    "# 每epoch需要五分钟左右(在CPU上)， 自然结束需要～20个epoch\n",
    "# Train the model - the final loss should be within 0.52 and 0.53.\n",
    "# In each epoch, validation check is run 10 times, and the best model is saved.\n",
    "# It takes about 5 minutes to complete one epoch. \n",
    "# A natural stop will go between 20 and 30 epochs\n",
    "train_loss, valid_loss = fp.train(model, train_data, num_epochs=21, validate_callback = fp.func_partial(fp.validate_in_train, midata=valid_data, save_dir='./'))\n",
    "# 注1：软标签的情况下不能得到0的交叉熵\n",
    "# Note1: zero cross-entropy is not possible with soft labels\n",
    "# 注2: 没有做特别的超参数优化, 基本上是缺省设置. 如前面提到, 更宽和更深的网络没有得到更好的结果.\n",
    "# 因为train_data and valid_data得到相近的结果, 没有用L1/L2 regularization\n",
    "# 一个显著的问题是整体收敛过快, 可能是因为网络较小. 后期工作可以调整learning_rate, dropout等等\n",
    "# Note2: No particular efforts were made towards optimizer tweaking. Default values appeard to work fine.\n",
    "# As mentioned earlier, wider and deeper nets didn't fare better, so were discarded in later trainings.\n",
    "# No significant variations were observed between train_data and valid_data, so didn't use L1/L2 regularization.\n",
    "# One conspicuous issue is that the model converges too quickly, future work may attempt to tune \n",
    "# hyperparameters such as learning rate, dropout, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = train_loss.plot.scatter('ibatch', 'loss')\n",
    "ax = train_loss.groupby('epoch').mean().plot.scatter('ibatch', 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:37:28,357 - INFO - Loading model states from: earlystop_0.5188\n",
      "2021-06-19 16:37:28,361 - INFO - Loaded net state: earlystop_0.5188/net.state\n",
      "2021-06-19 16:37:28,366 - WARNING - Error in optim state_dict loading!\n",
      "2021-06-19 16:37:28,367 - INFO - Getting loss function: ['softmax+mse']\n",
      "2021-06-19 16:37:28,368 - INFO - Validating, data size: 500\n",
      "2021-06-19 16:37:28,369 - INFO -            batch size: 64\n",
      "2021-06-19 16:37:28,369 - INFO -               shuffle: False\n",
      "2021-06-19 16:37:28,370 - INFO -          # of batches: 8\n",
      "2021-06-19 16:37:28,370 - INFO -        recap interval: 1\n",
      "2021-06-19 16:37:28,370 - INFO -          loss padding: False\n",
      "2021-06-19 16:37:31,293 - INFO - ibatch:    0, loss: 0.2217, std: 0.3167\n",
      "2021-06-19 16:37:32,010 - INFO - ibatch:    1, loss: 0.2059, std: 0.2972\n",
      "2021-06-19 16:37:32,731 - INFO - ibatch:    2, loss: 0.1888, std: 0.2776\n",
      "2021-06-19 16:37:33,448 - INFO - ibatch:    3, loss: 0.2023, std: 0.2916\n",
      "2021-06-19 16:37:34,163 - INFO - ibatch:    4, loss: 0.2014, std: 0.2902\n",
      "2021-06-19 16:37:34,881 - INFO - ibatch:    5, loss: 0.2082, std: 0.3020\n",
      "2021-06-19 16:37:35,602 - INFO - ibatch:    6, loss: 0.2092, std: 0.3002\n",
      "2021-06-19 16:37:36,185 - INFO - ibatch:    7, loss: 0.2215, std: 0.3161\n",
      "2021-06-19 16:37:37,117 - INFO - Validate mean: \u001b[0;46m0.2070\u001b[0m, std: 0.1003\n"
     ]
    }
   ],
   "source": [
    "# 读取最后一个checkpoint目录 (忽略优化器state_dict读取错误)\n",
    "# Load the last saved earlystop directory （ignore the error in optimizer state_dict loading)\n",
    "fp.state_dict_load(model, model.validate_hist.saved_dirs[-1])\n",
    "# 可以改动损失函数，检测mse损失（单个模型最好的结果：～0.20）\n",
    "# The loss_fn can be changed to softmax+mse to check the mse loss.\n",
    "# (the best/lowest loss obtained from a single model was ~0.20)\n",
    "args.loss_fn = ['softmax+mse']\n",
    "model.loss_fn = fp.get_loss_fn(args)\n",
    "valid_loss = fp.validate(model, valid_data, verbose=1, batch_size=64) # try a larger batch_size, should make no difference though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 16:37:37,149 - INFO - Loading data: work/data/predict.pkl\n",
      "2021-06-19 16:37:37,151 - INFO -    # of data: 112,  max seqlen: 861, user seq_length: -1\n",
      "2021-06-19 16:37:37,152 - INFO -  residue fmt: vector, nn: 0, dbn: True, attr: False, genre: upp\n",
      "2021-06-19 16:37:40,172 - INFO - Predicting, data size: 112\n",
      "2021-06-19 16:37:40,175 - INFO -            batch size: 1\n",
      "2021-06-19 16:37:40,176 - INFO -               shuffle: False\n",
      "2021-06-19 16:37:40,177 - INFO -          # of batches: 112\n",
      "2021-06-19 16:37:40,177 - INFO -        recap interval: 4\n",
      "2021-06-19 16:37:40,178 - INFO - Predicted files will be saved in: predict.files\n",
      "100%|██████████| 112/112 [00:06<00:00, 16.02it/s]\n",
      "2021-06-19 16:37:47,248 - INFO - Completed prediction of 112 samples\n"
     ]
    }
   ],
   "source": [
    "# 读取预测数据， 存储预测结果\n",
    "# 提交的结果是平均了三次运行最好checkpoint模型， 它们分别得到了0.24, 0.24, 0.242的sqrt(mse)损失， 平均后得到了0.238\n",
    "# 可惜前两次的checkpoint没有被保存， 只保存了预测的结果.\n",
    "# 虽然是同一个网络架构， 因为每次训练的train_test_split是随机的， 模型平均的效果和cross_validate相近\n",
    "# Load the prediction data, and save the predicted results\n",
    "# The submitted results are the average of the best checkpoints/earlystops from three independent trainings.\n",
    "# Unfortunately the checkpoints for the first two were not kept, except for the predicted results。\n",
    "# As each training randomly splits the train and validation data, this model averaging approximates\n",
    "# the effect/benefit of cross-validation.\n",
    "predict_data = fp.get_midata(args, data_name='predict', seq_length=-1)\n",
    "y_model, std_model = fp.predict(model, predict_data, save_dir='predict.files', batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: predict.files/ (stored 0%)\n",
      "updating: predict.files/87.predict.txt (deflated 58%)\n",
      "updating: predict.files/79.predict.txt (deflated 56%)\n",
      "updating: predict.files/55.predict.txt (deflated 55%)\n",
      "updating: predict.files/33.predict.txt (deflated 55%)\n",
      "updating: predict.files/19.predict.txt (deflated 59%)\n",
      "updating: predict.files/72.predict.txt (deflated 57%)\n",
      "updating: predict.files/78.predict.txt (deflated 56%)\n",
      "updating: predict.files/88.predict.txt (deflated 57%)\n",
      "updating: predict.files/98.predict.txt (deflated 56%)\n",
      "updating: predict.files/59.predict.txt (deflated 57%)\n",
      "updating: predict.files/35.predict.txt (deflated 57%)\n",
      "updating: predict.files/54.predict.txt (deflated 56%)\n",
      "updating: predict.files/70.predict.txt (deflated 56%)\n",
      "updating: predict.files/12.predict.txt (deflated 57%)\n",
      "updating: predict.files/112.predict.txt (deflated 56%)\n",
      "updating: predict.files/22.predict.txt (deflated 58%)\n",
      "updating: predict.files/48.predict.txt (deflated 56%)\n",
      "updating: predict.files/17.predict.txt (deflated 57%)\n",
      "updating: predict.files/23.predict.txt (deflated 56%)\n",
      "updating: predict.files/99.predict.txt (deflated 56%)\n",
      "updating: predict.files/102.predict.txt (deflated 56%)\n",
      "updating: predict.files/14.predict.txt (deflated 55%)\n",
      "updating: predict.files/13.predict.txt (deflated 57%)\n",
      "updating: predict.files/111.predict.txt (deflated 56%)\n",
      "updating: predict.files/93.predict.txt (deflated 56%)\n",
      "updating: predict.files/9.predict.txt (deflated 56%)\n",
      "updating: predict.files/103.predict.txt (deflated 56%)\n",
      "updating: predict.files/108.predict.txt (deflated 56%)\n",
      "updating: predict.files/51.predict.txt (deflated 56%)\n",
      "updating: predict.files/97.predict.txt (deflated 56%)\n",
      "updating: predict.files/76.predict.txt (deflated 54%)\n",
      "updating: predict.files/30.predict.txt (deflated 55%)\n",
      "updating: predict.files/24.predict.txt (deflated 60%)\n",
      "updating: predict.files/64.predict.txt (deflated 56%)\n",
      "updating: predict.files/62.predict.txt (deflated 56%)\n",
      "updating: predict.files/90.predict.txt (deflated 56%)\n",
      "updating: predict.files/37.predict.txt (deflated 58%)\n",
      "updating: predict.files/36.predict.txt (deflated 59%)\n",
      "updating: predict.files/52.predict.txt (deflated 56%)\n",
      "updating: predict.files/56.predict.txt (deflated 59%)\n",
      "updating: predict.files/3.predict.txt (deflated 57%)\n",
      "updating: predict.files/63.predict.txt (deflated 57%)\n",
      "updating: predict.files/104.predict.txt (deflated 57%)\n",
      "updating: predict.files/85.predict.txt (deflated 55%)\n",
      "updating: predict.files/5.predict.txt (deflated 56%)\n",
      "updating: predict.files/58.predict.txt (deflated 57%)\n",
      "updating: predict.files/11.predict.txt (deflated 57%)\n",
      "updating: predict.files/91.predict.txt (deflated 56%)\n",
      "updating: predict.files/89.predict.txt (deflated 57%)\n",
      "updating: predict.files/18.predict.txt (deflated 55%)\n",
      "updating: predict.files/100.predict.txt (deflated 55%)\n",
      "updating: predict.files/7.predict.txt (deflated 57%)\n",
      "updating: predict.files/26.predict.txt (deflated 60%)\n",
      "updating: predict.files/61.predict.txt (deflated 56%)\n",
      "updating: predict.files/105.predict.txt (deflated 56%)\n",
      "updating: predict.files/32.predict.txt (deflated 55%)\n",
      "updating: predict.files/96.predict.txt (deflated 55%)\n",
      "updating: predict.files/46.predict.txt (deflated 56%)\n",
      "updating: predict.files/53.predict.txt (deflated 56%)\n",
      "updating: predict.files/49.predict.txt (deflated 58%)\n",
      "updating: predict.files/74.predict.txt (deflated 58%)\n",
      "updating: predict.files/67.predict.txt (deflated 57%)\n",
      "updating: predict.files/71.predict.txt (deflated 55%)\n",
      "updating: predict.files/45.predict.txt (deflated 57%)\n",
      "updating: predict.files/86.predict.txt (deflated 57%)\n",
      "updating: predict.files/65.predict.txt (deflated 57%)\n",
      "updating: predict.files/80.predict.txt (deflated 56%)\n",
      "updating: predict.files/42.predict.txt (deflated 63%)\n",
      "updating: predict.files/8.predict.txt (deflated 56%)\n",
      "updating: predict.files/34.predict.txt (deflated 55%)\n",
      "updating: predict.files/16.predict.txt (deflated 58%)\n",
      "updating: predict.files/50.predict.txt (deflated 57%)\n",
      "updating: predict.files/95.predict.txt (deflated 55%)\n",
      "updating: predict.files/6.predict.txt (deflated 56%)\n",
      "updating: predict.files/92.predict.txt (deflated 55%)\n",
      "updating: predict.files/77.predict.txt (deflated 56%)\n",
      "updating: predict.files/1.predict.txt (deflated 57%)\n",
      "updating: predict.files/57.predict.txt (deflated 56%)\n",
      "updating: predict.files/43.predict.txt (deflated 58%)\n",
      "updating: predict.files/28.predict.txt (deflated 60%)\n",
      "updating: predict.files/29.predict.txt (deflated 61%)\n",
      "updating: predict.files/68.predict.txt (deflated 57%)\n",
      "updating: predict.files/25.predict.txt (deflated 57%)\n",
      "updating: predict.files/10.predict.txt (deflated 57%)\n",
      "updating: predict.files/41.predict.txt (deflated 56%)\n",
      "updating: predict.files/15.predict.txt (deflated 59%)\n",
      "updating: predict.files/84.predict.txt (deflated 54%)\n",
      "updating: predict.files/94.predict.txt (deflated 56%)\n",
      "updating: predict.files/81.predict.txt (deflated 56%)\n",
      "updating: predict.files/21.predict.txt (deflated 58%)\n",
      "updating: predict.files/20.predict.txt (deflated 60%)\n",
      "updating: predict.files/27.predict.txt (deflated 60%)\n",
      "updating: predict.files/2.predict.txt (deflated 57%)\n",
      "updating: predict.files/38.predict.txt (deflated 59%)\n",
      "updating: predict.files/60.predict.txt (deflated 57%)\n",
      "updating: predict.files/39.predict.txt (deflated 60%)\n",
      "updating: predict.files/73.predict.txt (deflated 55%)\n",
      "updating: predict.files/83.predict.txt (deflated 55%)\n",
      "updating: predict.files/107.predict.txt (deflated 57%)\n",
      "updating: predict.files/101.predict.txt (deflated 56%)\n",
      "updating: predict.files/47.predict.txt (deflated 57%)\n",
      "updating: predict.files/69.predict.txt (deflated 56%)\n",
      "updating: predict.files/75.predict.txt (deflated 56%)\n",
      "updating: predict.files/44.predict.txt (deflated 58%)\n",
      "updating: predict.files/40.predict.txt (deflated 63%)\n",
      "updating: predict.files/109.predict.txt (deflated 55%)\n",
      "updating: predict.files/4.predict.txt (deflated 56%)\n",
      "updating: predict.files/66.predict.txt (deflated 56%)\n",
      "updating: predict.files/110.predict.txt (deflated 56%)\n",
      "updating: predict.files/106.predict.txt (deflated 56%)\n",
      "updating: predict.files/82.predict.txt (deflated 55%)\n",
      "updating: predict.files/31.predict.txt (deflated 56%)\n",
      "test of predict.files.zip OK\n"
     ]
    }
   ],
   "source": [
    "!zip -r9oT predict.files.zip predict.files/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
