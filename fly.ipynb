{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "# !mkdir /home/aistudio/external-libraries\n",
    "# !pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 程序在/work/code目录下， 需先加入路径\n",
    "import sys \n",
    "sys.path.append('/home/aistudio/work/code')\n",
    "# fly_paddle是唯一需要直接调用的模块\n",
    "# fly_paddle is the only module users need to interact with\n",
    "import fly_paddle as fp\n",
    "\n",
    "# args包括所有需要的参数， 贯穿于几乎所有的程序调用中\n",
    "# fp.parse_args2() 根据任务初始化args, 要用到的任务包括： ‘train', 'predict'\n",
    "# args is a structure containing most (if not all) parameters\n",
    "# fp.parse_args2() initializes args based on the task to run, such as \"train\", \"predict\"\n",
    "args, _ = fp.parse_args2('train')\n",
    "print(fp.gwio.json_str(args.__dict__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 注： 根据不同的网络等等需要， args可能包含一些用不到的参数\n",
    "# Attention: some parameters in args may not be used depending on the network etc.\n",
    "# 两种更新args的方法： 1） args.update(**dict), 2) args.[key] = value\n",
    "# Two main ways to update values in args: 1) args.update(**dict), 2) args.[key] = value\n",
    "args.update(data_dir='work/data', data_name='train', residue_dbn=True, residue_extra=True)\n",
    "\n",
    "# 网络参数 （net parameters): \n",
    "# 网络的设计主要考虑了三个RNA碱基配对的支配作用： \n",
    "#    1) 来自于全部序列的排列组合（配分）竞争，用Attention机制来模拟\n",
    "#    2）来自于线性大分子的一维序列限制， 用LSTM结构来模拟\n",
    "#    3）来自于局部紧邻碱基的合作（比如，一个孤立的碱基对极不稳定）， 用1D Convolution来模拟\n",
    "# 所以框架由以上三个模块组成， 并在输入和输出层加了1-3个线性层。除非特意说明， 所有的隐藏层的维度为32. \n",
    "# 训练中发现高维度和深度的网络并不能给出更好的结果！\n",
    "# Three main governing mechanisms for RNA base pairing are taken into consideration for the \n",
    "# design of the network architecture. \n",
    "#   1) The combinatorial configurational space of base pairing among all RNA bases, accounted for with Attention Mechanism\n",
    "#   2) The quasi-1D nature of unbranched RNA polymers, accounted for with LSTM\n",
    "#   3) The cooperativity of neighboring bases for stable base pairing, accounted for with 1D Convolution\n",
    "# Hence the neural net comprises of three main building blocks, with addiitional linear layers for input and output. \n",
    "# The dimensions of all hidden layers are 32 unless noted otherwise\n",
    "# Larger and deeper nets gave similar, but no better, performances!\n",
    "args.net='seq2seq_attnlstmconv1d'  # the net name defined in paddle_nets.py\n",
    "args.linear_num = 1 # the number of linear feedforward layers\n",
    "args.attn_num = 1 # the number of transformer encoder layers\n",
    "args.lstm_num = 1 # the number of bidirectional lstm layers\n",
    "args.conv1d_num = 1 # the number of 1D convolutional layers\n",
    "# 输出模块由三个线性层组成， 维度分别为32, 32, 2\n",
    "# three linear layers for the final output, with dimensions of 32, 32, and 2, respectively\n",
    "args.output_dim = [32, 32, 2] \n",
    "args.norm_fn = 'layer' # layer normalization\n",
    "args.batch_size = 1 # 1 is used in consideration of the layer norm above\n",
    "# 最后递交用的损失函数选为 softmax+bce, 也可以用 softmax+mse, 结果几乎一样\n",
    "args.loss_fn = ['softmax+bce'] # softmax is needed here as the final output has a dimension of 2\n",
    "args.label_tone = 'soft'\n",
    "args.loss_sqrt = True # sqrt(loss) is only necessary for softmax+mse\n",
    "args.loss_padding = False # exclude padded residues from loss\n",
    "# 需要运行autoconfig_args()来消除参数的不一致性\n",
    "# autoconfig_args() is needed to resolve inconsistencies between parameters\n",
    "args = fp.autoconfig_args(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:35:15,630 - INFO - Used net definition: \u001b[0;39;46m/home/aistudio/work/code/paddle_nets.py\u001b[0m\n",
      "2021-05-12 23:35:15,704 - INFO - {'total_params': 36418, 'trainable_params': 36418}\n",
      "2021-05-12 23:35:15,705 - INFO - Optimizer method: adam\n",
      "2021-05-12 23:35:15,706 - INFO -    learning rate: 0.003\n",
      "2021-05-12 23:35:15,706 - INFO -     lr_scheduler: reduced\n",
      "2021-05-12 23:35:15,706 - INFO -     weight decay: none\n",
      "2021-05-12 23:35:15,707 - INFO -          l1decay: 0.0001\n",
      "2021-05-12 23:35:15,707 - INFO -          l2decay: 0.0001\n",
      "2021-05-12 23:35:15,708 - INFO - Getting loss function: ['softmax+bce']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "      Layer (type)                          Input Shape                                  Output Shape                   Param #    \n",
      "=====================================================================================================================================\n",
      "   MyEmbeddingLayer-1                      [[2, 512, 10]]                                [2, 512, 10]                      0       \n",
      "        Linear-1                           [[2, 512, 10]]                                [2, 512, 32]                     352      \n",
      "         ReLU-1                            [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-1                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Dropout-1                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "     MyLinearTower-1                       [[2, 512, 10]]                                [2, 512, 32]                      0       \n",
      "    PositionEncoder-1                      [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-2                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Linear-2                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Linear-3                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Linear-4                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Linear-5                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "  MultiHeadAttention-1    [[2, 512, 32], [2, 512, 32], [2, 512, 32], None]               [2, 512, 32]                      0       \n",
      "        Dropout-3                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-3                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Linear-6                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Dropout-2                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "        Linear-7                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "        Dropout-4                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "TransformerEncoderLayer-1                  [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "  TransformerEncoder-1                  [[2, 512, 32], None]                             [2, 512, 32]                      0       \n",
      "      MyAttnTower-1                        [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "         LSTM-1                            [[2, 512, 32]]                  [[2, 512, 64], [[2, 2, 32], [2, 2, 32]]]     16,896     \n",
      "      MyLSTMTower-1                        [[2, 512, 32]]                                [2, 512, 64]                      0       \n",
      "        Conv1D-1                           [[2, 512, 64]]                                [2, 512, 32]                   10,272     \n",
      "         ReLU-2                            [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-4                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Dropout-5                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "     MyConv1DTower-1                       [[2, 512, 64]]                                [2, 512, 32]                      0       \n",
      "        Linear-8                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "         ReLU-3                            [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-5                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Dropout-6                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "        Linear-9                           [[2, 512, 32]]                                [2, 512, 32]                    1,056     \n",
      "         ReLU-4                            [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "       LayerNorm-6                         [[2, 512, 32]]                                [2, 512, 32]                     64       \n",
      "        Dropout-7                          [[2, 512, 32]]                                [2, 512, 32]                      0       \n",
      "        Linear-10                          [[2, 512, 32]]                                [2, 512, 2]                      66       \n",
      "     MyLinearTower-2                       [[2, 512, 32]]                                [2, 512, 2]                       0       \n",
      "=====================================================================================================================================\n",
      "Total params: 36,418\n",
      "Trainable params: 36,418\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 9.73\n",
      "Params size (MB): 0.14\n",
      "Estimated Total Size (MB): 9.91\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 建立和检测模型 （Get and inspect the model）\n",
    "model = fp.get_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:35:15,714 - INFO - Loading data: work/data/train.pkl\n",
      "2021-05-12 23:35:15,760 - INFO -    # of data: 5000,  max seqlen: 500, user seq_length: [0, 512, -1]\n",
      "2021-05-12 23:35:15,761 - INFO -  residue fmt: vector, nn: 0, dbn: True, attr: False, genre: upp\n",
      "2021-05-12 23:35:15,779 - INFO - Selected 5000 data sets with length range: [0, 512, -1]\n",
      "2021-05-12 23:35:20,618 - INFO - Processing upp data...\n"
     ]
    }
   ],
   "source": [
    "# 读取数据 （read in data)\n",
    "midata = fp.get_midata(args)\n",
    "train_data, valid_data = fp.train_test_split(midata, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:35:21,410 - INFO - Training, data size: 4500\n",
      "2021-05-12 23:35:21,508 - INFO -          batch size: 1\n",
      "2021-05-12 23:35:21,509 - INFO -             shuffle: True\n",
      "2021-05-12 23:35:21,510 - INFO -        # of batches: 4500\n",
      "2021-05-12 23:35:21,511 - INFO -      recap interval: 151\n",
      "2021-05-12 23:35:21,511 - INFO -   validate interval: 450\n",
      "2021-05-12 23:35:21,512 - INFO -         # of epochs: 21\n",
      "2021-05-12 23:35:21,513 - INFO -        loss padding: False\n",
      "2021-05-12 23:35:22,468 - INFO - Epoch/batch: 0/   0, ibatch:    0, loss: \u001b[0;36m0.8524\u001b[0m, std: 0.5392\n",
      "2021-05-12 23:35:30,114 - INFO - loss: \u001b[0;32m0.8924\u001b[0m, std: 0.5263\n",
      "2021-05-12 23:35:36,074 - INFO - Epoch/batch: 0/ 151, ibatch:  151, loss: \u001b[0;36m0.6988\u001b[0m, std: 0.6297\n",
      "2021-05-12 23:35:41,863 - INFO - Epoch/batch: 0/ 302, ibatch:  302, loss: \u001b[0;36m0.6065\u001b[0m, std: 0.6719\n",
      "2021-05-12 23:35:57,380 - INFO - loss: \u001b[0;32m0.5724\u001b[0m, std: 0.6974\n",
      "2021-05-12 23:35:57,396 - INFO - Saved model states in: earlystop_0.5724\n",
      "2021-05-12 23:35:57,397 - INFO - Saved net python code: earlystop_0.5724/paddle_nets.py\n",
      "2021-05-12 23:35:57,407 - INFO - Saved best model: earlystop_0.5724\n",
      "2021-05-12 23:35:57,518 - INFO - Epoch/batch: 0/ 453, ibatch:  453, loss: \u001b[0;36m0.5954\u001b[0m, std: 0.6647\n",
      "2021-05-12 23:36:03,262 - INFO - Epoch/batch: 0/ 604, ibatch:  604, loss: \u001b[0;36m0.5718\u001b[0m, std: 0.6531\n",
      "2021-05-12 23:36:08,893 - INFO - Epoch/batch: 0/ 755, ibatch:  755, loss: \u001b[0;36m0.5827\u001b[0m, std: 0.6680\n",
      "2021-05-12 23:36:24,335 - INFO - loss: \u001b[0;32m0.5670\u001b[0m, std: 0.7107\n",
      "2021-05-12 23:36:24,350 - INFO - Saved model states in: earlystop_0.5670\n",
      "2021-05-12 23:36:24,351 - INFO - Saved net python code: earlystop_0.5670/paddle_nets.py\n",
      "2021-05-12 23:36:24,358 - INFO - Saved best model: earlystop_0.5670\n",
      "2021-05-12 23:36:24,358 - INFO - Removing earlystop model: earlystop_0.5724\n",
      "2021-05-12 23:36:24,609 - INFO - Epoch/batch: 0/ 906, ibatch:  906, loss: \u001b[0;36m0.5474\u001b[0m, std: 0.6387\n",
      "2021-05-12 23:36:30,898 - INFO - Epoch/batch: 0/1057, ibatch: 1057, loss: \u001b[0;36m0.5475\u001b[0m, std: 0.6331\n",
      "2021-05-12 23:36:36,618 - INFO - Epoch/batch: 0/1208, ibatch: 1208, loss: \u001b[0;36m0.5666\u001b[0m, std: 0.6455\n",
      "2021-05-12 23:36:51,707 - INFO - loss: \u001b[0;32m0.5509\u001b[0m, std: 0.6609\n",
      "2021-05-12 23:36:51,724 - INFO - Saved model states in: earlystop_0.5509\n",
      "2021-05-12 23:36:51,726 - INFO - Saved net python code: earlystop_0.5509/paddle_nets.py\n",
      "2021-05-12 23:36:51,733 - INFO - Saved best model: earlystop_0.5509\n",
      "2021-05-12 23:36:51,734 - INFO - Removing earlystop model: earlystop_0.5670\n",
      "2021-05-12 23:36:52,000 - INFO - Epoch/batch: 0/1359, ibatch: 1359, loss: \u001b[0;36m0.5668\u001b[0m, std: 0.6423\n",
      "2021-05-12 23:36:57,291 - INFO - Epoch/batch: 0/1510, ibatch: 1510, loss: \u001b[0;36m0.5609\u001b[0m, std: 0.6387\n",
      "2021-05-12 23:37:02,968 - INFO - Epoch/batch: 0/1661, ibatch: 1661, loss: \u001b[0;36m0.5538\u001b[0m, std: 0.6327\n",
      "2021-05-12 23:37:18,098 - INFO - loss: \u001b[0;32m0.5442\u001b[0m, std: 0.6219\n",
      "2021-05-12 23:37:18,115 - INFO - Saved model states in: earlystop_0.5442\n",
      "2021-05-12 23:37:18,117 - INFO - Saved net python code: earlystop_0.5442/paddle_nets.py\n",
      "2021-05-12 23:37:18,127 - INFO - Saved best model: earlystop_0.5442\n",
      "2021-05-12 23:37:18,128 - INFO - Removing earlystop model: earlystop_0.5509\n",
      "2021-05-12 23:37:18,600 - INFO - Epoch/batch: 0/1812, ibatch: 1812, loss: \u001b[0;36m0.5678\u001b[0m, std: 0.6375\n",
      "2021-05-12 23:37:24,366 - INFO - Epoch/batch: 0/1963, ibatch: 1963, loss: \u001b[0;36m0.5530\u001b[0m, std: 0.6327\n",
      "2021-05-12 23:37:30,014 - INFO - Epoch/batch: 0/2114, ibatch: 2114, loss: \u001b[0;36m0.5748\u001b[0m, std: 0.6540\n",
      "2021-05-12 23:37:45,294 - INFO - loss: \u001b[0;32m0.5427\u001b[0m, std: 0.6480\n",
      "2021-05-12 23:37:45,309 - INFO - Saved model states in: earlystop_0.5427\n",
      "2021-05-12 23:37:45,311 - INFO - Saved net python code: earlystop_0.5427/paddle_nets.py\n",
      "2021-05-12 23:37:45,318 - INFO - Saved best model: earlystop_0.5427\n",
      "2021-05-12 23:37:45,320 - INFO - Removing earlystop model: earlystop_0.5442\n",
      "2021-05-12 23:37:45,963 - INFO - Epoch/batch: 0/2265, ibatch: 2265, loss: \u001b[0;36m0.5509\u001b[0m, std: 0.6343\n",
      "2021-05-12 23:37:51,633 - INFO - Epoch/batch: 0/2416, ibatch: 2416, loss: \u001b[0;36m0.5397\u001b[0m, std: 0.6255\n",
      "2021-05-12 23:37:57,161 - INFO - Epoch/batch: 0/2567, ibatch: 2567, loss: \u001b[0;36m0.5531\u001b[0m, std: 0.6214\n",
      "2021-05-12 23:38:11,943 - INFO - loss: \u001b[0;32m0.5423\u001b[0m, std: 0.6389\n",
      "2021-05-12 23:38:11,958 - INFO - Saved model states in: earlystop_0.5423\n",
      "2021-05-12 23:38:11,959 - INFO - Saved net python code: earlystop_0.5423/paddle_nets.py\n",
      "2021-05-12 23:38:11,965 - INFO - Saved best model: earlystop_0.5423\n",
      "2021-05-12 23:38:11,966 - INFO - Removing earlystop model: earlystop_0.5427\n",
      "2021-05-12 23:38:12,648 - INFO - Epoch/batch: 0/2718, ibatch: 2718, loss: \u001b[0;36m0.5571\u001b[0m, std: 0.6408\n",
      "2021-05-12 23:38:18,371 - INFO - Epoch/batch: 0/2869, ibatch: 2869, loss: \u001b[0;36m0.5479\u001b[0m, std: 0.6258\n",
      "2021-05-12 23:38:23,996 - INFO - Epoch/batch: 0/3020, ibatch: 3020, loss: \u001b[0;36m0.5330\u001b[0m, std: 0.6240\n",
      "2021-05-12 23:38:38,697 - INFO - loss: \u001b[0;32m0.5529\u001b[0m, std: 0.5635\n",
      "2021-05-12 23:38:39,589 - INFO - Epoch/batch: 0/3171, ibatch: 3171, loss: \u001b[0;36m0.5602\u001b[0m, std: 0.6362\n",
      "2021-05-12 23:38:45,403 - INFO - Epoch/batch: 0/3322, ibatch: 3322, loss: \u001b[0;36m0.5391\u001b[0m, std: 0.6231\n",
      "2021-05-12 23:38:51,252 - INFO - Epoch/batch: 0/3473, ibatch: 3473, loss: \u001b[0;36m0.5554\u001b[0m, std: 0.6297\n",
      "2021-05-12 23:39:06,017 - INFO - loss: \u001b[0;32m0.5453\u001b[0m, std: 0.5803\n",
      "2021-05-12 23:39:06,909 - INFO - Epoch/batch: 0/3624, ibatch: 3624, loss: \u001b[0;36m0.5669\u001b[0m, std: 0.6353\n",
      "2021-05-12 23:39:12,582 - INFO - Epoch/batch: 0/3775, ibatch: 3775, loss: \u001b[0;36m0.5632\u001b[0m, std: 0.6389\n",
      "2021-05-12 23:39:18,191 - INFO - Epoch/batch: 0/3926, ibatch: 3926, loss: \u001b[0;36m0.5435\u001b[0m, std: 0.6220\n",
      "2021-05-12 23:39:32,701 - INFO - loss: \u001b[0;32m0.5406\u001b[0m, std: 0.6270\n",
      "2021-05-12 23:39:32,717 - INFO - Saved model states in: earlystop_0.5406\n",
      "2021-05-12 23:39:32,718 - INFO - Saved net python code: earlystop_0.5406/paddle_nets.py\n",
      "2021-05-12 23:39:32,725 - INFO - Saved best model: earlystop_0.5406\n",
      "2021-05-12 23:39:32,726 - INFO - Removing earlystop model: earlystop_0.5423\n",
      "2021-05-12 23:39:33,735 - INFO - Epoch/batch: 0/4077, ibatch: 4077, loss: \u001b[0;36m0.5639\u001b[0m, std: 0.6358\n",
      "2021-05-12 23:39:39,305 - INFO - Epoch/batch: 0/4228, ibatch: 4228, loss: \u001b[0;36m0.5502\u001b[0m, std: 0.6213\n",
      "2021-05-12 23:39:45,142 - INFO - Epoch/batch: 0/4379, ibatch: 4379, loss: \u001b[0;36m0.5714\u001b[0m, std: 0.6373\n",
      "2021-05-12 23:39:59,614 - INFO - loss: \u001b[0;32m0.5379\u001b[0m, std: 0.6245\n",
      "2021-05-12 23:39:59,630 - INFO - Saved model states in: earlystop_0.5379\n",
      "2021-05-12 23:39:59,631 - INFO - Saved net python code: earlystop_0.5379/paddle_nets.py\n",
      "2021-05-12 23:39:59,637 - INFO - Saved best model: earlystop_0.5379\n",
      "2021-05-12 23:39:59,638 - INFO - Removing earlystop model: earlystop_0.5406\n",
      "2021-05-12 23:40:00,112 - INFO - Epoch 0 average training loss: \u001b[0;46m0.5653\u001b[0m std: 0.6377\n",
      "2021-05-12 23:40:00,116 - INFO - Epoch 0 average validate loss: \u001b[0;46m0.5808\u001b[0m std: 0.6272\n",
      "2021-05-12 23:40:02,250 - INFO - Epoch/batch: 1/   0, ibatch: 4500, loss: \u001b[0;36m0.5675\u001b[0m, std: 0.6375\n",
      "2021-05-12 23:40:12,530 - INFO - loss: \u001b[0;32m0.5384\u001b[0m, std: 0.6295\n",
      "2021-05-12 23:40:18,187 - INFO - Epoch/batch: 1/ 151, ibatch: 4651, loss: \u001b[0;36m0.5414\u001b[0m, std: 0.6249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: ReduceOnPlateau set learning rate to 0.0027.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:40:23,826 - INFO - Epoch/batch: 1/ 302, ibatch: 4802, loss: \u001b[0;36m0.5439\u001b[0m, std: 0.6257\n",
      "2021-05-12 23:40:39,182 - INFO - loss: \u001b[0;32m0.5391\u001b[0m, std: 0.6482\n",
      "2021-05-12 23:40:39,290 - INFO - Epoch/batch: 1/ 453, ibatch: 4953, loss: \u001b[0;36m0.5396\u001b[0m, std: 0.6252\n",
      "2021-05-12 23:40:44,939 - INFO - Epoch/batch: 1/ 604, ibatch: 5104, loss: \u001b[0;36m0.5315\u001b[0m, std: 0.6126\n",
      "2021-05-12 23:40:50,707 - INFO - Epoch/batch: 1/ 755, ibatch: 5255, loss: \u001b[0;36m0.5455\u001b[0m, std: 0.6300\n",
      "2021-05-12 23:41:06,312 - INFO - loss: \u001b[0;32m0.5395\u001b[0m, std: 0.6522\n",
      "2021-05-12 23:41:06,537 - INFO - Epoch/batch: 1/ 906, ibatch: 5406, loss: \u001b[0;36m0.5352\u001b[0m, std: 0.6250\n",
      "2021-05-12 23:41:12,258 - INFO - Epoch/batch: 1/1057, ibatch: 5557, loss: \u001b[0;36m0.5537\u001b[0m, std: 0.6323\n",
      "2021-05-12 23:41:18,032 - INFO - Epoch/batch: 1/1208, ibatch: 5708, loss: \u001b[0;36m0.5416\u001b[0m, std: 0.6192\n",
      "2021-05-12 23:41:34,114 - INFO - loss: \u001b[0;32m0.5382\u001b[0m, std: 0.5862\n",
      "2021-05-12 23:41:34,414 - INFO - Epoch/batch: 1/1359, ibatch: 5859, loss: \u001b[0;36m0.5469\u001b[0m, std: 0.6269\n",
      "2021-05-12 23:41:40,255 - INFO - Epoch/batch: 1/1510, ibatch: 6010, loss: \u001b[0;36m0.5546\u001b[0m, std: 0.6302\n",
      "2021-05-12 23:41:45,776 - INFO - Epoch/batch: 1/1661, ibatch: 6161, loss: \u001b[0;36m0.5208\u001b[0m, std: 0.6197\n",
      "2021-05-12 23:42:00,491 - INFO - loss: \u001b[0;32m0.5408\u001b[0m, std: 0.6570\n",
      "2021-05-12 23:42:00,998 - INFO - Epoch/batch: 1/1812, ibatch: 6312, loss: \u001b[0;36m0.5372\u001b[0m, std: 0.6190\n",
      "2021-05-12 23:42:06,417 - INFO - Epoch/batch: 1/1963, ibatch: 6463, loss: \u001b[0;36m0.5565\u001b[0m, std: 0.6294\n",
      "2021-05-12 23:42:12,258 - INFO - Epoch/batch: 1/2114, ibatch: 6614, loss: \u001b[0;36m0.5641\u001b[0m, std: 0.6355\n",
      "2021-05-12 23:42:27,390 - INFO - loss: \u001b[0;32m0.5348\u001b[0m, std: 0.5923\n",
      "2021-05-12 23:42:27,406 - INFO - Saved model states in: earlystop_0.5348\n",
      "2021-05-12 23:42:27,408 - INFO - Saved net python code: earlystop_0.5348/paddle_nets.py\n",
      "2021-05-12 23:42:27,420 - INFO - Saved best model: earlystop_0.5348\n",
      "2021-05-12 23:42:27,420 - INFO - Removing earlystop model: earlystop_0.5379\n",
      "2021-05-12 23:42:27,999 - INFO - Epoch/batch: 1/2265, ibatch: 6765, loss: \u001b[0;36m0.5550\u001b[0m, std: 0.6290\n",
      "2021-05-12 23:42:33,768 - INFO - Epoch/batch: 1/2416, ibatch: 6916, loss: \u001b[0;36m0.5409\u001b[0m, std: 0.6243\n",
      "2021-05-12 23:42:39,639 - INFO - Epoch/batch: 1/2567, ibatch: 7067, loss: \u001b[0;36m0.5451\u001b[0m, std: 0.6083\n",
      "2021-05-12 23:42:54,087 - INFO - loss: \u001b[0;32m0.5339\u001b[0m, std: 0.6237\n",
      "2021-05-12 23:42:54,118 - INFO - Saved model states in: earlystop_0.5339\n",
      "2021-05-12 23:42:54,120 - INFO - Saved net python code: earlystop_0.5339/paddle_nets.py\n",
      "2021-05-12 23:42:54,129 - INFO - Saved best model: earlystop_0.5339\n",
      "2021-05-12 23:42:54,130 - INFO - Removing earlystop model: earlystop_0.5348\n",
      "2021-05-12 23:42:54,888 - INFO - Epoch/batch: 1/2718, ibatch: 7218, loss: \u001b[0;36m0.5361\u001b[0m, std: 0.6250\n",
      "2021-05-12 23:43:00,492 - INFO - Epoch/batch: 1/2869, ibatch: 7369, loss: \u001b[0;36m0.5530\u001b[0m, std: 0.6210\n",
      "2021-05-12 23:43:06,137 - INFO - Epoch/batch: 1/3020, ibatch: 7520, loss: \u001b[0;36m0.5630\u001b[0m, std: 0.6380\n",
      "2021-05-12 23:43:21,169 - INFO - loss: \u001b[0;32m0.5456\u001b[0m, std: 0.5678\n",
      "2021-05-12 23:43:21,948 - INFO - Epoch/batch: 1/3171, ibatch: 7671, loss: \u001b[0;36m0.5800\u001b[0m, std: 0.6366\n",
      "2021-05-12 23:43:27,496 - INFO - Epoch/batch: 1/3322, ibatch: 7822, loss: \u001b[0;36m0.5514\u001b[0m, std: 0.6204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: ReduceOnPlateau set learning rate to 0.0024300000000000003.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:43:33,070 - INFO - Epoch/batch: 1/3473, ibatch: 7973, loss: \u001b[0;36m0.5384\u001b[0m, std: 0.6229\n",
      "2021-05-12 23:43:47,382 - INFO - loss: \u001b[0;32m0.5342\u001b[0m, std: 0.6276\n",
      "2021-05-12 23:43:48,310 - INFO - Epoch/batch: 1/3624, ibatch: 8124, loss: \u001b[0;36m0.5465\u001b[0m, std: 0.6263\n",
      "2021-05-12 23:43:54,213 - INFO - Epoch/batch: 1/3775, ibatch: 8275, loss: \u001b[0;36m0.5469\u001b[0m, std: 0.6219\n",
      "2021-05-12 23:44:00,371 - INFO - Epoch/batch: 1/3926, ibatch: 8426, loss: \u001b[0;36m0.5618\u001b[0m, std: 0.6340\n",
      "2021-05-12 23:44:14,705 - INFO - loss: \u001b[0;32m0.5343\u001b[0m, std: 0.6310\n",
      "2021-05-12 23:44:15,692 - INFO - Epoch/batch: 1/4077, ibatch: 8577, loss: \u001b[0;36m0.5432\u001b[0m, std: 0.6203\n",
      "2021-05-12 23:44:21,373 - INFO - Epoch/batch: 1/4228, ibatch: 8728, loss: \u001b[0;36m0.5372\u001b[0m, std: 0.6168\n",
      "2021-05-12 23:44:26,814 - INFO - Epoch/batch: 1/4379, ibatch: 8879, loss: \u001b[0;36m0.5421\u001b[0m, std: 0.6231\n",
      "2021-05-12 23:44:40,678 - INFO - loss: \u001b[0;32m0.5385\u001b[0m, std: 0.5695\n",
      "2021-05-12 23:44:41,911 - INFO - Epoch 1 average training loss: \u001b[0;46m0.5462\u001b[0m std: 0.6246\n",
      "2021-05-12 23:44:41,920 - INFO - Epoch 1 average validate loss: \u001b[0;46m0.5379\u001b[0m std: 0.6168\n",
      "2021-05-12 23:44:43,785 - INFO - Epoch/batch: 2/   0, ibatch: 9000, loss: \u001b[0;36m0.5305\u001b[0m, std: 0.6114\n",
      "2021-05-12 23:44:53,430 - INFO - loss: \u001b[0;32m0.5378\u001b[0m, std: 0.5717\n",
      "2021-05-12 23:44:59,245 - INFO - Epoch/batch: 2/ 151, ibatch: 9151, loss: \u001b[0;36m0.5525\u001b[0m, std: 0.6287\n",
      "2021-05-12 23:45:05,087 - INFO - Epoch/batch: 2/ 302, ibatch: 9302, loss: \u001b[0;36m0.5293\u001b[0m, std: 0.6078\n",
      "2021-05-12 23:45:20,070 - INFO - loss: \u001b[0;32m0.5381\u001b[0m, std: 0.5957\n",
      "2021-05-12 23:45:20,174 - INFO - Epoch/batch: 2/ 453, ibatch: 9453, loss: \u001b[0;36m0.5303\u001b[0m, std: 0.6091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: ReduceOnPlateau set learning rate to 0.002187.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:45:25,712 - INFO - Epoch/batch: 2/ 604, ibatch: 9604, loss: \u001b[0;36m0.5291\u001b[0m, std: 0.6114\n",
      "2021-05-12 23:45:31,264 - INFO - Epoch/batch: 2/ 755, ibatch: 9755, loss: \u001b[0;36m0.5527\u001b[0m, std: 0.6204\n",
      "2021-05-12 23:45:45,999 - INFO - loss: \u001b[0;32m0.5343\u001b[0m, std: 0.5891\n",
      "2021-05-12 23:45:46,246 - INFO - Epoch/batch: 2/ 906, ibatch: 9906, loss: \u001b[0;36m0.5330\u001b[0m, std: 0.6142\n",
      "2021-05-12 23:45:51,964 - INFO - Epoch/batch: 2/1057, ibatch: 10057, loss: \u001b[0;36m0.5457\u001b[0m, std: 0.6274\n",
      "2021-05-12 23:45:57,653 - INFO - Epoch/batch: 2/1208, ibatch: 10208, loss: \u001b[0;36m0.5433\u001b[0m, std: 0.6217\n",
      "2021-05-12 23:46:12,735 - INFO - loss: \u001b[0;32m0.5305\u001b[0m, std: 0.6121\n",
      "2021-05-12 23:46:12,751 - INFO - Saved model states in: earlystop_0.5305\n",
      "2021-05-12 23:46:12,753 - INFO - Saved net python code: earlystop_0.5305/paddle_nets.py\n",
      "2021-05-12 23:46:12,760 - INFO - Saved best model: earlystop_0.5305\n",
      "2021-05-12 23:46:12,761 - INFO - Removing earlystop model: earlystop_0.5339\n",
      "2021-05-12 23:46:13,103 - INFO - Epoch/batch: 2/1359, ibatch: 10359, loss: \u001b[0;36m0.5519\u001b[0m, std: 0.6303\n",
      "2021-05-12 23:46:19,097 - INFO - Epoch/batch: 2/1510, ibatch: 10510, loss: \u001b[0;36m0.5386\u001b[0m, std: 0.6140\n",
      "2021-05-12 23:46:24,812 - INFO - Epoch/batch: 2/1661, ibatch: 10661, loss: \u001b[0;36m0.5376\u001b[0m, std: 0.6158\n",
      "2021-05-12 23:46:40,326 - INFO - loss: \u001b[0;32m0.5311\u001b[0m, std: 0.6108\n",
      "2021-05-12 23:46:40,800 - INFO - Epoch/batch: 2/1812, ibatch: 10812, loss: \u001b[0;36m0.5497\u001b[0m, std: 0.6264\n",
      "2021-05-12 23:46:46,561 - INFO - Epoch/batch: 2/1963, ibatch: 10963, loss: \u001b[0;36m0.5498\u001b[0m, std: 0.6279\n",
      "2021-05-12 23:46:52,103 - INFO - Epoch/batch: 2/2114, ibatch: 11114, loss: \u001b[0;36m0.5458\u001b[0m, std: 0.6199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75: ReduceOnPlateau set learning rate to 0.0019683.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:47:07,046 - INFO - loss: \u001b[0;32m0.5364\u001b[0m, std: 0.5905\n",
      "2021-05-12 23:47:07,597 - INFO - Epoch/batch: 2/2265, ibatch: 11265, loss: \u001b[0;36m0.5546\u001b[0m, std: 0.6260\n",
      "2021-05-12 23:47:13,089 - INFO - Epoch/batch: 2/2416, ibatch: 11416, loss: \u001b[0;36m0.5535\u001b[0m, std: 0.6319\n",
      "2021-05-12 23:47:18,609 - INFO - Epoch/batch: 2/2567, ibatch: 11567, loss: \u001b[0;36m0.5466\u001b[0m, std: 0.6297\n",
      "2021-05-12 23:47:33,241 - INFO - loss: \u001b[0;32m0.5426\u001b[0m, std: 0.5611\n",
      "2021-05-12 23:47:33,877 - INFO - Epoch/batch: 2/2718, ibatch: 11718, loss: \u001b[0;36m0.5277\u001b[0m, std: 0.6112\n",
      "2021-05-12 23:47:39,525 - INFO - Epoch/batch: 2/2869, ibatch: 11869, loss: \u001b[0;36m0.5164\u001b[0m, std: 0.5988\n",
      "2021-05-12 23:47:45,399 - INFO - Epoch/batch: 2/3020, ibatch: 12020, loss: \u001b[0;36m0.5466\u001b[0m, std: 0.6318\n",
      "2021-05-12 23:48:00,534 - INFO - loss: \u001b[0;32m0.5323\u001b[0m, std: 0.6269\n",
      "2021-05-12 23:48:01,355 - INFO - Epoch/batch: 2/3171, ibatch: 12171, loss: \u001b[0;36m0.5377\u001b[0m, std: 0.6173\n",
      "2021-05-12 23:48:07,217 - INFO - Epoch/batch: 2/3322, ibatch: 12322, loss: \u001b[0;36m0.5424\u001b[0m, std: 0.6107\n",
      "2021-05-12 23:48:12,749 - INFO - Epoch/batch: 2/3473, ibatch: 12473, loss: \u001b[0;36m0.5163\u001b[0m, std: 0.6067\n",
      "2021-05-12 23:48:27,334 - INFO - loss: \u001b[0;32m0.5317\u001b[0m, std: 0.5897\n",
      "2021-05-12 23:48:28,264 - INFO - Epoch/batch: 2/3624, ibatch: 12624, loss: \u001b[0;36m0.5447\u001b[0m, std: 0.6249\n",
      "2021-05-12 23:48:34,053 - INFO - Epoch/batch: 2/3775, ibatch: 12775, loss: \u001b[0;36m0.5435\u001b[0m, std: 0.6268\n",
      "2021-05-12 23:48:39,859 - INFO - Epoch/batch: 2/3926, ibatch: 12926, loss: \u001b[0;36m0.5416\u001b[0m, std: 0.6177\n",
      "2021-05-12 23:48:54,052 - INFO - loss: \u001b[0;32m0.5295\u001b[0m, std: 0.6066\n",
      "2021-05-12 23:48:54,068 - INFO - Saved model states in: earlystop_0.5295\n",
      "2021-05-12 23:48:54,069 - INFO - Saved net python code: earlystop_0.5295/paddle_nets.py\n",
      "2021-05-12 23:48:54,076 - INFO - Saved best model: earlystop_0.5295\n",
      "2021-05-12 23:48:54,076 - INFO - Removing earlystop model: earlystop_0.5305\n",
      "2021-05-12 23:48:55,238 - INFO - Epoch/batch: 2/4077, ibatch: 13077, loss: \u001b[0;36m0.5551\u001b[0m, std: 0.6325\n",
      "2021-05-12 23:49:00,839 - INFO - Epoch/batch: 2/4228, ibatch: 13228, loss: \u001b[0;36m0.5391\u001b[0m, std: 0.6174\n",
      "2021-05-12 23:49:06,648 - INFO - Epoch/batch: 2/4379, ibatch: 13379, loss: \u001b[0;36m0.5565\u001b[0m, std: 0.6229\n",
      "2021-05-12 23:49:21,070 - INFO - loss: \u001b[0;32m0.5323\u001b[0m, std: 0.5930\n",
      "2021-05-12 23:49:22,308 - INFO - Epoch 2 average training loss: \u001b[0;46m0.5417\u001b[0m std: 0.6202\n",
      "2021-05-12 23:49:22,316 - INFO - Epoch 2 average validate loss: \u001b[0;46m0.5342\u001b[0m std: 0.5952\n",
      "2021-05-12 23:49:24,170 - INFO - Epoch/batch: 3/   0, ibatch: 13500, loss: \u001b[0;36m0.5392\u001b[0m, std: 0.6257\n",
      "2021-05-12 23:49:34,324 - INFO - loss: \u001b[0;32m0.5325\u001b[0m, std: 0.5918\n",
      "2021-05-12 23:49:40,476 - INFO - Epoch/batch: 3/ 151, ibatch: 13651, loss: \u001b[0;36m0.5537\u001b[0m, std: 0.6282\n",
      "2021-05-12 23:49:46,343 - INFO - Epoch/batch: 3/ 302, ibatch: 13802, loss: \u001b[0;36m0.5379\u001b[0m, std: 0.6146\n",
      "2021-05-12 23:50:01,955 - INFO - loss: \u001b[0;32m0.5328\u001b[0m, std: 0.6361\n",
      "2021-05-12 23:50:02,088 - INFO - Epoch/batch: 3/ 453, ibatch: 13953, loss: \u001b[0;36m0.5392\u001b[0m, std: 0.6164\n",
      "2021-05-12 23:50:07,517 - INFO - Epoch/batch: 3/ 604, ibatch: 14104, loss: \u001b[0;36m0.5252\u001b[0m, std: 0.6125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: ReduceOnPlateau set learning rate to 0.00177147.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:50:12,843 - INFO - Epoch/batch: 3/ 755, ibatch: 14255, loss: \u001b[0;36m0.5260\u001b[0m, std: 0.6131\n",
      "2021-05-12 23:50:27,841 - INFO - loss: \u001b[0;32m0.5334\u001b[0m, std: 0.6372\n",
      "2021-05-12 23:50:28,093 - INFO - Epoch/batch: 3/ 906, ibatch: 14406, loss: \u001b[0;36m0.5455\u001b[0m, std: 0.6239\n",
      "2021-05-12 23:50:33,742 - INFO - Epoch/batch: 3/1057, ibatch: 14557, loss: \u001b[0;36m0.5522\u001b[0m, std: 0.6231\n",
      "2021-05-12 23:50:39,259 - INFO - Epoch/batch: 3/1208, ibatch: 14708, loss: \u001b[0;36m0.5372\u001b[0m, std: 0.6090\n",
      "2021-05-12 23:50:54,072 - INFO - loss: \u001b[0;32m0.5333\u001b[0m, std: 0.6407\n",
      "2021-05-12 23:50:54,363 - INFO - Epoch/batch: 3/1359, ibatch: 14859, loss: \u001b[0;36m0.5296\u001b[0m, std: 0.6085\n",
      "2021-05-12 23:51:00,146 - INFO - Epoch/batch: 3/1510, ibatch: 15010, loss: \u001b[0;36m0.5460\u001b[0m, std: 0.6181\n",
      "2021-05-12 23:51:06,151 - INFO - Epoch/batch: 3/1661, ibatch: 15161, loss: \u001b[0;36m0.5439\u001b[0m, std: 0.6218\n",
      "2021-05-12 23:51:21,267 - INFO - loss: \u001b[0;32m0.5324\u001b[0m, std: 0.5850\n",
      "2021-05-12 23:51:21,765 - INFO - Epoch/batch: 3/1812, ibatch: 15312, loss: \u001b[0;36m0.5398\u001b[0m, std: 0.6185\n",
      "2021-05-12 23:51:27,756 - INFO - Epoch/batch: 3/1963, ibatch: 15463, loss: \u001b[0;36m0.5326\u001b[0m, std: 0.6121\n",
      "2021-05-12 23:51:33,409 - INFO - Epoch/batch: 3/2114, ibatch: 15614, loss: \u001b[0;36m0.5374\u001b[0m, std: 0.6162\n",
      "2021-05-12 23:51:47,734 - INFO - loss: \u001b[0;32m0.5292\u001b[0m, std: 0.6028\n",
      "2021-05-12 23:51:47,750 - INFO - Saved model states in: earlystop_0.5292\n",
      "2021-05-12 23:51:47,752 - INFO - Saved net python code: earlystop_0.5292/paddle_nets.py\n",
      "2021-05-12 23:51:47,759 - INFO - Saved best model: earlystop_0.5292\n",
      "2021-05-12 23:51:47,760 - INFO - Removing earlystop model: earlystop_0.5295\n",
      "2021-05-12 23:51:48,338 - INFO - Epoch/batch: 3/2265, ibatch: 15765, loss: \u001b[0;36m0.5460\u001b[0m, std: 0.6262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106: ReduceOnPlateau set learning rate to 0.0015943230000000001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:51:53,697 - INFO - Epoch/batch: 3/2416, ibatch: 15916, loss: \u001b[0;36m0.5244\u001b[0m, std: 0.6038\n",
      "2021-05-12 23:51:59,199 - INFO - Epoch/batch: 3/2567, ibatch: 16067, loss: \u001b[0;36m0.5416\u001b[0m, std: 0.6203\n",
      "2021-05-12 23:52:13,608 - INFO - loss: \u001b[0;32m0.5309\u001b[0m, std: 0.6057\n",
      "2021-05-12 23:52:14,380 - INFO - Epoch/batch: 3/2718, ibatch: 16218, loss: \u001b[0;36m0.5563\u001b[0m, std: 0.6316\n",
      "2021-05-12 23:52:19,880 - INFO - Epoch/batch: 3/2869, ibatch: 16369, loss: \u001b[0;36m0.5472\u001b[0m, std: 0.6187\n",
      "2021-05-12 23:52:25,362 - INFO - Epoch/batch: 3/3020, ibatch: 16520, loss: \u001b[0;36m0.5252\u001b[0m, std: 0.6070\n",
      "2021-05-12 23:52:39,682 - INFO - loss: \u001b[0;32m0.5295\u001b[0m, std: 0.6070\n",
      "2021-05-12 23:52:40,399 - INFO - Epoch/batch: 3/3171, ibatch: 16671, loss: \u001b[0;36m0.5312\u001b[0m, std: 0.6129\n",
      "2021-05-12 23:52:46,244 - INFO - Epoch/batch: 3/3322, ibatch: 16822, loss: \u001b[0;36m0.5491\u001b[0m, std: 0.6285\n",
      "2021-05-12 23:52:51,999 - INFO - Epoch/batch: 3/3473, ibatch: 16973, loss: \u001b[0;36m0.5360\u001b[0m, std: 0.6189\n",
      "2021-05-12 23:53:06,120 - INFO - loss: \u001b[0;32m0.5304\u001b[0m, std: 0.5953\n",
      "2021-05-12 23:53:07,101 - INFO - Epoch/batch: 3/3624, ibatch: 17124, loss: \u001b[0;36m0.5422\u001b[0m, std: 0.6191\n",
      "2021-05-12 23:53:12,904 - INFO - Epoch/batch: 3/3775, ibatch: 17275, loss: \u001b[0;36m0.5455\u001b[0m, std: 0.6210\n",
      "2021-05-12 23:53:18,364 - INFO - Epoch/batch: 3/3926, ibatch: 17426, loss: \u001b[0;36m0.5317\u001b[0m, std: 0.6154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117: ReduceOnPlateau set learning rate to 0.0014348907.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:53:33,689 - INFO - loss: \u001b[0;32m0.5287\u001b[0m, std: 0.5965\n",
      "2021-05-12 23:53:33,718 - INFO - Saved model states in: earlystop_0.5287\n",
      "2021-05-12 23:53:33,720 - INFO - Saved net python code: earlystop_0.5287/paddle_nets.py\n",
      "2021-05-12 23:53:33,727 - INFO - Saved best model: earlystop_0.5287\n",
      "2021-05-12 23:53:33,728 - INFO - Removing earlystop model: earlystop_0.5292\n",
      "2021-05-12 23:53:34,776 - INFO - Epoch/batch: 3/4077, ibatch: 17577, loss: \u001b[0;36m0.5310\u001b[0m, std: 0.6136\n",
      "2021-05-12 23:53:40,267 - INFO - Epoch/batch: 3/4228, ibatch: 17728, loss: \u001b[0;36m0.5238\u001b[0m, std: 0.6057\n",
      "2021-05-12 23:53:45,859 - INFO - Epoch/batch: 3/4379, ibatch: 17879, loss: \u001b[0;36m0.5502\u001b[0m, std: 0.6343\n",
      "2021-05-12 23:53:59,882 - INFO - loss: \u001b[0;32m0.5285\u001b[0m, std: 0.6159\n",
      "2021-05-12 23:53:59,898 - INFO - Saved model states in: earlystop_0.5285\n",
      "2021-05-12 23:53:59,899 - INFO - Saved net python code: earlystop_0.5285/paddle_nets.py\n",
      "2021-05-12 23:53:59,905 - INFO - Saved best model: earlystop_0.5285\n",
      "2021-05-12 23:53:59,906 - INFO - Removing earlystop model: earlystop_0.5287\n",
      "2021-05-12 23:54:01,186 - INFO - Epoch 3 average training loss: \u001b[0;46m0.5388\u001b[0m std: 0.6176\n",
      "2021-05-12 23:54:01,213 - INFO - Epoch 3 average validate loss: \u001b[0;46m0.5311\u001b[0m std: 0.6104\n",
      "2021-05-12 23:54:03,310 - INFO - Epoch/batch: 4/   0, ibatch: 18000, loss: \u001b[0;36m0.5349\u001b[0m, std: 0.6135\n",
      "2021-05-12 23:54:13,108 - INFO - loss: \u001b[0;32m0.5285\u001b[0m, std: 0.6151\n",
      "2021-05-12 23:54:19,043 - INFO - Epoch/batch: 4/ 151, ibatch: 18151, loss: \u001b[0;36m0.5327\u001b[0m, std: 0.6138\n",
      "2021-05-12 23:54:24,404 - INFO - Epoch/batch: 4/ 302, ibatch: 18302, loss: \u001b[0;36m0.5455\u001b[0m, std: 0.6199\n",
      "2021-05-12 23:54:39,489 - INFO - loss: \u001b[0;32m0.5288\u001b[0m, std: 0.6037\n",
      "2021-05-12 23:54:39,647 - INFO - Epoch/batch: 4/ 453, ibatch: 18453, loss: \u001b[0;36m0.5425\u001b[0m, std: 0.6166\n",
      "2021-05-12 23:54:45,530 - INFO - Epoch/batch: 4/ 604, ibatch: 18604, loss: \u001b[0;36m0.5506\u001b[0m, std: 0.6368\n",
      "2021-05-12 23:54:51,168 - INFO - Epoch/batch: 4/ 755, ibatch: 18755, loss: \u001b[0;36m0.5428\u001b[0m, std: 0.6224\n",
      "2021-05-12 23:55:06,011 - INFO - loss: \u001b[0;32m0.5279\u001b[0m, std: 0.6012\n",
      "2021-05-12 23:55:06,029 - INFO - Saved model states in: earlystop_0.5279\n",
      "2021-05-12 23:55:06,031 - INFO - Saved net python code: earlystop_0.5279/paddle_nets.py\n",
      "2021-05-12 23:55:06,039 - INFO - Saved best model: earlystop_0.5279\n",
      "2021-05-12 23:55:06,039 - INFO - Removing earlystop model: earlystop_0.5285\n",
      "2021-05-12 23:55:06,275 - INFO - Epoch/batch: 4/ 906, ibatch: 18906, loss: \u001b[0;36m0.5278\u001b[0m, std: 0.6129\n",
      "2021-05-12 23:55:12,306 - INFO - Epoch/batch: 4/1057, ibatch: 19057, loss: \u001b[0;36m0.5375\u001b[0m, std: 0.6146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128: ReduceOnPlateau set learning rate to 0.00129140163.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:55:18,216 - INFO - Epoch/batch: 4/1208, ibatch: 19208, loss: \u001b[0;36m0.5346\u001b[0m, std: 0.6132\n",
      "2021-05-12 23:55:33,520 - INFO - loss: \u001b[0;32m0.5287\u001b[0m, std: 0.6228\n",
      "2021-05-12 23:55:33,855 - INFO - Epoch/batch: 4/1359, ibatch: 19359, loss: \u001b[0;36m0.5302\u001b[0m, std: 0.6208\n",
      "2021-05-12 23:55:39,411 - INFO - Epoch/batch: 4/1510, ibatch: 19510, loss: \u001b[0;36m0.5391\u001b[0m, std: 0.6123\n",
      "2021-05-12 23:55:44,988 - INFO - Epoch/batch: 4/1661, ibatch: 19661, loss: \u001b[0;36m0.5457\u001b[0m, std: 0.6171\n",
      "2021-05-12 23:56:00,222 - INFO - loss: \u001b[0;32m0.5299\u001b[0m, std: 0.6276\n",
      "2021-05-12 23:56:00,781 - INFO - Epoch/batch: 4/1812, ibatch: 19812, loss: \u001b[0;36m0.5245\u001b[0m, std: 0.6055\n",
      "2021-05-12 23:56:06,384 - INFO - Epoch/batch: 4/1963, ibatch: 19963, loss: \u001b[0;36m0.5322\u001b[0m, std: 0.6120\n",
      "2021-05-12 23:56:11,669 - INFO - Epoch/batch: 4/2114, ibatch: 20114, loss: \u001b[0;36m0.5117\u001b[0m, std: 0.5965\n",
      "2021-05-12 23:56:26,357 - INFO - loss: \u001b[0;32m0.5297\u001b[0m, std: 0.6286\n",
      "2021-05-12 23:56:26,973 - INFO - Epoch/batch: 4/2265, ibatch: 20265, loss: \u001b[0;36m0.5374\u001b[0m, std: 0.6138\n",
      "2021-05-12 23:56:32,648 - INFO - Epoch/batch: 4/2416, ibatch: 20416, loss: \u001b[0;36m0.5417\u001b[0m, std: 0.6125\n",
      "2021-05-12 23:56:38,121 - INFO - Epoch/batch: 4/2567, ibatch: 20567, loss: \u001b[0;36m0.5312\u001b[0m, std: 0.6130\n",
      "2021-05-12 23:56:52,820 - INFO - loss: \u001b[0;32m0.5280\u001b[0m, std: 0.5980\n",
      "2021-05-12 23:56:53,514 - INFO - Epoch/batch: 4/2718, ibatch: 20718, loss: \u001b[0;36m0.5342\u001b[0m, std: 0.6162\n",
      "2021-05-12 23:56:58,927 - INFO - Epoch/batch: 4/2869, ibatch: 20869, loss: \u001b[0;36m0.5177\u001b[0m, std: 0.6099\n",
      "2021-05-12 23:57:04,641 - INFO - Epoch/batch: 4/3020, ibatch: 21020, loss: \u001b[0;36m0.5582\u001b[0m, std: 0.6259\n",
      "2021-05-12 23:57:18,910 - INFO - loss: \u001b[0;32m0.5300\u001b[0m, std: 0.5902\n",
      "2021-05-12 23:57:19,648 - INFO - Epoch/batch: 4/3171, ibatch: 21171, loss: \u001b[0;36m0.5271\u001b[0m, std: 0.5985\n",
      "2021-05-12 23:57:25,349 - INFO - Epoch/batch: 4/3322, ibatch: 21322, loss: \u001b[0;36m0.5592\u001b[0m, std: 0.6357\n",
      "2021-05-12 23:57:31,221 - INFO - Epoch/batch: 4/3473, ibatch: 21473, loss: \u001b[0;36m0.5502\u001b[0m, std: 0.6284\n",
      "2021-05-12 23:57:45,690 - INFO - loss: \u001b[0;32m0.5281\u001b[0m, std: 0.6198\n",
      "2021-05-12 23:57:46,561 - INFO - Epoch/batch: 4/3624, ibatch: 21624, loss: \u001b[0;36m0.5255\u001b[0m, std: 0.6067\n",
      "2021-05-12 23:57:52,060 - INFO - Epoch/batch: 4/3775, ibatch: 21775, loss: \u001b[0;36m0.5507\u001b[0m, std: 0.6207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146: ReduceOnPlateau set learning rate to 0.001162261467.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:57:57,576 - INFO - Epoch/batch: 4/3926, ibatch: 21926, loss: \u001b[0;36m0.5337\u001b[0m, std: 0.6135\n",
      "2021-05-12 23:58:11,522 - INFO - loss: \u001b[0;32m0.5285\u001b[0m, std: 0.6255\n",
      "2021-05-12 23:58:12,462 - INFO - Epoch/batch: 4/4077, ibatch: 22077, loss: \u001b[0;36m0.5400\u001b[0m, std: 0.6168\n",
      "2021-05-12 23:58:18,278 - INFO - Epoch/batch: 4/4228, ibatch: 22228, loss: \u001b[0;36m0.5220\u001b[0m, std: 0.6005\n",
      "2021-05-12 23:58:24,385 - INFO - Epoch/batch: 4/4379, ibatch: 22379, loss: \u001b[0;36m0.5389\u001b[0m, std: 0.6214\n",
      "2021-05-12 23:58:38,410 - INFO - loss: \u001b[0;32m0.5274\u001b[0m, std: 0.5995\n",
      "2021-05-12 23:58:38,426 - INFO - Saved model states in: earlystop_0.5274\n",
      "2021-05-12 23:58:38,428 - INFO - Saved net python code: earlystop_0.5274/paddle_nets.py\n",
      "2021-05-12 23:58:38,435 - INFO - Saved best model: earlystop_0.5274\n",
      "2021-05-12 23:58:38,435 - INFO - Removing earlystop model: earlystop_0.5279\n",
      "2021-05-12 23:58:39,708 - INFO - Epoch 4 average training loss: \u001b[0;46m0.5364\u001b[0m std: 0.6153\n",
      "2021-05-12 23:58:39,712 - INFO - Epoch 4 average validate loss: \u001b[0;46m0.5287\u001b[0m std: 0.6120\n",
      "2021-05-12 23:58:41,779 - INFO - Epoch/batch: 5/   0, ibatch: 22500, loss: \u001b[0;36m0.5235\u001b[0m, std: 0.6112\n",
      "2021-05-12 23:58:51,523 - INFO - loss: \u001b[0;32m0.5277\u001b[0m, std: 0.5971\n",
      "2021-05-12 23:58:57,314 - INFO - Epoch/batch: 5/ 151, ibatch: 22651, loss: \u001b[0;36m0.5560\u001b[0m, std: 0.6263\n",
      "2021-05-12 23:59:03,065 - INFO - Epoch/batch: 5/ 302, ibatch: 22802, loss: \u001b[0;36m0.5547\u001b[0m, std: 0.6280\n",
      "2021-05-12 23:59:18,101 - INFO - loss: \u001b[0;32m0.5276\u001b[0m, std: 0.6074\n",
      "2021-05-12 23:59:18,209 - INFO - Epoch/batch: 5/ 453, ibatch: 22953, loss: \u001b[0;36m0.5357\u001b[0m, std: 0.6194\n",
      "2021-05-12 23:59:23,663 - INFO - Epoch/batch: 5/ 604, ibatch: 23104, loss: \u001b[0;36m0.5183\u001b[0m, std: 0.5974\n",
      "2021-05-12 23:59:29,297 - INFO - Epoch/batch: 5/ 755, ibatch: 23255, loss: \u001b[0;36m0.5244\u001b[0m, std: 0.6016\n",
      "2021-05-12 23:59:44,920 - INFO - loss: \u001b[0;32m0.5271\u001b[0m, std: 0.6116\n",
      "2021-05-12 23:59:44,937 - INFO - Saved model states in: earlystop_0.5271\n",
      "2021-05-12 23:59:44,939 - INFO - Saved net python code: earlystop_0.5271/paddle_nets.py\n",
      "2021-05-12 23:59:44,947 - INFO - Saved best model: earlystop_0.5271\n",
      "2021-05-12 23:59:44,947 - INFO - Removing earlystop model: earlystop_0.5274\n",
      "2021-05-12 23:59:45,227 - INFO - Epoch/batch: 5/ 906, ibatch: 23406, loss: \u001b[0;36m0.5304\u001b[0m, std: 0.6144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157: ReduceOnPlateau set learning rate to 0.0010460353203000001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-12 23:59:51,096 - INFO - Epoch/batch: 5/1057, ibatch: 23557, loss: \u001b[0;36m0.5337\u001b[0m, std: 0.6090\n",
      "2021-05-12 23:59:56,996 - INFO - Epoch/batch: 5/1208, ibatch: 23708, loss: \u001b[0;36m0.5254\u001b[0m, std: 0.6104\n",
      "2021-05-13 00:00:12,188 - INFO - loss: \u001b[0;32m0.5276\u001b[0m, std: 0.6115\n",
      "2021-05-13 00:00:12,607 - INFO - Epoch/batch: 5/1359, ibatch: 23859, loss: \u001b[0;36m0.5378\u001b[0m, std: 0.6192\n",
      "2021-05-13 00:00:18,059 - INFO - Epoch/batch: 5/1510, ibatch: 24010, loss: \u001b[0;36m0.5162\u001b[0m, std: 0.6083\n",
      "2021-05-13 00:00:24,042 - INFO - Epoch/batch: 5/1661, ibatch: 24161, loss: \u001b[0;36m0.5394\u001b[0m, std: 0.6181\n",
      "2021-05-13 00:00:38,868 - INFO - loss: \u001b[0;32m0.5299\u001b[0m, std: 0.5788\n",
      "2021-05-13 00:00:39,365 - INFO - Epoch/batch: 5/1812, ibatch: 24312, loss: \u001b[0;36m0.5562\u001b[0m, std: 0.6248\n",
      "2021-05-13 00:00:45,181 - INFO - Epoch/batch: 5/1963, ibatch: 24463, loss: \u001b[0;36m0.5412\u001b[0m, std: 0.6161\n",
      "2021-05-13 00:00:50,943 - INFO - Epoch/batch: 5/2114, ibatch: 24614, loss: \u001b[0;36m0.5493\u001b[0m, std: 0.6223\n",
      "2021-05-13 00:01:06,114 - INFO - loss: \u001b[0;32m0.5263\u001b[0m, std: 0.6098\n",
      "2021-05-13 00:01:06,138 - INFO - Saved model states in: earlystop_0.5263\n",
      "2021-05-13 00:01:06,140 - INFO - Saved net python code: earlystop_0.5263/paddle_nets.py\n",
      "2021-05-13 00:01:06,151 - INFO - Saved best model: earlystop_0.5263\n",
      "2021-05-13 00:01:06,152 - INFO - Removing earlystop model: earlystop_0.5271\n",
      "2021-05-13 00:01:06,699 - INFO - Epoch/batch: 5/2265, ibatch: 24765, loss: \u001b[0;36m0.5352\u001b[0m, std: 0.6135\n",
      "2021-05-13 00:01:12,095 - INFO - Epoch/batch: 5/2416, ibatch: 24916, loss: \u001b[0;36m0.5323\u001b[0m, std: 0.6088\n",
      "2021-05-13 00:01:17,743 - INFO - Epoch/batch: 5/2567, ibatch: 25067, loss: \u001b[0;36m0.5273\u001b[0m, std: 0.6114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168: ReduceOnPlateau set learning rate to 0.0009414317882700001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:01:32,665 - INFO - loss: \u001b[0;32m0.5275\u001b[0m, std: 0.6024\n",
      "2021-05-13 00:01:33,395 - INFO - Epoch/batch: 5/2718, ibatch: 25218, loss: \u001b[0;36m0.5457\u001b[0m, std: 0.6195\n",
      "2021-05-13 00:01:39,030 - INFO - Epoch/batch: 5/2869, ibatch: 25369, loss: \u001b[0;36m0.5239\u001b[0m, std: 0.5998\n",
      "2021-05-13 00:01:44,630 - INFO - Epoch/batch: 5/3020, ibatch: 25520, loss: \u001b[0;36m0.5239\u001b[0m, std: 0.6033\n",
      "2021-05-13 00:01:59,226 - INFO - loss: \u001b[0;32m0.5265\u001b[0m, std: 0.6178\n",
      "2021-05-13 00:02:00,081 - INFO - Epoch/batch: 5/3171, ibatch: 25671, loss: \u001b[0;36m0.5372\u001b[0m, std: 0.6221\n",
      "2021-05-13 00:02:05,627 - INFO - Epoch/batch: 5/3322, ibatch: 25822, loss: \u001b[0;36m0.5249\u001b[0m, std: 0.6114\n",
      "2021-05-13 00:02:11,183 - INFO - Epoch/batch: 5/3473, ibatch: 25973, loss: \u001b[0;36m0.5221\u001b[0m, std: 0.6072\n",
      "2021-05-13 00:02:26,001 - INFO - loss: \u001b[0;32m0.5259\u001b[0m, std: 0.6022\n",
      "2021-05-13 00:02:26,017 - INFO - Saved model states in: earlystop_0.5259\n",
      "2021-05-13 00:02:26,019 - INFO - Saved net python code: earlystop_0.5259/paddle_nets.py\n",
      "2021-05-13 00:02:26,025 - INFO - Saved best model: earlystop_0.5259\n",
      "2021-05-13 00:02:26,026 - INFO - Removing earlystop model: earlystop_0.5263\n",
      "2021-05-13 00:02:26,974 - INFO - Epoch/batch: 5/3624, ibatch: 26124, loss: \u001b[0;36m0.5491\u001b[0m, std: 0.6224\n",
      "2021-05-13 00:02:32,961 - INFO - Epoch/batch: 5/3775, ibatch: 26275, loss: \u001b[0;36m0.5314\u001b[0m, std: 0.6127\n",
      "2021-05-13 00:02:38,815 - INFO - Epoch/batch: 5/3926, ibatch: 26426, loss: \u001b[0;36m0.5206\u001b[0m, std: 0.5964\n",
      "2021-05-13 00:02:53,332 - INFO - loss: \u001b[0;32m0.5262\u001b[0m, std: 0.6089\n",
      "2021-05-13 00:02:54,349 - INFO - Epoch/batch: 5/4077, ibatch: 26577, loss: \u001b[0;36m0.5243\u001b[0m, std: 0.6118\n",
      "2021-05-13 00:03:00,073 - INFO - Epoch/batch: 5/4228, ibatch: 26728, loss: \u001b[0;36m0.5363\u001b[0m, std: 0.6203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179: ReduceOnPlateau set learning rate to 0.0008472886094430002.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:03:06,050 - INFO - Epoch/batch: 5/4379, ibatch: 26879, loss: \u001b[0;36m0.5440\u001b[0m, std: 0.6207\n",
      "2021-05-13 00:03:20,487 - INFO - loss: \u001b[0;32m0.5262\u001b[0m, std: 0.6102\n",
      "2021-05-13 00:03:21,669 - INFO - Epoch 5 average training loss: \u001b[0;46m0.5345\u001b[0m std: 0.6137\n",
      "2021-05-13 00:03:21,711 - INFO - Epoch 5 average validate loss: \u001b[0;46m0.5271\u001b[0m std: 0.6052\n",
      "2021-05-13 00:03:23,576 - INFO - Epoch/batch: 6/   0, ibatch: 27000, loss: \u001b[0;36m0.5378\u001b[0m, std: 0.6141\n",
      "2021-05-13 00:03:33,212 - INFO - loss: \u001b[0;32m0.5262\u001b[0m, std: 0.6107\n",
      "2021-05-13 00:03:38,842 - INFO - Epoch/batch: 6/ 151, ibatch: 27151, loss: \u001b[0;36m0.5322\u001b[0m, std: 0.6126\n",
      "2021-05-13 00:03:44,509 - INFO - Epoch/batch: 6/ 302, ibatch: 27302, loss: \u001b[0;36m0.5281\u001b[0m, std: 0.6105\n",
      "2021-05-13 00:03:59,992 - INFO - loss: \u001b[0;32m0.5267\u001b[0m, std: 0.5979\n",
      "2021-05-13 00:04:00,121 - INFO - Epoch/batch: 6/ 453, ibatch: 27453, loss: \u001b[0;36m0.5474\u001b[0m, std: 0.6248\n",
      "2021-05-13 00:04:05,624 - INFO - Epoch/batch: 6/ 604, ibatch: 27604, loss: \u001b[0;36m0.5323\u001b[0m, std: 0.6093\n",
      "2021-05-13 00:04:11,246 - INFO - Epoch/batch: 6/ 755, ibatch: 27755, loss: \u001b[0;36m0.5321\u001b[0m, std: 0.6038\n",
      "2021-05-13 00:04:26,478 - INFO - loss: \u001b[0;32m0.5264\u001b[0m, std: 0.6007\n",
      "2021-05-13 00:04:26,717 - INFO - Epoch/batch: 6/ 906, ibatch: 27906, loss: \u001b[0;36m0.5291\u001b[0m, std: 0.6063\n",
      "2021-05-13 00:04:32,313 - INFO - Epoch/batch: 6/1057, ibatch: 28057, loss: \u001b[0;36m0.5211\u001b[0m, std: 0.5974\n",
      "2021-05-13 00:04:38,030 - INFO - Epoch/batch: 6/1208, ibatch: 28208, loss: \u001b[0;36m0.5333\u001b[0m, std: 0.6172\n",
      "2021-05-13 00:04:53,827 - INFO - loss: \u001b[0;32m0.5262\u001b[0m, std: 0.5947\n",
      "2021-05-13 00:04:54,228 - INFO - Epoch/batch: 6/1359, ibatch: 28359, loss: \u001b[0;36m0.5457\u001b[0m, std: 0.6266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190: ReduceOnPlateau set learning rate to 0.0007625597484987002.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:05:00,835 - INFO - Epoch/batch: 6/1510, ibatch: 28510, loss: \u001b[0;36m0.5438\u001b[0m, std: 0.6166\n",
      "2021-05-13 00:05:07,511 - INFO - Epoch/batch: 6/1661, ibatch: 28661, loss: \u001b[0;36m0.5366\u001b[0m, std: 0.6135\n",
      "2021-05-13 00:05:22,590 - INFO - loss: \u001b[0;32m0.5284\u001b[0m, std: 0.6358\n",
      "2021-05-13 00:05:23,051 - INFO - Epoch/batch: 6/1812, ibatch: 28812, loss: \u001b[0;36m0.5207\u001b[0m, std: 0.6035\n",
      "2021-05-13 00:05:28,726 - INFO - Epoch/batch: 6/1963, ibatch: 28963, loss: \u001b[0;36m0.5305\u001b[0m, std: 0.6102\n",
      "2021-05-13 00:05:34,317 - INFO - Epoch/batch: 6/2114, ibatch: 29114, loss: \u001b[0;36m0.5261\u001b[0m, std: 0.6116\n",
      "2021-05-13 00:05:49,382 - INFO - loss: \u001b[0;32m0.5262\u001b[0m, std: 0.6010\n",
      "2021-05-13 00:05:49,960 - INFO - Epoch/batch: 6/2265, ibatch: 29265, loss: \u001b[0;36m0.5277\u001b[0m, std: 0.6077\n",
      "2021-05-13 00:05:56,388 - INFO - Epoch/batch: 6/2416, ibatch: 29416, loss: \u001b[0;36m0.5196\u001b[0m, std: 0.6059\n",
      "2021-05-13 00:06:02,762 - INFO - Epoch/batch: 6/2567, ibatch: 29567, loss: \u001b[0;36m0.5224\u001b[0m, std: 0.6047\n",
      "2021-05-13 00:06:17,789 - INFO - loss: \u001b[0;32m0.5254\u001b[0m, std: 0.6049\n",
      "2021-05-13 00:06:17,805 - INFO - Saved model states in: earlystop_0.5254\n",
      "2021-05-13 00:06:17,807 - INFO - Saved net python code: earlystop_0.5254/paddle_nets.py\n",
      "2021-05-13 00:06:17,815 - INFO - Saved best model: earlystop_0.5254\n",
      "2021-05-13 00:06:17,816 - INFO - Removing earlystop model: earlystop_0.5259\n",
      "2021-05-13 00:06:18,493 - INFO - Epoch/batch: 6/2718, ibatch: 29718, loss: \u001b[0;36m0.5471\u001b[0m, std: 0.6258\n",
      "2021-05-13 00:06:24,053 - INFO - Epoch/batch: 6/2869, ibatch: 29869, loss: \u001b[0;36m0.5354\u001b[0m, std: 0.6061\n",
      "2021-05-13 00:06:29,971 - INFO - Epoch/batch: 6/3020, ibatch: 30020, loss: \u001b[0;36m0.5655\u001b[0m, std: 0.6338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201: ReduceOnPlateau set learning rate to 0.0006863037736488302.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:06:44,479 - INFO - loss: \u001b[0;32m0.5266\u001b[0m, std: 0.6221\n",
      "2021-05-13 00:06:45,340 - INFO - Epoch/batch: 6/3171, ibatch: 30171, loss: \u001b[0;36m0.5230\u001b[0m, std: 0.5964\n",
      "2021-05-13 00:06:51,243 - INFO - Epoch/batch: 6/3322, ibatch: 30322, loss: \u001b[0;36m0.5300\u001b[0m, std: 0.6162\n",
      "2021-05-13 00:06:57,054 - INFO - Epoch/batch: 6/3473, ibatch: 30473, loss: \u001b[0;36m0.5368\u001b[0m, std: 0.6102\n",
      "2021-05-13 00:07:11,288 - INFO - loss: \u001b[0;32m0.5261\u001b[0m, std: 0.6108\n",
      "2021-05-13 00:07:12,219 - INFO - Epoch/batch: 6/3624, ibatch: 30624, loss: \u001b[0;36m0.5231\u001b[0m, std: 0.6029\n",
      "2021-05-13 00:07:18,075 - INFO - Epoch/batch: 6/3775, ibatch: 30775, loss: \u001b[0;36m0.5430\u001b[0m, std: 0.6242\n",
      "2021-05-13 00:07:23,689 - INFO - Epoch/batch: 6/3926, ibatch: 30926, loss: \u001b[0;36m0.5319\u001b[0m, std: 0.6139\n",
      "2021-05-13 00:07:38,163 - INFO - loss: \u001b[0;32m0.5276\u001b[0m, std: 0.6307\n",
      "2021-05-13 00:07:39,121 - INFO - Epoch/batch: 6/4077, ibatch: 31077, loss: \u001b[0;36m0.5268\u001b[0m, std: 0.6022\n",
      "2021-05-13 00:07:44,930 - INFO - Epoch/batch: 6/4228, ibatch: 31228, loss: \u001b[0;36m0.5331\u001b[0m, std: 0.6135\n",
      "2021-05-13 00:07:50,496 - INFO - Epoch/batch: 6/4379, ibatch: 31379, loss: \u001b[0;36m0.5232\u001b[0m, std: 0.6028\n",
      "2021-05-13 00:08:04,672 - INFO - loss: \u001b[0;32m0.5257\u001b[0m, std: 0.6063\n",
      "2021-05-13 00:08:05,926 - INFO - Epoch 6 average training loss: \u001b[0;46m0.5328\u001b[0m std: 0.6117\n",
      "2021-05-13 00:08:05,930 - INFO - Epoch 6 average validate loss: \u001b[0;46m0.5265\u001b[0m std: 0.6105\n",
      "2021-05-13 00:08:07,857 - INFO - Epoch/batch: 7/   0, ibatch: 31500, loss: \u001b[0;36m0.5384\u001b[0m, std: 0.6250\n",
      "2021-05-13 00:08:18,035 - INFO - loss: \u001b[0;32m0.5257\u001b[0m, std: 0.6049\n",
      "2021-05-13 00:08:24,294 - INFO - Epoch/batch: 7/ 151, ibatch: 31651, loss: \u001b[0;36m0.5372\u001b[0m, std: 0.6188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 212: ReduceOnPlateau set learning rate to 0.0006176733962839472.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:08:30,221 - INFO - Epoch/batch: 7/ 302, ibatch: 31802, loss: \u001b[0;36m0.5374\u001b[0m, std: 0.6177\n",
      "2021-05-13 00:08:46,073 - INFO - loss: \u001b[0;32m0.5259\u001b[0m, std: 0.6058\n",
      "2021-05-13 00:08:46,171 - INFO - Epoch/batch: 7/ 453, ibatch: 31953, loss: \u001b[0;36m0.5431\u001b[0m, std: 0.6134\n",
      "2021-05-13 00:08:51,946 - INFO - Epoch/batch: 7/ 604, ibatch: 32104, loss: \u001b[0;36m0.5214\u001b[0m, std: 0.5979\n",
      "2021-05-13 00:08:57,810 - INFO - Epoch/batch: 7/ 755, ibatch: 32255, loss: \u001b[0;36m0.5394\u001b[0m, std: 0.6224\n",
      "2021-05-13 00:09:13,069 - INFO - loss: \u001b[0;32m0.5258\u001b[0m, std: 0.6143\n",
      "2021-05-13 00:09:13,459 - INFO - Epoch/batch: 7/ 906, ibatch: 32406, loss: \u001b[0;36m0.5345\u001b[0m, std: 0.6121\n",
      "2021-05-13 00:09:19,353 - INFO - Epoch/batch: 7/1057, ibatch: 32557, loss: \u001b[0;36m0.5363\u001b[0m, std: 0.6100\n",
      "2021-05-13 00:09:24,983 - INFO - Epoch/batch: 7/1208, ibatch: 32708, loss: \u001b[0;36m0.5129\u001b[0m, std: 0.6035\n",
      "2021-05-13 00:09:40,362 - INFO - loss: \u001b[0;32m0.5254\u001b[0m, std: 0.5930\n",
      "2021-05-13 00:09:40,698 - INFO - Epoch/batch: 7/1359, ibatch: 32859, loss: \u001b[0;36m0.5293\u001b[0m, std: 0.6205\n",
      "2021-05-13 00:09:46,445 - INFO - Epoch/batch: 7/1510, ibatch: 33010, loss: \u001b[0;36m0.5248\u001b[0m, std: 0.6059\n",
      "2021-05-13 00:09:51,997 - INFO - Epoch/batch: 7/1661, ibatch: 33161, loss: \u001b[0;36m0.5122\u001b[0m, std: 0.5972\n",
      "2021-05-13 00:10:06,954 - INFO - loss: \u001b[0;32m0.5257\u001b[0m, std: 0.6097\n",
      "2021-05-13 00:10:07,434 - INFO - Epoch/batch: 7/1812, ibatch: 33312, loss: \u001b[0;36m0.5166\u001b[0m, std: 0.5976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223: ReduceOnPlateau set learning rate to 0.0005559060566555524.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:10:13,018 - INFO - Epoch/batch: 7/1963, ibatch: 33463, loss: \u001b[0;36m0.5368\u001b[0m, std: 0.6184\n",
      "2021-05-13 00:10:18,504 - INFO - Epoch/batch: 7/2114, ibatch: 33614, loss: \u001b[0;36m0.5105\u001b[0m, std: 0.5957\n",
      "2021-05-13 00:10:33,902 - INFO - loss: \u001b[0;32m0.5250\u001b[0m, std: 0.6094\n",
      "2021-05-13 00:10:33,925 - INFO - Saved model states in: earlystop_0.5250\n",
      "2021-05-13 00:10:33,927 - INFO - Saved net python code: earlystop_0.5250/paddle_nets.py\n",
      "2021-05-13 00:10:33,935 - INFO - Saved best model: earlystop_0.5250\n",
      "2021-05-13 00:10:33,936 - INFO - Removing earlystop model: earlystop_0.5254\n",
      "2021-05-13 00:10:34,558 - INFO - Epoch/batch: 7/2265, ibatch: 33765, loss: \u001b[0;36m0.5386\u001b[0m, std: 0.6039\n",
      "2021-05-13 00:10:40,233 - INFO - Epoch/batch: 7/2416, ibatch: 33916, loss: \u001b[0;36m0.5600\u001b[0m, std: 0.6214\n",
      "2021-05-13 00:10:46,090 - INFO - Epoch/batch: 7/2567, ibatch: 34067, loss: \u001b[0;36m0.5371\u001b[0m, std: 0.6211\n",
      "2021-05-13 00:11:01,117 - INFO - loss: \u001b[0;32m0.5252\u001b[0m, std: 0.6075\n",
      "2021-05-13 00:11:01,888 - INFO - Epoch/batch: 7/2718, ibatch: 34218, loss: \u001b[0;36m0.5406\u001b[0m, std: 0.6163\n",
      "2021-05-13 00:11:07,592 - INFO - Epoch/batch: 7/2869, ibatch: 34369, loss: \u001b[0;36m0.5315\u001b[0m, std: 0.6015\n",
      "2021-05-13 00:11:13,402 - INFO - Epoch/batch: 7/3020, ibatch: 34520, loss: \u001b[0;36m0.5569\u001b[0m, std: 0.6365\n",
      "2021-05-13 00:11:28,816 - INFO - loss: \u001b[0;32m0.5261\u001b[0m, std: 0.6174\n",
      "2021-05-13 00:11:29,580 - INFO - Epoch/batch: 7/3171, ibatch: 34671, loss: \u001b[0;36m0.5403\u001b[0m, std: 0.6197\n",
      "2021-05-13 00:11:35,540 - INFO - Epoch/batch: 7/3322, ibatch: 34822, loss: \u001b[0;36m0.5396\u001b[0m, std: 0.6074\n",
      "2021-05-13 00:11:41,518 - INFO - Epoch/batch: 7/3473, ibatch: 34973, loss: \u001b[0;36m0.5284\u001b[0m, std: 0.6049\n",
      "2021-05-13 00:11:56,360 - INFO - loss: \u001b[0;32m0.5263\u001b[0m, std: 0.6281\n",
      "2021-05-13 00:11:57,274 - INFO - Epoch/batch: 7/3624, ibatch: 35124, loss: \u001b[0;36m0.5270\u001b[0m, std: 0.6126\n",
      "2021-05-13 00:12:02,836 - INFO - Epoch/batch: 7/3775, ibatch: 35275, loss: \u001b[0;36m0.5344\u001b[0m, std: 0.6122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236: ReduceOnPlateau set learning rate to 0.0005003154509899972.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:12:08,483 - INFO - Epoch/batch: 7/3926, ibatch: 35426, loss: \u001b[0;36m0.5347\u001b[0m, std: 0.6143\n",
      "2021-05-13 00:12:22,933 - INFO - loss: \u001b[0;32m0.5273\u001b[0m, std: 0.5855\n",
      "2021-05-13 00:12:23,997 - INFO - Epoch/batch: 7/4077, ibatch: 35577, loss: \u001b[0;36m0.5327\u001b[0m, std: 0.6161\n",
      "2021-05-13 00:12:29,886 - INFO - Epoch/batch: 7/4228, ibatch: 35728, loss: \u001b[0;36m0.5250\u001b[0m, std: 0.6037\n",
      "2021-05-13 00:12:36,369 - INFO - Epoch/batch: 7/4379, ibatch: 35879, loss: \u001b[0;36m0.5090\u001b[0m, std: 0.5892\n",
      "2021-05-13 00:12:51,119 - INFO - loss: \u001b[0;32m0.5254\u001b[0m, std: 0.6082\n",
      "2021-05-13 00:12:52,511 - INFO - Epoch 7 average training loss: \u001b[0;46m0.5316\u001b[0m std: 0.6104\n",
      "2021-05-13 00:12:52,516 - INFO - Epoch 7 average validate loss: \u001b[0;46m0.5258\u001b[0m std: 0.6076\n",
      "2021-05-13 00:12:54,534 - INFO - Epoch/batch: 8/   0, ibatch: 36000, loss: \u001b[0;36m0.5165\u001b[0m, std: 0.6003\n",
      "2021-05-13 00:13:04,445 - INFO - loss: \u001b[0;32m0.5254\u001b[0m, std: 0.6088\n",
      "2021-05-13 00:13:10,225 - INFO - Epoch/batch: 8/ 151, ibatch: 36151, loss: \u001b[0;36m0.5391\u001b[0m, std: 0.6181\n",
      "2021-05-13 00:13:16,953 - INFO - Epoch/batch: 8/ 302, ibatch: 36302, loss: \u001b[0;36m0.5276\u001b[0m, std: 0.6058\n",
      "2021-05-13 00:13:32,796 - INFO - loss: \u001b[0;32m0.5247\u001b[0m, std: 0.6066\n",
      "2021-05-13 00:13:32,814 - INFO - Saved model states in: earlystop_0.5247\n",
      "2021-05-13 00:13:32,817 - INFO - Saved net python code: earlystop_0.5247/paddle_nets.py\n",
      "2021-05-13 00:13:32,826 - INFO - Saved best model: earlystop_0.5247\n",
      "2021-05-13 00:13:32,827 - INFO - Removing earlystop model: earlystop_0.5250\n",
      "2021-05-13 00:13:32,932 - INFO - Epoch/batch: 8/ 453, ibatch: 36453, loss: \u001b[0;36m0.5220\u001b[0m, std: 0.6034\n",
      "2021-05-13 00:13:39,068 - INFO - Epoch/batch: 8/ 604, ibatch: 36604, loss: \u001b[0;36m0.5380\u001b[0m, std: 0.6141\n",
      "2021-05-13 00:13:44,924 - INFO - Epoch/batch: 8/ 755, ibatch: 36755, loss: \u001b[0;36m0.5341\u001b[0m, std: 0.6070\n",
      "2021-05-13 00:14:00,768 - INFO - loss: \u001b[0;32m0.5257\u001b[0m, std: 0.5922\n",
      "2021-05-13 00:14:00,974 - INFO - Epoch/batch: 8/ 906, ibatch: 36906, loss: \u001b[0;36m0.5530\u001b[0m, std: 0.6288\n",
      "2021-05-13 00:14:06,914 - INFO - Epoch/batch: 8/1057, ibatch: 37057, loss: \u001b[0;36m0.5366\u001b[0m, std: 0.6154\n",
      "2021-05-13 00:14:12,373 - INFO - Epoch/batch: 8/1208, ibatch: 37208, loss: \u001b[0;36m0.5117\u001b[0m, std: 0.5982\n",
      "2021-05-13 00:14:27,322 - INFO - loss: \u001b[0;32m0.5259\u001b[0m, std: 0.6030\n",
      "2021-05-13 00:14:27,592 - INFO - Epoch/batch: 8/1359, ibatch: 37359, loss: \u001b[0;36m0.5360\u001b[0m, std: 0.6134\n",
      "2021-05-13 00:14:33,100 - INFO - Epoch/batch: 8/1510, ibatch: 37510, loss: \u001b[0;36m0.5145\u001b[0m, std: 0.5972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 251: ReduceOnPlateau set learning rate to 0.00045028390589099747.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:14:38,566 - INFO - Epoch/batch: 8/1661, ibatch: 37661, loss: \u001b[0;36m0.5284\u001b[0m, std: 0.6158\n",
      "2021-05-13 00:14:53,236 - INFO - loss: \u001b[0;32m0.5256\u001b[0m, std: 0.5945\n",
      "2021-05-13 00:14:53,708 - INFO - Epoch/batch: 8/1812, ibatch: 37812, loss: \u001b[0;36m0.5406\u001b[0m, std: 0.6183\n",
      "2021-05-13 00:14:59,297 - INFO - Epoch/batch: 8/1963, ibatch: 37963, loss: \u001b[0;36m0.5218\u001b[0m, std: 0.5954\n",
      "2021-05-13 00:15:05,021 - INFO - Epoch/batch: 8/2114, ibatch: 38114, loss: \u001b[0;36m0.5294\u001b[0m, std: 0.6097\n",
      "2021-05-13 00:15:20,672 - INFO - loss: \u001b[0;32m0.5254\u001b[0m, std: 0.5976\n",
      "2021-05-13 00:15:21,335 - INFO - Epoch/batch: 8/2265, ibatch: 38265, loss: \u001b[0;36m0.5431\u001b[0m, std: 0.6169\n",
      "2021-05-13 00:15:27,062 - INFO - Epoch/batch: 8/2416, ibatch: 38416, loss: \u001b[0;36m0.5279\u001b[0m, std: 0.5995\n",
      "2021-05-13 00:15:32,660 - INFO - Epoch/batch: 8/2567, ibatch: 38567, loss: \u001b[0;36m0.5176\u001b[0m, std: 0.6018\n",
      "2021-05-13 00:15:47,425 - INFO - loss: \u001b[0;32m0.5248\u001b[0m, std: 0.6066\n",
      "2021-05-13 00:15:48,133 - INFO - Epoch/batch: 8/2718, ibatch: 38718, loss: \u001b[0;36m0.5374\u001b[0m, std: 0.6183\n",
      "2021-05-13 00:15:53,903 - INFO - Epoch/batch: 8/2869, ibatch: 38869, loss: \u001b[0;36m0.5347\u001b[0m, std: 0.6141\n",
      "2021-05-13 00:15:59,350 - INFO - Epoch/batch: 8/3020, ibatch: 39020, loss: \u001b[0;36m0.5187\u001b[0m, std: 0.6040\n",
      "2021-05-13 00:16:13,917 - INFO - loss: \u001b[0;32m0.5249\u001b[0m, std: 0.6102\n",
      "2021-05-13 00:16:14,762 - INFO - Epoch/batch: 8/3171, ibatch: 39171, loss: \u001b[0;36m0.5329\u001b[0m, std: 0.6102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 262: ReduceOnPlateau set learning rate to 0.0004052555153018977.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:16:20,430 - INFO - Epoch/batch: 8/3322, ibatch: 39322, loss: \u001b[0;36m0.5374\u001b[0m, std: 0.6225\n",
      "2021-05-13 00:16:26,404 - INFO - Epoch/batch: 8/3473, ibatch: 39473, loss: \u001b[0;36m0.5368\u001b[0m, std: 0.6218\n",
      "2021-05-13 00:16:41,621 - INFO - loss: \u001b[0;32m0.5244\u001b[0m, std: 0.5992\n",
      "2021-05-13 00:16:41,637 - INFO - Saved model states in: earlystop_0.5244\n",
      "2021-05-13 00:16:41,638 - INFO - Saved net python code: earlystop_0.5244/paddle_nets.py\n",
      "2021-05-13 00:16:41,645 - INFO - Saved best model: earlystop_0.5244\n",
      "2021-05-13 00:16:41,646 - INFO - Removing earlystop model: earlystop_0.5247\n",
      "2021-05-13 00:16:42,584 - INFO - Epoch/batch: 8/3624, ibatch: 39624, loss: \u001b[0;36m0.5304\u001b[0m, std: 0.6076\n",
      "2021-05-13 00:16:48,274 - INFO - Epoch/batch: 8/3775, ibatch: 39775, loss: \u001b[0;36m0.5320\u001b[0m, std: 0.6046\n",
      "2021-05-13 00:16:53,942 - INFO - Epoch/batch: 8/3926, ibatch: 39926, loss: \u001b[0;36m0.5384\u001b[0m, std: 0.6122\n",
      "2021-05-13 00:17:08,780 - INFO - loss: \u001b[0;32m0.5256\u001b[0m, std: 0.6207\n",
      "2021-05-13 00:17:09,781 - INFO - Epoch/batch: 8/4077, ibatch: 40077, loss: \u001b[0;36m0.5059\u001b[0m, std: 0.5903\n",
      "2021-05-13 00:17:15,608 - INFO - Epoch/batch: 8/4228, ibatch: 40228, loss: \u001b[0;36m0.5414\u001b[0m, std: 0.6229\n",
      "2021-05-13 00:17:21,105 - INFO - Epoch/batch: 8/4379, ibatch: 40379, loss: \u001b[0;36m0.5214\u001b[0m, std: 0.5969\n",
      "2021-05-13 00:17:35,273 - INFO - loss: \u001b[0;32m0.5250\u001b[0m, std: 0.6098\n",
      "2021-05-13 00:17:36,608 - INFO - Epoch 8 average training loss: \u001b[0;46m0.5308\u001b[0m std: 0.6098\n",
      "2021-05-13 00:17:36,614 - INFO - Epoch 8 average validate loss: \u001b[0;46m0.5252\u001b[0m std: 0.6045\n",
      "2021-05-13 00:17:38,551 - INFO - Epoch/batch: 9/   0, ibatch: 40500, loss: \u001b[0;36m0.5361\u001b[0m, std: 0.6081\n",
      "2021-05-13 00:17:48,537 - INFO - loss: \u001b[0;32m0.5249\u001b[0m, std: 0.6088\n",
      "2021-05-13 00:17:54,562 - INFO - Epoch/batch: 9/ 151, ibatch: 40651, loss: \u001b[0;36m0.5379\u001b[0m, std: 0.6122\n",
      "2021-05-13 00:18:00,284 - INFO - Epoch/batch: 9/ 302, ibatch: 40802, loss: \u001b[0;36m0.5392\u001b[0m, std: 0.6249\n",
      "2021-05-13 00:18:15,665 - INFO - loss: \u001b[0;32m0.5251\u001b[0m, std: 0.6114\n",
      "2021-05-13 00:18:15,788 - INFO - Epoch/batch: 9/ 453, ibatch: 40953, loss: \u001b[0;36m0.5340\u001b[0m, std: 0.6064\n",
      "2021-05-13 00:18:21,217 - INFO - Epoch/batch: 9/ 604, ibatch: 41104, loss: \u001b[0;36m0.5132\u001b[0m, std: 0.5987\n",
      "2021-05-13 00:18:26,950 - INFO - Epoch/batch: 9/ 755, ibatch: 41255, loss: \u001b[0;36m0.5199\u001b[0m, std: 0.6003\n",
      "2021-05-13 00:18:42,066 - INFO - loss: \u001b[0;32m0.5260\u001b[0m, std: 0.6199\n",
      "2021-05-13 00:18:42,312 - INFO - Epoch/batch: 9/ 906, ibatch: 41406, loss: \u001b[0;36m0.5219\u001b[0m, std: 0.6034\n",
      "2021-05-13 00:18:48,048 - INFO - Epoch/batch: 9/1057, ibatch: 41557, loss: \u001b[0;36m0.5278\u001b[0m, std: 0.6090\n",
      "2021-05-13 00:18:53,490 - INFO - Epoch/batch: 9/1208, ibatch: 41708, loss: \u001b[0;36m0.5414\u001b[0m, std: 0.6233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 279: ReduceOnPlateau set learning rate to 0.00036472996377170795.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:19:08,877 - INFO - loss: \u001b[0;32m0.5244\u001b[0m, std: 0.6018\n",
      "2021-05-13 00:19:08,895 - INFO - Saved model states in: earlystop_0.5244.1\n",
      "2021-05-13 00:19:08,896 - INFO - Saved net python code: earlystop_0.5244.1/paddle_nets.py\n",
      "2021-05-13 00:19:08,904 - INFO - Saved best model: earlystop_0.5244.1\n",
      "2021-05-13 00:19:08,905 - INFO - Removing earlystop model: earlystop_0.5244\n",
      "2021-05-13 00:19:09,271 - INFO - Epoch/batch: 9/1359, ibatch: 41859, loss: \u001b[0;36m0.5181\u001b[0m, std: 0.6017\n",
      "2021-05-13 00:19:15,276 - INFO - Epoch/batch: 9/1510, ibatch: 42010, loss: \u001b[0;36m0.5521\u001b[0m, std: 0.6284\n",
      "2021-05-13 00:19:21,106 - INFO - Epoch/batch: 9/1661, ibatch: 42161, loss: \u001b[0;36m0.5266\u001b[0m, std: 0.6060\n",
      "2021-05-13 00:19:36,188 - INFO - loss: \u001b[0;32m0.5259\u001b[0m, std: 0.5931\n",
      "2021-05-13 00:19:36,675 - INFO - Epoch/batch: 9/1812, ibatch: 42312, loss: \u001b[0;36m0.5452\u001b[0m, std: 0.6202\n",
      "2021-05-13 00:19:42,269 - INFO - Epoch/batch: 9/1963, ibatch: 42463, loss: \u001b[0;36m0.5166\u001b[0m, std: 0.6012\n",
      "2021-05-13 00:19:47,841 - INFO - Epoch/batch: 9/2114, ibatch: 42614, loss: \u001b[0;36m0.5292\u001b[0m, std: 0.6102\n",
      "2021-05-13 00:20:02,465 - INFO - loss: \u001b[0;32m0.5245\u001b[0m, std: 0.5946\n",
      "2021-05-13 00:20:03,035 - INFO - Epoch/batch: 9/2265, ibatch: 42765, loss: \u001b[0;36m0.5184\u001b[0m, std: 0.5980\n",
      "2021-05-13 00:20:08,636 - INFO - Epoch/batch: 9/2416, ibatch: 42916, loss: \u001b[0;36m0.5495\u001b[0m, std: 0.6191\n",
      "2021-05-13 00:20:14,251 - INFO - Epoch/batch: 9/2567, ibatch: 43067, loss: \u001b[0;36m0.5235\u001b[0m, std: 0.6033\n",
      "2021-05-13 00:20:29,278 - INFO - loss: \u001b[0;32m0.5242\u001b[0m, std: 0.6101\n",
      "2021-05-13 00:20:29,293 - INFO - Saved model states in: earlystop_0.5242\n",
      "2021-05-13 00:20:29,294 - INFO - Saved net python code: earlystop_0.5242/paddle_nets.py\n",
      "2021-05-13 00:20:29,300 - INFO - Saved best model: earlystop_0.5242\n",
      "2021-05-13 00:20:29,301 - INFO - Removing earlystop model: earlystop_0.5244.1\n",
      "2021-05-13 00:20:29,950 - INFO - Epoch/batch: 9/2718, ibatch: 43218, loss: \u001b[0;36m0.5110\u001b[0m, std: 0.5955\n",
      "2021-05-13 00:20:35,775 - INFO - Epoch/batch: 9/2869, ibatch: 43369, loss: \u001b[0;36m0.5511\u001b[0m, std: 0.6182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 290: ReduceOnPlateau set learning rate to 0.00032825696739453717.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:20:41,509 - INFO - Epoch/batch: 9/3020, ibatch: 43520, loss: \u001b[0;36m0.5289\u001b[0m, std: 0.6071\n",
      "2021-05-13 00:20:56,197 - INFO - loss: \u001b[0;32m0.5244\u001b[0m, std: 0.6011\n",
      "2021-05-13 00:20:57,057 - INFO - Epoch/batch: 9/3171, ibatch: 43671, loss: \u001b[0;36m0.5282\u001b[0m, std: 0.5991\n",
      "2021-05-13 00:21:03,127 - INFO - Epoch/batch: 9/3322, ibatch: 43822, loss: \u001b[0;36m0.5237\u001b[0m, std: 0.6113\n",
      "2021-05-13 00:21:08,845 - INFO - Epoch/batch: 9/3473, ibatch: 43973, loss: \u001b[0;36m0.5250\u001b[0m, std: 0.5992\n",
      "2021-05-13 00:21:22,983 - INFO - loss: \u001b[0;32m0.5241\u001b[0m, std: 0.6107\n",
      "2021-05-13 00:21:22,998 - INFO - Saved model states in: earlystop_0.5241\n",
      "2021-05-13 00:21:23,008 - INFO - Saved net python code: earlystop_0.5241/paddle_nets.py\n",
      "2021-05-13 00:21:23,014 - INFO - Saved best model: earlystop_0.5241\n",
      "2021-05-13 00:21:23,015 - INFO - Removing earlystop model: earlystop_0.5242\n",
      "2021-05-13 00:21:24,114 - INFO - Epoch/batch: 9/3624, ibatch: 44124, loss: \u001b[0;36m0.5110\u001b[0m, std: 0.5998\n",
      "2021-05-13 00:21:29,882 - INFO - Epoch/batch: 9/3775, ibatch: 44275, loss: \u001b[0;36m0.5308\u001b[0m, std: 0.6039\n",
      "2021-05-13 00:21:35,771 - INFO - Epoch/batch: 9/3926, ibatch: 44426, loss: \u001b[0;36m0.5431\u001b[0m, std: 0.6129\n",
      "2021-05-13 00:21:50,802 - INFO - loss: \u001b[0;32m0.5242\u001b[0m, std: 0.5970\n",
      "2021-05-13 00:21:51,875 - INFO - Epoch/batch: 9/4077, ibatch: 44577, loss: \u001b[0;36m0.5430\u001b[0m, std: 0.6202\n",
      "2021-05-13 00:21:57,334 - INFO - Epoch/batch: 9/4228, ibatch: 44728, loss: \u001b[0;36m0.5114\u001b[0m, std: 0.5962\n",
      "2021-05-13 00:22:03,013 - INFO - Epoch/batch: 9/4379, ibatch: 44879, loss: \u001b[0;36m0.5303\u001b[0m, std: 0.6084\n",
      "2021-05-13 00:22:17,278 - INFO - loss: \u001b[0;32m0.5259\u001b[0m, std: 0.5880\n",
      "2021-05-13 00:22:18,608 - INFO - Epoch 9 average training loss: \u001b[0;46m0.5299\u001b[0m std: 0.6088\n",
      "2021-05-13 00:22:18,613 - INFO - Epoch 9 average validate loss: \u001b[0;46m0.5249\u001b[0m std: 0.6033\n",
      "2021-05-13 00:22:20,567 - INFO - Epoch/batch: 10/   0, ibatch: 45000, loss: \u001b[0;36m0.5521\u001b[0m, std: 0.6318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301: ReduceOnPlateau set learning rate to 0.00029543127065508344.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:22:30,336 - INFO - loss: \u001b[0;32m0.5258\u001b[0m, std: 0.5888\n",
      "2021-05-13 00:22:36,073 - INFO - Epoch/batch: 10/ 151, ibatch: 45151, loss: \u001b[0;36m0.5214\u001b[0m, std: 0.5921\n",
      "2021-05-13 00:22:41,859 - INFO - Epoch/batch: 10/ 302, ibatch: 45302, loss: \u001b[0;36m0.5416\u001b[0m, std: 0.6268\n",
      "2021-05-13 00:22:57,347 - INFO - loss: \u001b[0;32m0.5255\u001b[0m, std: 0.5912\n",
      "2021-05-13 00:22:57,461 - INFO - Epoch/batch: 10/ 453, ibatch: 45453, loss: \u001b[0;36m0.5440\u001b[0m, std: 0.6126\n",
      "2021-05-13 00:23:03,452 - INFO - Epoch/batch: 10/ 604, ibatch: 45604, loss: \u001b[0;36m0.5296\u001b[0m, std: 0.5996\n",
      "2021-05-13 00:23:09,307 - INFO - Epoch/batch: 10/ 755, ibatch: 45755, loss: \u001b[0;36m0.5357\u001b[0m, std: 0.6130\n",
      "2021-05-13 00:23:24,851 - INFO - loss: \u001b[0;32m0.5245\u001b[0m, std: 0.6032\n",
      "2021-05-13 00:23:25,077 - INFO - Epoch/batch: 10/ 906, ibatch: 45906, loss: \u001b[0;36m0.5381\u001b[0m, std: 0.6115\n",
      "2021-05-13 00:23:31,042 - INFO - Epoch/batch: 10/1057, ibatch: 46057, loss: \u001b[0;36m0.5430\u001b[0m, std: 0.6208\n",
      "2021-05-13 00:23:36,681 - INFO - Epoch/batch: 10/1208, ibatch: 46208, loss: \u001b[0;36m0.5402\u001b[0m, std: 0.6196\n",
      "2021-05-13 00:23:52,900 - INFO - loss: \u001b[0;32m0.5250\u001b[0m, std: 0.5981\n",
      "2021-05-13 00:23:53,248 - INFO - Epoch/batch: 10/1359, ibatch: 46359, loss: \u001b[0;36m0.5393\u001b[0m, std: 0.6088\n",
      "2021-05-13 00:23:59,079 - INFO - Epoch/batch: 10/1510, ibatch: 46510, loss: \u001b[0;36m0.5272\u001b[0m, std: 0.6182\n",
      "2021-05-13 00:24:04,394 - INFO - Epoch/batch: 10/1661, ibatch: 46661, loss: \u001b[0;36m0.5202\u001b[0m, std: 0.5978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 312: ReduceOnPlateau set learning rate to 0.0002658881435895751.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 00:24:19,096 - INFO - loss: \u001b[0;32m0.5248\u001b[0m, std: 0.5930\n",
      "2021-05-13 00:24:19,570 - INFO - Epoch/batch: 10/1812, ibatch: 46812, loss: \u001b[0;36m0.5209\u001b[0m, std: 0.6100\n",
      "2021-05-13 00:24:25,311 - INFO - Epoch/batch: 10/1963, ibatch: 46963, loss: \u001b[0;36m0.5237\u001b[0m, std: 0.5978\n",
      "2021-05-13 00:24:30,959 - INFO - Epoch/batch: 10/2114, ibatch: 47114, loss: \u001b[0;36m0.5165\u001b[0m, std: 0.5939\n"
     ]
    }
   ],
   "source": [
    "# 训练模型， 最后的loss应该在[0.52, 0.53]区间内. \n",
    "# 每epoch需要五分钟左右(在CPU上)， 自然结束需要～20个epoch\n",
    "train_loss, valid_loss = fp.train(model, train_data, num_epochs=21, validate_callback = fp.func_partial(fp.validate_in_train, midata=valid_data, save_dir='./'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 读取最后一个checkpoint目录 (忽略优化器state_dict读取的问题)\n",
    "# read the last saved earlystop folder （ignore the error in optimizer state_dict loading)\n",
    "fp.state_dict_load(model, model.validate_hist.saved_dirs[-1])\n",
    "# 可以改动损失函数，检测mse损失\n",
    "args.loss_fn = ['softmax+mse']\n",
    "model.loss_fn = fp.get_loss_fn(args)\n",
    "valid_loss = fp.validate(model, valid_data, verbose=1, batch_size=64) # try a larger batch_size, should make no difference though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 读取预测数据， 存储预测结果\n",
    "# Read in prediction data, and save the predicted results\n",
    "predict_data = fp.get_midata(args, data_name='predict', seq_length=-1)\n",
    "y_model, std_model = fp.predict(model, predict_data, save_dir='predict.files', batch_size=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
